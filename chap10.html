
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Testing &#8212; Think Bayes</title>
    
  <link href="_static/css/theme.css" rel="stylesheet" />
  <link href="_static/css/index.c5995385ac14fb8791e8eb36b4908be2.css" rel="stylesheet" />

    
  <link rel="stylesheet"
    href="_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="_static/sphinx-book-theme.e8f53015daec13862f6db5e763c41738.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="_static/js/index.1c5a1a01449ed65a7b51.js">

    <script id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/togglebutton.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="_static/sphinx-book-theme.12a9622fbb08dcb3a2a40b2c02b83a57.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
    <script async="async" src="https://unpkg.com/thebe@0.5.1/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="_static/sphinx-thebe.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Comparison" href="chap11.html" />
    <link rel="prev" title="Decision Analysis" href="chap09.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="index.html">
      
      
      <h1 class="site-logo" id="site-title">Think Bayes</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="index.html">
   Think Bayes 2
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Chapters
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="preface.html">
   Preface
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="chap01.html">
   Probability
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="chap02.html">
   Bayes’s Theorem
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="chap03.html">
   Distributions
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="chap04.html">
   Estimating Proportions
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="chap05.html">
   Estimating Counts
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="chap06.html">
   Odds and Addends
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="chap07.html">
   Minimum, Maximum, and Mixture
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="chap08.html">
   Poisson Processes
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="chap09.html">
   Decision Analysis
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   Testing
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="chap11.html">
   Comparison
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="chap12.html">
   Classification
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="chap13.html">
   Inference
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="chap14.html">
   Survival Analysis
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="chap15.html">
   Mark and Recapture
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="chap16.html">
   Logistic Regression
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="chap17.html">
   Regression
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="chap18.html">
   Conjugate Priors
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="chap19.html">
   MCMC
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="chap20.html">
   Approximate Bayesian Computation
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Examples
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="redline.html">
   The Red Line Problem
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="vaccine2.html">
   Estimating vaccine efficacy
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="usb.html">
   Flipping USB Connectors
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="sister.html">
   The Left Handed Sister Problem
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="bayes_dice.html">
   Bayesian Dice
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="radiation.html">
   The Emitter-Detector Problem
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="hospital.html">
   Grid algorithms for hierarchical models
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="_sources/chap10.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        <a class="repository-button"
            href="https://github.com/AllenDowney/ThinkBayes2"><button type="button" class="btn btn-secondary topbarbtn"
                data-toggle="tooltip" data-placement="left" title="Source repository"><i
                    class="fab fa-github"></i>repository</button></a>
        
        
    </div>
</div>

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        <a class="binder-button" href="https://mybinder.org/v2/gh/AllenDowney/ThinkBayes2/master?urlpath=tree/chap10.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Launch Binder" data-toggle="tooltip"
                data-placement="left"><img class="binder-button-logo"
                    src="_static/images/logo_binder.svg"
                    alt="Interact on binder">Binder</button></a>
        
        
        
        
    </div>
</div>

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav" aria-label="Page">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#estimation">
   Estimation
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#evidence">
   Evidence
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#uniformly-distributed-bias">
   Uniformly Distributed Bias
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#bayesian-hypothesis-testing">
   Bayesian Hypothesis Testing
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#bayesian-bandits">
   Bayesian Bandits
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#prior-beliefs">
   Prior Beliefs
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#the-update">
   The Update
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#multiple-bandits">
   Multiple Bandits
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#explore-and-exploit">
   Explore and Exploit
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#the-strategy">
   The Strategy
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#summary">
   Summary
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#exercises">
   Exercises
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#simulating-the-test">
   Simulating the Test
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#the-prior">
   The Prior
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id1">
   The Update
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#adaptation">
   Adaptation
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#quantifying-precision">
   Quantifying Precision
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#discriminatory-power">
   Discriminatory Power
  </a>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="testing">
<h1>Testing<a class="headerlink" href="#testing" title="Permalink to this headline">¶</a></h1>
<p>In &lt;&lt;_TheEuroProblem&gt;&gt; I presented a problem from David MacKay’s book, <a class="reference external" href="http://www.inference.org.uk/mackay/itila/p0.html"><em>Information Theory, Inference, and Learning Algorithms</em></a>:</p>
<p>“A statistical statement appeared in <em>The Guardian</em> on Friday January 4, 2002:</p>
<blockquote>
<div><p>When spun on edge 250 times, a Belgian one-euro coin came up heads 140 times and tails 110.  `It looks very suspicious to me,’ said Barry Blight, a statistics lecturer at the London School of Economics.  `If the coin were unbiased, the chance of getting a result as extreme as that would be less than 7%.’</p>
</div></blockquote>
<p>“But [MacKay asks] do these data give evidence that the coin is biased rather than fair?”</p>
<p>We started to answer this question in &lt;&lt;_EstimatingProportions&gt;&gt;; to review, our answer was based on these modeling decisions:</p>
<ul class="simple">
<li><p>If you spin a coin on edge, there is some probability, <span class="math notranslate nohighlight">\(x\)</span>, that it will land heads up.</p></li>
<li><p>The value of <span class="math notranslate nohighlight">\(x\)</span> varies from one coin to the next, depending on how the coin is balanced and possibly other factors.</p></li>
</ul>
<p>Starting with a uniform prior distribution for <span class="math notranslate nohighlight">\(x\)</span>, we updated it with the given data, 140 heads and 110 tails.  Then we used the posterior distribution to compute the most likely value of <span class="math notranslate nohighlight">\(x\)</span>, the posterior mean, and a credible interval.</p>
<p>But we never really answered MacKay’s question: “Do these data give evidence that the coin is biased rather than fair?”</p>
<p>In this chapter, finally, we will.</p>
<div class="section" id="estimation">
<h2>Estimation<a class="headerlink" href="#estimation" title="Permalink to this headline">¶</a></h2>
<p>Let’s review the solution to the Euro problem from &lt;&lt;_TheBinomialLikelihoodFunction&gt;&gt;.  We started with a uniform prior.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">empiricaldist</span> <span class="kn">import</span> <span class="n">Pmf</span>

<span class="n">xs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">101</span><span class="p">)</span>
<span class="n">uniform</span> <span class="o">=</span> <span class="n">Pmf</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">xs</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>And we used the binomial distribution to compute the probability of the data for each possible value of <span class="math notranslate nohighlight">\(x\)</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">scipy.stats</span> <span class="kn">import</span> <span class="n">binom</span>

<span class="n">k</span><span class="p">,</span> <span class="n">n</span> <span class="o">=</span> <span class="mi">140</span><span class="p">,</span> <span class="mi">250</span>
<span class="n">likelihood</span> <span class="o">=</span> <span class="n">binom</span><span class="o">.</span><span class="n">pmf</span><span class="p">(</span><span class="n">k</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">xs</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>We computed the posterior distribution in the usual way.</p>
<div class="cell tag_remove-output docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">posterior</span> <span class="o">=</span> <span class="n">uniform</span> <span class="o">*</span> <span class="n">likelihood</span>
<span class="n">posterior</span><span class="o">.</span><span class="n">normalize</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<p>And here’s what it looks like.</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">utils</span> <span class="kn">import</span> <span class="n">decorate</span>

<span class="n">posterior</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">label</span><span class="o">=</span><span class="s1">&#39;140 heads out of 250&#39;</span><span class="p">)</span>

<span class="n">decorate</span><span class="p">(</span><span class="n">xlabel</span><span class="o">=</span><span class="s1">&#39;Proportion of heads (x)&#39;</span><span class="p">,</span>
         <span class="n">ylabel</span><span class="o">=</span><span class="s1">&#39;Probability&#39;</span><span class="p">,</span>
         <span class="n">title</span><span class="o">=</span><span class="s1">&#39;Posterior distribution of x&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/chap10_13_0.png" src="_images/chap10_13_0.png" />
</div>
</div>
<p>Again, the posterior mean is about 0.56, with a 90% credible interval from 0.51 to 0.61.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">posterior</span><span class="o">.</span><span class="n">mean</span><span class="p">(),</span> 
      <span class="n">posterior</span><span class="o">.</span><span class="n">credible_interval</span><span class="p">(</span><span class="mf">0.9</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>0.5595238095238095 [0.51 0.61]
</pre></div>
</div>
</div>
</div>
<p>The prior mean was 0.5, and the posterior mean is 0.56, so it seems like the data is evidence that the coin is biased.</p>
<p>But, it turns out not to be that simple.</p>
</div>
<div class="section" id="evidence">
<h2>Evidence<a class="headerlink" href="#evidence" title="Permalink to this headline">¶</a></h2>
<p>In &lt;&lt;_OliversBlood&gt;&gt;, I said that data are considered evidence in favor of a hypothesis, <span class="math notranslate nohighlight">\(A\)</span>, if the data are more likely under <span class="math notranslate nohighlight">\(A\)</span> than under the alternative, <span class="math notranslate nohighlight">\(B\)</span>; that is if</p>
<div class="math notranslate nohighlight">
\[P(D|A) &gt; P(D|B)\]</div>
<p>Furthermore, we can quantify the strength of the evidence by computing the ratio of these likelihoods, which is known as the <a class="reference external" href="https://en.wikipedia.org/wiki/Bayes_factor">Bayes factor</a> and often denoted <span class="math notranslate nohighlight">\(K\)</span>:</p>
<div class="math notranslate nohighlight">
\[K = \frac{P(D|A)}{P(D|B)}\]</div>
<p>So, for the Euro problem, let’s consider two hypotheses, <code class="docutils literal notranslate"><span class="pre">fair</span></code> and <code class="docutils literal notranslate"><span class="pre">biased</span></code>, and compute the likelihood of the data under each hypothesis.</p>
<p>If the coin is fair, the probability of heads is 50%, and we can compute the probability of the data (140 heads out of 250 spins) using the binomial distribution:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">k</span> <span class="o">=</span> <span class="mi">140</span>
<span class="n">n</span> <span class="o">=</span> <span class="mi">250</span>

<span class="n">like_fair</span> <span class="o">=</span> <span class="n">binom</span><span class="o">.</span><span class="n">pmf</span><span class="p">(</span><span class="n">k</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
<span class="n">like_fair</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>0.008357181724917673
</pre></div>
</div>
</div>
</div>
<p>That’s the probability of the data, given that the coin is fair.</p>
<p>But if the coin is biased, what’s the probability of the data?  That depends on what “biased” means.
If we know ahead of time that “biased” means the probability of heads is 56%, we can use the binomial distribution again:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">like_biased</span> <span class="o">=</span> <span class="n">binom</span><span class="o">.</span><span class="n">pmf</span><span class="p">(</span><span class="n">k</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="mf">0.56</span><span class="p">)</span>
<span class="n">like_biased</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>0.05077815959517949
</pre></div>
</div>
</div>
</div>
<p>Now we can compute the likelihood ratio:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">K</span> <span class="o">=</span> <span class="n">like_biased</span> <span class="o">/</span> <span class="n">like_fair</span>
<span class="n">K</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>6.075990838368387
</pre></div>
</div>
</div>
</div>
<p>The data are about 6 times more likely if the coin is biased, by this definition, than if it is fair.</p>
<p>But we used the data to define the hypothesis, which seems like cheating.  To be fair, we should define “biased” before we see the data.</p>
</div>
<div class="section" id="uniformly-distributed-bias">
<h2>Uniformly Distributed Bias<a class="headerlink" href="#uniformly-distributed-bias" title="Permalink to this headline">¶</a></h2>
<p>Suppose “biased” means that the probability of heads is anything except 50%, and all other values are equally likely.</p>
<p>We can represent that definition by making a uniform distribution and removing 50%.</p>
<div class="cell tag_remove-output docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">biased_uniform</span> <span class="o">=</span> <span class="n">uniform</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
<span class="n">biased_uniform</span><span class="p">[</span><span class="mf">0.5</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">biased_uniform</span><span class="o">.</span><span class="n">normalize</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<p>To compute the total probability of the data under this hypothesis, we compute the conditional probability of the data for each value of <span class="math notranslate nohighlight">\(x\)</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">xs</span> <span class="o">=</span> <span class="n">biased_uniform</span><span class="o">.</span><span class="n">qs</span>
<span class="n">likelihood</span> <span class="o">=</span> <span class="n">binom</span><span class="o">.</span><span class="n">pmf</span><span class="p">(</span><span class="n">k</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">xs</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Then multiply by the prior probabilities and add up the products:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">like_uniform</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">biased_uniform</span> <span class="o">*</span> <span class="n">likelihood</span><span class="p">)</span>
<span class="n">like_uniform</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>0.0039004919277704267
</pre></div>
</div>
</div>
</div>
<p>So that’s the probability of the data under the “biased uniform” hypothesis.</p>
<p>Now we can compute the likelihood ratio of the data under the <code class="docutils literal notranslate"><span class="pre">fair</span></code> and <code class="docutils literal notranslate"><span class="pre">biased</span> <span class="pre">uniform</span></code> hypotheses:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">K</span> <span class="o">=</span> <span class="n">like_fair</span> <span class="o">/</span> <span class="n">like_uniform</span>
<span class="n">K</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>2.1425968518013954
</pre></div>
</div>
</div>
</div>
<p>The data are about two times more likely if the coin is fair than if it is biased, by this definition of “biased”.</p>
<p>To get a sense of how strong that evidence is, we can apply Bayes’s rule.
For example, if the prior probability is 50% that the coin is biased, the prior odds are 1, so the posterior odds are about 2.1 to 1 and the posterior probability is about 68%.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">prior_odds</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">posterior_odds</span> <span class="o">=</span> <span class="n">prior_odds</span> <span class="o">*</span> <span class="n">K</span>
<span class="n">posterior_odds</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>2.1425968518013954
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">prob</span><span class="p">(</span><span class="n">o</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">o</span> <span class="o">/</span> <span class="p">(</span><span class="n">o</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">posterior_probability</span> <span class="o">=</span> <span class="n">prob</span><span class="p">(</span><span class="n">posterior_odds</span><span class="p">)</span>
<span class="n">posterior_probability</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>0.6817918278551125
</pre></div>
</div>
</div>
</div>
<p>Evidence that “moves the needle” from 50% to 68% is not very strong.</p>
<p>Now suppose “biased” doesn’t mean every value of <span class="math notranslate nohighlight">\(x\)</span> is equally likely.  Maybe values near 50% are more likely and values near the extremes are less likely.
We could use a triangle-shaped distribution to represent this alternative definition of “biased”:</p>
<div class="cell tag_remove-output docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">ramp_up</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">50</span><span class="p">)</span>
<span class="n">ramp_down</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">50</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
<span class="n">a</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">ramp_up</span><span class="p">,</span> <span class="n">ramp_down</span><span class="p">)</span>

<span class="n">triangle</span> <span class="o">=</span> <span class="n">Pmf</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">xs</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;triangle&#39;</span><span class="p">)</span>
<span class="n">triangle</span><span class="o">.</span><span class="n">normalize</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<p>As we did with the uniform distribution, we can remove 50% as a possible value of <span class="math notranslate nohighlight">\(x\)</span> (but it doesn’t make much difference if we skip this detail).</p>
<div class="cell tag_remove-output docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">biased_triangle</span> <span class="o">=</span> <span class="n">triangle</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
<span class="n">biased_triangle</span><span class="p">[</span><span class="mf">0.5</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">biased_triangle</span><span class="o">.</span><span class="n">normalize</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<p>Here’s what the triangle prior looks like, compared to the uniform prior.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">biased_uniform</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">label</span><span class="o">=</span><span class="s1">&#39;uniform prior&#39;</span><span class="p">)</span>
<span class="n">biased_triangle</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">label</span><span class="o">=</span><span class="s1">&#39;triangle prior&#39;</span><span class="p">)</span>

<span class="n">decorate</span><span class="p">(</span><span class="n">xlabel</span><span class="o">=</span><span class="s1">&#39;Proportion of heads (x)&#39;</span><span class="p">,</span>
         <span class="n">ylabel</span><span class="o">=</span><span class="s1">&#39;Probability&#39;</span><span class="p">,</span>
         <span class="n">title</span><span class="o">=</span><span class="s1">&#39;Uniform and triangle prior distributions&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/chap10_42_0.png" src="_images/chap10_42_0.png" />
</div>
</div>
<p><strong>Exercise:</strong> Now compute the total probability of the data under this definition of “biased” and compute the Bayes factor, compared with the fair hypothesis.
Is the data evidence that the coin is biased?</p>
<div class="cell tag_hide-cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Solution</span>

<span class="n">like_triangle</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">biased_triangle</span> <span class="o">*</span> <span class="n">likelihood</span><span class="p">)</span>
<span class="n">like_triangle</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>0.006981325464857327
</pre></div>
</div>
</div>
</div>
<div class="cell tag_hide-cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Solution</span>

<span class="n">K</span> <span class="o">=</span> <span class="n">like_fair</span> <span class="o">/</span> <span class="n">like_triangle</span>
<span class="n">K</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>1.1970766535647344
</pre></div>
</div>
</div>
</div>
<div class="cell tag_hide-cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Solution</span>

<span class="c1"># For this definition of &quot;biased&quot;, the data are </span>
<span class="c1"># very slightly in favor of the fair hypothesis.</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="bayesian-hypothesis-testing">
<h2>Bayesian Hypothesis Testing<a class="headerlink" href="#bayesian-hypothesis-testing" title="Permalink to this headline">¶</a></h2>
<p>What we’ve done so far in this chapter is sometimes called “Bayesian hypothesis testing” in contrast with <a class="reference external" href="https://en.wikipedia.org/wiki/Statistical_hypothesis_testing">statistical hypothesis testing</a>.</p>
<p>In statistical hypothesis testing, we compute a p-value, which is hard to define concisely, and use it to determine whether the results are “statistically significant”, which is also  hard to define concisely.</p>
<p>The Bayesian alternative is to report the Bayes factor, <span class="math notranslate nohighlight">\(K\)</span>, which summarizes the strength of the evidence in favor of one hypothesis or the other.</p>
<p>Some people think it is better to report <span class="math notranslate nohighlight">\(K\)</span> than a posterior probability because <span class="math notranslate nohighlight">\(K\)</span> does not depend on a prior probability.
But as we saw in this example, <span class="math notranslate nohighlight">\(K\)</span> often depends on a precise definition of the hypotheses, which can be just as controversial as a prior probability.</p>
<p>In my opinion, Bayesian hypothesis testing is better because it measures the strength of the evidence on a continuum, rather that trying to make a binary determination.
But it doesn’t solve what I think is the fundamental problem, which is that hypothesis testing is not asking the question we really care about.</p>
<p>To see why, suppose you test the coin and decide that it is biased after all.  What can you do with this answer?  In my opinion, not much.
In contrast, there are two questions I think are more useful (and therefore more meaningful):</p>
<ul class="simple">
<li><p>Prediction: Based on what we know about the coin, what should we expect to happen in the future?</p></li>
<li><p>Decision-making: Can we use those predictions to make better decisions?</p></li>
</ul>
<p>At this point, we’ve seen a few examples of prediction.  For example, in &lt;&lt;_PoissonProcesses&gt;&gt; we used the posterior distribution of goal-scoring rates to predict the outcome of soccer games.</p>
<p>And we’ve seen one previous example of decision analysis: In &lt;&lt;_DecisionAnalysis&gt;&gt; we used the distribution of prices to choose an optimal bid on <em>The Price is Right</em>.</p>
<p>So let’s finish this chapter with another example of Bayesian decision analysis, the Bayesian Bandit strategy.</p>
</div>
<div class="section" id="bayesian-bandits">
<h2>Bayesian Bandits<a class="headerlink" href="#bayesian-bandits" title="Permalink to this headline">¶</a></h2>
<p>If you have ever been to a casino, you have probably seen a slot machine, which is sometimes called a “one-armed bandit” because it has a handle like an arm and the ability to take money like a bandit.</p>
<p>The Bayesian Bandit strategy is named after one-armed bandits because it solves a problem based on a simplified version of a slot machine.</p>
<p>Suppose that each time you play a slot machine, there is a fixed probability that you win.  And suppose that different machines give you different probabilities of winning, but you don’t know what the probabilities are.</p>
<p>Initially, you have the same prior belief about each of the machines, so you have no reason to prefer one over the others.  But if you play each machine a few times, you can use the results to estimate the probabilities.  And you can use the estimated probabilities to decide which machine to play next.</p>
<p>At a high level, that’s the Bayesian bandit strategy.  Now let’s see the details.</p>
</div>
<div class="section" id="prior-beliefs">
<h2>Prior Beliefs<a class="headerlink" href="#prior-beliefs" title="Permalink to this headline">¶</a></h2>
<p>If we know nothing about the probability of winning, we can start with a uniform prior.</p>
<div class="cell tag_remove-output docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">xs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">101</span><span class="p">)</span>
<span class="n">prior</span> <span class="o">=</span> <span class="n">Pmf</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">xs</span><span class="p">)</span>
<span class="n">prior</span><span class="o">.</span><span class="n">normalize</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<p>Supposing we are choosing from four slot machines, I’ll make four copies of the prior, one for each machine.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">beliefs</span> <span class="o">=</span> <span class="p">[</span><span class="n">prior</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">4</span><span class="p">)]</span>
</pre></div>
</div>
</div>
</div>
<p>This function displays four distributions in a grid.</p>
<div class="cell tag_hide-cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="n">options</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="n">xticklabels</span><span class="o">=</span><span class="s1">&#39;invisible&#39;</span><span class="p">,</span> <span class="n">yticklabels</span><span class="o">=</span><span class="s1">&#39;invisible&#39;</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">plot</span><span class="p">(</span><span class="n">beliefs</span><span class="p">,</span> <span class="o">**</span><span class="n">options</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">pmf</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">beliefs</span><span class="p">):</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">pmf</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">label</span><span class="o">=</span><span class="s1">&#39;Machine </span><span class="si">%s</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">i</span><span class="p">)</span>
        <span class="n">decorate</span><span class="p">(</span><span class="n">yticklabels</span><span class="o">=</span><span class="p">[])</span>
        
        <span class="k">if</span> <span class="n">i</span> <span class="ow">in</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">]:</span>
            <span class="n">decorate</span><span class="p">(</span><span class="n">ylabel</span><span class="o">=</span><span class="s1">&#39;PDF&#39;</span><span class="p">)</span>
        
        <span class="k">if</span> <span class="n">i</span> <span class="ow">in</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]:</span>
            <span class="n">decorate</span><span class="p">(</span><span class="n">xlabel</span><span class="o">=</span><span class="s1">&#39;Probability of winning&#39;</span><span class="p">)</span>
        
    <span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<p>Here’s what the prior distributions look like for the four machines.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plot</span><span class="p">(</span><span class="n">beliefs</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/chap10_57_0.png" src="_images/chap10_57_0.png" />
</div>
</div>
</div>
<div class="section" id="the-update">
<h2>The Update<a class="headerlink" href="#the-update" title="Permalink to this headline">¶</a></h2>
<p>Each time we play a machine, we can use the outcome to update our beliefs.  The following function does the update.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">likelihood</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">&#39;W&#39;</span><span class="p">:</span> <span class="n">xs</span><span class="p">,</span>
    <span class="s1">&#39;L&#39;</span><span class="p">:</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">xs</span>
<span class="p">}</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">update</span><span class="p">(</span><span class="n">pmf</span><span class="p">,</span> <span class="n">data</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Update the probability of winning.&quot;&quot;&quot;</span>
    <span class="n">pmf</span> <span class="o">*=</span> <span class="n">likelihood</span><span class="p">[</span><span class="n">data</span><span class="p">]</span>
    <span class="n">pmf</span><span class="o">.</span><span class="n">normalize</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<p>This function updates the prior distribution in place.
<code class="docutils literal notranslate"><span class="pre">pmf</span></code> is a <code class="docutils literal notranslate"><span class="pre">Pmf</span></code> that represents the prior distribution of <code class="docutils literal notranslate"><span class="pre">x</span></code>, which is the probability of winning.</p>
<p><code class="docutils literal notranslate"><span class="pre">data</span></code> is a string, either <code class="docutils literal notranslate"><span class="pre">W</span></code> if the outcome is a win or <code class="docutils literal notranslate"><span class="pre">L</span></code> if the outcome is a loss.</p>
<p>The likelihood of the data is either <code class="docutils literal notranslate"><span class="pre">xs</span></code> or <code class="docutils literal notranslate"><span class="pre">1-xs</span></code>, depending on the outcome.</p>
<p>Suppose we choose a machine, play 10 times, and win once.  We can compute the posterior distribution of <code class="docutils literal notranslate"><span class="pre">x</span></code>, based on this outcome, like this:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">bandit</span> <span class="o">=</span> <span class="n">prior</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>

<span class="k">for</span> <span class="n">outcome</span> <span class="ow">in</span> <span class="s1">&#39;WLLLLLLLLL&#39;</span><span class="p">:</span>
    <span class="n">update</span><span class="p">(</span><span class="n">bandit</span><span class="p">,</span> <span class="n">outcome</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Here’s what the posterior looks like.</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">bandit</span><span class="o">.</span><span class="n">plot</span><span class="p">()</span>
<span class="n">decorate</span><span class="p">(</span><span class="n">xlabel</span><span class="o">=</span><span class="s1">&#39;Probability of winning&#39;</span><span class="p">,</span>
         <span class="n">ylabel</span><span class="o">=</span><span class="s1">&#39;PDF&#39;</span><span class="p">,</span>
         <span class="n">title</span><span class="o">=</span><span class="s1">&#39;Posterior distribution, nine losses, one win&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/chap10_65_0.png" src="_images/chap10_65_0.png" />
</div>
</div>
</div>
<div class="section" id="multiple-bandits">
<h2>Multiple Bandits<a class="headerlink" href="#multiple-bandits" title="Permalink to this headline">¶</a></h2>
<p>Now suppose we have four machines with these probabilities:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">actual_probs</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.10</span><span class="p">,</span> <span class="mf">0.20</span><span class="p">,</span> <span class="mf">0.30</span><span class="p">,</span> <span class="mf">0.40</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<p>Remember that as a player, we don’t know these probabilities.</p>
<p>The following function takes the index of a machine, simulates playing the machine once, and returns the outcome, <code class="docutils literal notranslate"><span class="pre">W</span></code> or <code class="docutils literal notranslate"><span class="pre">L</span></code>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">collections</span> <span class="kn">import</span> <span class="n">Counter</span>

<span class="c1"># count how many times we&#39;ve played each machine</span>
<span class="n">counter</span> <span class="o">=</span> <span class="n">Counter</span><span class="p">()</span>

<span class="k">def</span> <span class="nf">play</span><span class="p">(</span><span class="n">i</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Play machine i.</span>
<span class="sd">    </span>
<span class="sd">    i: index of the machine to play</span>
<span class="sd">    </span>
<span class="sd">    returns: string &#39;W&#39; or &#39;L&#39;</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">counter</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>
    <span class="n">p</span> <span class="o">=</span> <span class="n">actual_probs</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
    <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">()</span> <span class="o">&lt;</span> <span class="n">p</span><span class="p">:</span>
        <span class="k">return</span> <span class="s1">&#39;W&#39;</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="s1">&#39;L&#39;</span>
</pre></div>
</div>
</div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">counter</span></code> is a <code class="docutils literal notranslate"><span class="pre">Counter</span></code>, which is a kind of dictionary we’ll use to keep track of how many times each machine is played.</p>
<p>Here’s a test that plays each machine 10 times.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">4</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">):</span>
        <span class="n">outcome</span> <span class="o">=</span> <span class="n">play</span><span class="p">(</span><span class="n">i</span><span class="p">)</span>
        <span class="n">update</span><span class="p">(</span><span class="n">beliefs</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">outcome</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Each time through the inner loop, we play one machine and update our beliefs.</p>
<p>Here’s what our posterior beliefs look like.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plot</span><span class="p">(</span><span class="n">beliefs</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/chap10_74_0.png" src="_images/chap10_74_0.png" />
</div>
</div>
<p>Here are the actual probabilities, posterior means, and 90% credible intervals.</p>
<div class="cell tag_hide-cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>

<span class="k">def</span> <span class="nf">summarize_beliefs</span><span class="p">(</span><span class="n">beliefs</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Compute means and credible intervals.</span>
<span class="sd">    </span>
<span class="sd">    beliefs: sequence of Pmf</span>
<span class="sd">    </span>
<span class="sd">    returns: DataFrame</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">columns</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;Actual P(win)&#39;</span><span class="p">,</span> 
               <span class="s1">&#39;Posterior mean&#39;</span><span class="p">,</span> 
               <span class="s1">&#39;Credible interval&#39;</span><span class="p">]</span>
    
    <span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="n">columns</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">b</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">beliefs</span><span class="p">):</span>
        <span class="n">mean</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">b</span><span class="o">.</span><span class="n">mean</span><span class="p">(),</span> <span class="mi">3</span><span class="p">)</span>
        <span class="n">ci</span> <span class="o">=</span> <span class="n">b</span><span class="o">.</span><span class="n">credible_interval</span><span class="p">(</span><span class="mf">0.9</span><span class="p">)</span>
        <span class="n">ci</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">ci</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
        <span class="n">df</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">actual_probs</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">mean</span><span class="p">,</span> <span class="n">ci</span>
    <span class="k">return</span> <span class="n">df</span>
</pre></div>
</div>
</div>
</div>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">summarize_beliefs</span><span class="p">(</span><span class="n">beliefs</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Actual P(win)</th>
      <th>Posterior mean</th>
      <th>Credible interval</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0.1</td>
      <td>0.250</td>
      <td>[0.08, 0.47]</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0.2</td>
      <td>0.250</td>
      <td>[0.08, 0.47]</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.3</td>
      <td>0.500</td>
      <td>[0.27, 0.73]</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.4</td>
      <td>0.417</td>
      <td>[0.2, 0.65]</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>We expect the credible intervals to contain the actual probabilities most of the time.</p>
</div>
<div class="section" id="explore-and-exploit">
<h2>Explore and Exploit<a class="headerlink" href="#explore-and-exploit" title="Permalink to this headline">¶</a></h2>
<p>Based on these posterior distributions, which machine do you think we should play next?  One option would be to choose the machine with the highest posterior mean.</p>
<p>That would not be a bad idea, but it has a drawback: since we have only played each machine a few times, the posterior distributions are wide and overlapping, which means we are not sure which machine is the best; if we focus on one machine too soon, we might choose the wrong machine and play it more than we should.</p>
<p>To avoid that problem, we could go to the other extreme and play all machines equally until we are confident we have identified the best machine, and then play it exclusively.</p>
<p>That’s not a bad idea either, but it has a drawback: while we are gathering data, we are not making good use of it; until we’re sure which machine is the best, we are playing the others more than we should.</p>
<p>The Bayesian Bandits strategy avoids both drawbacks by gathering and using data at the same time.  In other words, it balances exploration and exploitation.</p>
<p>The kernel of the idea is called <a class="reference external" href="https://en.wikipedia.org/wiki/Thompson_sampling">Thompson sampling</a>: when we choose a machine, we choose at random so that the probability of choosing each machine is proportional to the probability that it is the best.</p>
<p>Given the posterior distributions, we can compute the “probability of superiority” for each machine.</p>
<p>Here’s one way to do it.  We can draw a sample of 1000 values from each posterior distribution, like this:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">samples</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">b</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="mi">1000</span><span class="p">)</span> 
                    <span class="k">for</span> <span class="n">b</span> <span class="ow">in</span> <span class="n">beliefs</span><span class="p">])</span>
<span class="n">samples</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(4, 1000)
</pre></div>
</div>
</div>
</div>
<p>The result has 4 rows and 1000 columns.  We can use <code class="docutils literal notranslate"><span class="pre">argmax</span></code> to find the index of the largest value in each column:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">indices</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">samples</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">indices</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(1000,)
</pre></div>
</div>
</div>
</div>
<p>The <code class="docutils literal notranslate"><span class="pre">Pmf</span></code> of these indices is the fraction of times each machine yielded the highest values.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">pmf</span> <span class="o">=</span> <span class="n">Pmf</span><span class="o">.</span><span class="n">from_seq</span><span class="p">(</span><span class="n">indices</span><span class="p">)</span>
<span class="n">pmf</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>probs</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0.048</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0.043</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.625</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.284</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>These fractions approximate the probability of superiority for each machine.  So we could choose the next machine by choosing a value from this <code class="docutils literal notranslate"><span class="pre">Pmf</span></code>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">pmf</span><span class="o">.</span><span class="n">choice</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>1
</pre></div>
</div>
</div>
</div>
<p>But that’s a lot of work to choose a single value, and it’s not really necessary, because there’s a shortcut.</p>
<p>If we draw a single random value from each posterior distribution and select the machine that yields the highest value, it turns out that we’ll select each machine in proportion to its probability of superiority.</p>
<p>That’s what the following function does.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">choose</span><span class="p">(</span><span class="n">beliefs</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Use Thompson sampling to choose a machine.</span>
<span class="sd">    </span>
<span class="sd">    Draws a single sample from each distribution.</span>
<span class="sd">    </span>
<span class="sd">    returns: index of the machine that yielded the highest value</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">ps</span> <span class="o">=</span> <span class="p">[</span><span class="n">b</span><span class="o">.</span><span class="n">choice</span><span class="p">()</span> <span class="k">for</span> <span class="n">b</span> <span class="ow">in</span> <span class="n">beliefs</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">ps</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>This function chooses one value from the posterior distribution of each machine and then uses <code class="docutils literal notranslate"><span class="pre">argmax</span></code> to find the index of the machine that yielded the highest value.</p>
<p>Here’s an example.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">choose</span><span class="p">(</span><span class="n">beliefs</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>3
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="the-strategy">
<h2>The Strategy<a class="headerlink" href="#the-strategy" title="Permalink to this headline">¶</a></h2>
<p>Putting it all together, the following function chooses a machine, plays once, and updates <code class="docutils literal notranslate"><span class="pre">beliefs</span></code>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">choose_play_update</span><span class="p">(</span><span class="n">beliefs</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Choose a machine, play it, and update beliefs.&quot;&quot;&quot;</span>
    
    <span class="c1"># choose a machine</span>
    <span class="n">machine</span> <span class="o">=</span> <span class="n">choose</span><span class="p">(</span><span class="n">beliefs</span><span class="p">)</span>
    
    <span class="c1"># play it</span>
    <span class="n">outcome</span> <span class="o">=</span> <span class="n">play</span><span class="p">(</span><span class="n">machine</span><span class="p">)</span>
    
    <span class="c1"># update beliefs</span>
    <span class="n">update</span><span class="p">(</span><span class="n">beliefs</span><span class="p">[</span><span class="n">machine</span><span class="p">],</span> <span class="n">outcome</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>To test it out, let’s start again with a fresh set of beliefs and an empty <code class="docutils literal notranslate"><span class="pre">Counter</span></code>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">beliefs</span> <span class="o">=</span> <span class="p">[</span><span class="n">prior</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">4</span><span class="p">)]</span>
<span class="n">counter</span> <span class="o">=</span> <span class="n">Counter</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<p>If we run the bandit algorithm 100 times, we can see how <code class="docutils literal notranslate"><span class="pre">beliefs</span></code> gets updated:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">num_plays</span> <span class="o">=</span> <span class="mi">100</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_plays</span><span class="p">):</span>
    <span class="n">choose_play_update</span><span class="p">(</span><span class="n">beliefs</span><span class="p">)</span>
    
<span class="n">plot</span><span class="p">(</span><span class="n">beliefs</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/chap10_97_0.png" src="_images/chap10_97_0.png" />
</div>
</div>
<p>The following table summarizes the results.</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">summarize_beliefs</span><span class="p">(</span><span class="n">beliefs</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Actual P(win)</th>
      <th>Posterior mean</th>
      <th>Credible interval</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0.1</td>
      <td>0.107</td>
      <td>[0.0, 0.31]</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0.2</td>
      <td>0.269</td>
      <td>[0.14, 0.42]</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.3</td>
      <td>0.293</td>
      <td>[0.18, 0.41]</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.4</td>
      <td>0.438</td>
      <td>[0.3, 0.58]</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>The credible intervals usually contain the actual probabilities of winning.
The estimates are still rough, especially for the lower-probability machines.  But that’s a feature, not a bug: the goal is to play the high-probability machines most often.  Making the estimates more precise is a means to that end, but not an end itself.</p>
<p>More importantly, let’s see how many times each machine got played.</p>
<div class="cell tag_hide-cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">summarize_counter</span><span class="p">(</span><span class="n">counter</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Report the number of times each machine was played.</span>
<span class="sd">    </span>
<span class="sd">    counter: Collections.Counter</span>
<span class="sd">    </span>
<span class="sd">    returns: DataFrame</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">index</span> <span class="o">=</span> <span class="nb">range</span><span class="p">(</span><span class="mi">4</span><span class="p">)</span>
    <span class="n">columns</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;Actual P(win)&#39;</span><span class="p">,</span> <span class="s1">&#39;Times played&#39;</span><span class="p">]</span>
    <span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">index</span><span class="o">=</span><span class="n">index</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="n">columns</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">count</span> <span class="ow">in</span> <span class="n">counter</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
        <span class="n">df</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">actual_probs</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">count</span>
    <span class="k">return</span> <span class="n">df</span>
</pre></div>
</div>
</div>
</div>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">summarize_counter</span><span class="p">(</span><span class="n">counter</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Actual P(win)</th>
      <th>Times played</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0.1</td>
      <td>7</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0.2</td>
      <td>24</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.3</td>
      <td>39</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.4</td>
      <td>30</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>If things go according to plan, the machines with higher probabilities should get played more often.</p>
</div>
<div class="section" id="summary">
<h2>Summary<a class="headerlink" href="#summary" title="Permalink to this headline">¶</a></h2>
<p>In this chapter we finally solved the Euro problem, determining whether the data support the hypothesis that the coin is fair or biased.  We found that the answer depends on how we define “biased”.  And we summarized the results using a Bayes factor, which quantifies the strength of the evidence.</p>
<p>But the answer wasn’t satisfying because, in my opinion, the question wasn’t interesting.  Knowing whether the coin is biased is not useful unless it helps us make better predictions and better decisions.</p>
<p>As an example of a more interesting question, we looked at the “one-armed bandit” problem and a strategy for solving it, the Bayesian bandit algorithm, which tries to balance exploration and exploitation, that is, gathering more information and making the best use of the information we have.</p>
<p>As an exercise, you’ll have a chance to explore adaptive strategies for standardized testing.</p>
<p>Bayesian bandits and adaptive testing are examples of <a class="reference external" href="https://wiki.lesswrong.com/wiki/Bayesian_decision_theory">Bayesian decision theory</a>, which is the idea of using a posterior distribution as part of a decision-making process, often by choosing an action that minimizes the costs we expect on average (or maximizes a benefit).</p>
<p>The strategy we used in &lt;&lt;_MaximizingExpectedGain&gt;&gt; to bid on <em>The Price is Right</em> is another example.</p>
<p>These strategies demonstrate what I think is the biggest advantage of Bayesian methods over classical statistics.  When we represent knowledge in the form of probability distributions, Bayes’s theorem tells us how to change our beliefs as we get more data, and Bayesian decision theory tells us how to make that knowledge actionable.</p>
</div>
<div class="section" id="exercises">
<h2>Exercises<a class="headerlink" href="#exercises" title="Permalink to this headline">¶</a></h2>
<p><strong>Exercise:</strong> Standardized tests like the <a class="reference external" href="https://en.wikipedia.org/wiki/SAT">SAT</a> are often used as part of the admission process at colleges and universities.
The goal of the SAT is to measure the academic preparation of the test-takers; if it is accurate, their scores should reflect their actual ability in the domain of the test.</p>
<p>Until recently, tests like the SAT were taken with paper and pencil, but now students have the option of taking the test online.  In the online format, it is possible for the test to be “adaptive”, which means that it can <a class="reference external" href="https://www.nytimes.com/2018/04/05/education/learning/tests-act-sat.html">choose each question based on responses to previous questions</a>.</p>
<p>If a student gets the first few questions right, the test can challenge them with harder questions.  If they are struggling, it can give them easier questions.
Adaptive testing has the potential to be more “efficient”, meaning that with the same number of questions an adaptive test could measure the ability of a tester more precisely.</p>
<p>To see whether this is true, we will develop a model of an adaptive test and quantify the precision of its measurements.</p>
<p>Details of this exercise are in the notebook.</p>
<div class="cell tag_hide-cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">prob_correct</span><span class="p">(</span><span class="n">ability</span><span class="p">,</span> <span class="n">difficulty</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Probability of a correct response.&quot;&quot;&quot;</span>
    <span class="n">a</span> <span class="o">=</span> <span class="mi">100</span>
    <span class="n">c</span> <span class="o">=</span> <span class="mf">0.25</span>
    <span class="n">x</span> <span class="o">=</span> <span class="p">(</span><span class="n">ability</span> <span class="o">-</span> <span class="n">difficulty</span><span class="p">)</span> <span class="o">/</span> <span class="n">a</span>
    <span class="n">p</span> <span class="o">=</span> <span class="n">c</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">c</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">x</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">p</span>
</pre></div>
</div>
</div>
</div>
<p>I chose <code class="docutils literal notranslate"><span class="pre">a</span></code> to make the range of scores comparable to the SAT, which reports scores from 200 to 800.</p>
<p>Here’s what the logistic curve looks like for a question with difficulty 500 and a range of abilities.</p>
<div class="cell tag_hide-cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">abilities</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="mi">900</span><span class="p">)</span>
<span class="n">diff</span> <span class="o">=</span> <span class="mi">500</span>
<span class="n">ps</span> <span class="o">=</span> <span class="n">prob_correct</span><span class="p">(</span><span class="n">abilities</span><span class="p">,</span> <span class="n">diff</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell tag_hide-input tag_hide-cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">abilities</span><span class="p">,</span> <span class="n">ps</span><span class="p">)</span>
<span class="n">decorate</span><span class="p">(</span><span class="n">xlabel</span><span class="o">=</span><span class="s1">&#39;Ability&#39;</span><span class="p">,</span>
         <span class="n">ylabel</span><span class="o">=</span><span class="s1">&#39;Probability correct&#39;</span><span class="p">,</span>
         <span class="n">title</span><span class="o">=</span><span class="s1">&#39;Probability of correct answer, difficulty=500&#39;</span><span class="p">,</span>
         <span class="n">ylim</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mf">1.05</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/chap10_110_0.png" src="_images/chap10_110_0.png" />
</div>
</div>
<p>Someone with <code class="docutils literal notranslate"><span class="pre">ability=900</span></code> is nearly certain to get the right answer.
Someone with <code class="docutils literal notranslate"><span class="pre">ability=100</span></code> has about a 25% change of getting the right answer by guessing.</p>
</div>
<div class="section" id="simulating-the-test">
<h2>Simulating the Test<a class="headerlink" href="#simulating-the-test" title="Permalink to this headline">¶</a></h2>
<p>To simulate the test, we’ll use the same structure we used for the bandit strategy:</p>
<ul class="simple">
<li><p>A function called <code class="docutils literal notranslate"><span class="pre">play</span></code> that simulates a test-taker answering one question.</p></li>
<li><p>A function called <code class="docutils literal notranslate"><span class="pre">choose</span></code> that chooses the next question to pose.</p></li>
<li><p>A function called <code class="docutils literal notranslate"><span class="pre">update</span></code> that uses the outcome (a correct response or not) to update the estimate of the test-taker’s ability.</p></li>
</ul>
<p>Here’s <code class="docutils literal notranslate"><span class="pre">play</span></code>, which takes <code class="docutils literal notranslate"><span class="pre">ability</span></code> and <code class="docutils literal notranslate"><span class="pre">difficulty</span></code> as parameters.</p>
<div class="cell tag_hide-cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">play</span><span class="p">(</span><span class="n">ability</span><span class="p">,</span> <span class="n">difficulty</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Simulate a test-taker answering a question.&quot;&quot;&quot;</span>
    <span class="n">p</span> <span class="o">=</span> <span class="n">prob_correct</span><span class="p">(</span><span class="n">ability</span><span class="p">,</span> <span class="n">difficulty</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">()</span> <span class="o">&lt;</span> <span class="n">p</span>
</pre></div>
</div>
</div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">play</span></code> uses <code class="docutils literal notranslate"><span class="pre">prob_correct</span></code> to compute the probability of a correct answer and <code class="docutils literal notranslate"><span class="pre">np.random.random</span></code> to generate a random value between 0 and 1.  The return value is <code class="docutils literal notranslate"><span class="pre">True</span></code> for a correct response and <code class="docutils literal notranslate"><span class="pre">False</span></code> otherwise.</p>
<p>As a test, let’s simulate a test-taker with <code class="docutils literal notranslate"><span class="pre">ability=600</span></code> answering a question with <code class="docutils literal notranslate"><span class="pre">difficulty=500</span></code>.  The probability of a correct response is about 80%.</p>
<div class="cell tag_hide-cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">prob_correct</span><span class="p">(</span><span class="mi">600</span><span class="p">,</span> <span class="mi">500</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>0.7982939339725037
</pre></div>
</div>
</div>
</div>
<p>Suppose this person takes a test with 51 questions, all with the same difficulty, <code class="docutils literal notranslate"><span class="pre">500</span></code>.
We expect them to get about 80% of the questions correct.</p>
<p>Here’s the result of one simulation.</p>
<div class="cell tag_hide-cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">num_questions</span> <span class="o">=</span> <span class="mi">51</span>
<span class="n">outcomes</span> <span class="o">=</span> <span class="p">[</span><span class="n">play</span><span class="p">(</span><span class="mi">600</span><span class="p">,</span> <span class="mi">500</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_questions</span><span class="p">)]</span>
<span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">outcomes</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>0.803921568627451
</pre></div>
</div>
</div>
</div>
<p>We expect them to get about 80% of the questions right.</p>
<p>Now let’s suppose we don’t know the test-taker’s ability.  We can use the data we just generated to estimate it.
And that’s what we’ll do next.</p>
</div>
<div class="section" id="the-prior">
<h2>The Prior<a class="headerlink" href="#the-prior" title="Permalink to this headline">¶</a></h2>
<p>The SAT is designed so the distribution of scores is roughly normal, with mean 500 and standard deviation 100.
So the lowest score, 200, is three standard deviations below the mean, and the highest score, 800, is three standard deviations above.</p>
<p>We could use that distribution as a prior, but it would tend to cut off the low and high ends of the distribution.
Instead, I’ll inflate the standard deviation to 300, to leave open the possibility that <code class="docutils literal notranslate"><span class="pre">ability</span></code> can be less than 200 or more than 800.</p>
<p>Here’s a <code class="docutils literal notranslate"><span class="pre">Pmf</span></code> that represents the prior distribution.</p>
<div class="cell tag_remove-output tag_hide-cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">scipy.stats</span> <span class="kn">import</span> <span class="n">norm</span>

<span class="n">mean</span> <span class="o">=</span> <span class="mi">500</span>
<span class="n">std</span> <span class="o">=</span> <span class="mi">300</span>

<span class="n">qs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1000</span><span class="p">)</span>
<span class="n">ps</span> <span class="o">=</span> <span class="n">norm</span><span class="p">(</span><span class="n">mean</span><span class="p">,</span> <span class="n">std</span><span class="p">)</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">qs</span><span class="p">)</span>

<span class="n">prior</span> <span class="o">=</span> <span class="n">Pmf</span><span class="p">(</span><span class="n">ps</span><span class="p">,</span> <span class="n">qs</span><span class="p">)</span>
<span class="n">prior</span><span class="o">.</span><span class="n">normalize</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<p>And here’s what it looks like.</p>
<div class="cell tag_hide-input tag_hide-cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">prior</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">label</span><span class="o">=</span><span class="s1">&#39;std=300&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;C5&#39;</span><span class="p">)</span>

<span class="n">decorate</span><span class="p">(</span><span class="n">xlabel</span><span class="o">=</span><span class="s1">&#39;Ability&#39;</span><span class="p">,</span>
         <span class="n">ylabel</span><span class="o">=</span><span class="s1">&#39;PDF&#39;</span><span class="p">,</span>
         <span class="n">title</span><span class="o">=</span><span class="s1">&#39;Prior distribution of ability&#39;</span><span class="p">,</span>
         <span class="n">ylim</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mf">0.032</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/chap10_123_0.png" src="_images/chap10_123_0.png" />
</div>
</div>
</div>
<div class="section" id="id1">
<h2>The Update<a class="headerlink" href="#id1" title="Permalink to this headline">¶</a></h2>
<p>The following function takes a prior <code class="docutils literal notranslate"><span class="pre">Pmf</span></code> and the outcome of a single question, and updates the <code class="docutils literal notranslate"><span class="pre">Pmf</span></code> in place.</p>
<div class="cell tag_hide-cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">update_ability</span><span class="p">(</span><span class="n">pmf</span><span class="p">,</span> <span class="n">data</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Update the distribution of ability.&quot;&quot;&quot;</span>
    <span class="n">difficulty</span><span class="p">,</span> <span class="n">outcome</span> <span class="o">=</span> <span class="n">data</span>
    
    <span class="n">abilities</span> <span class="o">=</span> <span class="n">pmf</span><span class="o">.</span><span class="n">qs</span>
    <span class="n">ps</span> <span class="o">=</span> <span class="n">prob_correct</span><span class="p">(</span><span class="n">abilities</span><span class="p">,</span> <span class="n">difficulty</span><span class="p">)</span>
    
    <span class="k">if</span> <span class="n">outcome</span><span class="p">:</span>
        <span class="n">pmf</span> <span class="o">*=</span> <span class="n">ps</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">pmf</span> <span class="o">*=</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">ps</span>
        
    <span class="n">pmf</span><span class="o">.</span><span class="n">normalize</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">data</span></code> is a tuple that contains the difficulty of a question and the outcome: <code class="docutils literal notranslate"><span class="pre">True</span></code> if the response was correct and <code class="docutils literal notranslate"><span class="pre">False</span></code> otherwise.</p>
<p>As a test, let’s do an update based on the outcomes we simulated previously, based on a person with <code class="docutils literal notranslate"><span class="pre">ability=600</span></code> answering 51 questions with <code class="docutils literal notranslate"><span class="pre">difficulty=500</span></code>.</p>
<div class="cell tag_hide-cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">actual_600</span> <span class="o">=</span> <span class="n">prior</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>

<span class="k">for</span> <span class="n">outcome</span> <span class="ow">in</span> <span class="n">outcomes</span><span class="p">:</span>
    <span class="n">data</span> <span class="o">=</span> <span class="p">(</span><span class="mi">500</span><span class="p">,</span> <span class="n">outcome</span><span class="p">)</span>
    <span class="n">update_ability</span><span class="p">(</span><span class="n">actual_600</span><span class="p">,</span> <span class="n">data</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Here’s what the posterior distribution looks like.</p>
<div class="cell tag_hide-cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">actual_600</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">color</span><span class="o">=</span><span class="s1">&#39;C4&#39;</span><span class="p">)</span>

<span class="n">decorate</span><span class="p">(</span><span class="n">xlabel</span><span class="o">=</span><span class="s1">&#39;Ability&#39;</span><span class="p">,</span>
         <span class="n">ylabel</span><span class="o">=</span><span class="s1">&#39;PDF&#39;</span><span class="p">,</span>
         <span class="n">title</span><span class="o">=</span><span class="s1">&#39;Posterior distribution of ability&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/chap10_129_0.png" src="_images/chap10_129_0.png" />
</div>
</div>
<p>The posterior mean is pretty close to the test-taker’s actual ability, which is 600.</p>
<div class="cell tag_hide-cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">actual_600</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>604.3325737356816
</pre></div>
</div>
</div>
</div>
<p>If we run this simulation again, we’ll get different results.</p>
</div>
<div class="section" id="adaptation">
<h2>Adaptation<a class="headerlink" href="#adaptation" title="Permalink to this headline">¶</a></h2>
<p>Now let’s simulate an adaptive test.
I’ll use the following function to choose questions, starting with the simplest strategy: all questions have the same difficulty.</p>
<div class="cell tag_hide-cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">choose</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">belief</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Choose the difficulty of the next question.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="mi">500</span>
</pre></div>
</div>
</div>
</div>
<p>As parameters, <code class="docutils literal notranslate"><span class="pre">choose</span></code> takes <code class="docutils literal notranslate"><span class="pre">i</span></code>, which is the index of the question, and <code class="docutils literal notranslate"><span class="pre">belief</span></code>, which is a <code class="docutils literal notranslate"><span class="pre">Pmf</span></code> representing the posterior distribution of <code class="docutils literal notranslate"><span class="pre">ability</span></code>, based on responses to previous questions.</p>
<p>This version of <code class="docutils literal notranslate"><span class="pre">choose</span></code> doesn’t use these parameters; they are there so we can test other strategies (see the exercises at the end of the chapter).</p>
<p>The following function simulates a person taking a test, given that we know their actual ability.</p>
<div class="cell tag_hide-cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">simulate_test</span><span class="p">(</span><span class="n">actual_ability</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Simulate a person taking a test.&quot;&quot;&quot;</span>
    <span class="n">belief</span> <span class="o">=</span> <span class="n">prior</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
    <span class="n">trace</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;difficulty&#39;</span><span class="p">,</span> <span class="s1">&#39;outcome&#39;</span><span class="p">])</span>

    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_questions</span><span class="p">):</span>
        <span class="n">difficulty</span> <span class="o">=</span> <span class="n">choose</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">belief</span><span class="p">)</span>
        <span class="n">outcome</span> <span class="o">=</span> <span class="n">play</span><span class="p">(</span><span class="n">actual_ability</span><span class="p">,</span> <span class="n">difficulty</span><span class="p">)</span>
        <span class="n">data</span> <span class="o">=</span> <span class="p">(</span><span class="n">difficulty</span><span class="p">,</span> <span class="n">outcome</span><span class="p">)</span>
        <span class="n">update_ability</span><span class="p">(</span><span class="n">belief</span><span class="p">,</span> <span class="n">data</span><span class="p">)</span>
        <span class="n">trace</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">difficulty</span><span class="p">,</span> <span class="n">outcome</span>
        
    <span class="k">return</span> <span class="n">belief</span><span class="p">,</span> <span class="n">trace</span>
</pre></div>
</div>
</div>
</div>
<p>The return values are a <code class="docutils literal notranslate"><span class="pre">Pmf</span></code> representing the posterior distribution of ability and a <code class="docutils literal notranslate"><span class="pre">DataFrame</span></code> containing the difficulty of the questions and the outcomes.</p>
<p>Here’s an example, again for a test-taker with <code class="docutils literal notranslate"><span class="pre">ability=600</span></code>.</p>
<div class="cell tag_hide-cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">belief</span><span class="p">,</span> <span class="n">trace</span> <span class="o">=</span> <span class="n">simulate_test</span><span class="p">(</span><span class="mi">600</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>We can use the trace to see how many responses were correct.</p>
<div class="cell tag_hide-cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">trace</span><span class="p">[</span><span class="s1">&#39;outcome&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>42
</pre></div>
</div>
</div>
</div>
<p>And here’s what the posterior looks like.</p>
<div class="cell tag_hide-cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">belief</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">color</span><span class="o">=</span><span class="s1">&#39;C4&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;ability=600&#39;</span><span class="p">)</span>

<span class="n">decorate</span><span class="p">(</span><span class="n">xlabel</span><span class="o">=</span><span class="s1">&#39;Ability&#39;</span><span class="p">,</span>
         <span class="n">ylabel</span><span class="o">=</span><span class="s1">&#39;PDF&#39;</span><span class="p">,</span>
         <span class="n">title</span><span class="o">=</span><span class="s1">&#39;Posterior distribution of ability&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/chap10_142_0.png" src="_images/chap10_142_0.png" />
</div>
</div>
<p>Again, the posterior distribution represents a pretty good estimate of the test-taker’s actual ability.</p>
</div>
<div class="section" id="quantifying-precision">
<h2>Quantifying Precision<a class="headerlink" href="#quantifying-precision" title="Permalink to this headline">¶</a></h2>
<p>To quantify the precision of the estimates, I’ll use the standard deviation of the posterior distribution.  The standard deviation measures the spread of the distribution, so higher value indicates more uncertainty about the ability of the test-taker.</p>
<p>In the previous example, the standard deviation of the posterior distribution is about 40.</p>
<div class="cell tag_hide-cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">belief</span><span class="o">.</span><span class="n">mean</span><span class="p">(),</span> <span class="n">belief</span><span class="o">.</span><span class="n">std</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(618.6942050450824, 40.08554296596485)
</pre></div>
</div>
</div>
</div>
<p>For an exam where all questions have the same difficulty, the precision of the estimate depends strongly on the ability of the test-taker.  To show that, I’ll loop through a range of abilities and simulate a test using the version of <code class="docutils literal notranslate"><span class="pre">choice</span></code> that always returns <code class="docutils literal notranslate"><span class="pre">difficulty=500</span></code>.</p>
<div class="cell tag_hide-cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">actual_abilities</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">200</span><span class="p">,</span> <span class="mi">800</span><span class="p">)</span>
<span class="n">results</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;ability&#39;</span><span class="p">,</span> <span class="s1">&#39;posterior_std&#39;</span><span class="p">])</span>
<span class="n">series</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">(</span><span class="n">index</span><span class="o">=</span><span class="n">actual_abilities</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">float</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;std&#39;</span><span class="p">)</span>

<span class="k">for</span> <span class="n">actual_ability</span> <span class="ow">in</span> <span class="n">actual_abilities</span><span class="p">:</span>
    <span class="n">belief</span><span class="p">,</span> <span class="n">trace</span> <span class="o">=</span> <span class="n">simulate_test</span><span class="p">(</span><span class="n">actual_ability</span><span class="p">)</span>
    <span class="n">series</span><span class="p">[</span><span class="n">actual_ability</span><span class="p">]</span> <span class="o">=</span> <span class="n">belief</span><span class="o">.</span><span class="n">std</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<p>The following plot shows the standard deviation of the posterior distribution for one simulation at each level of ability.</p>
<p>The results are noisy, so I also plot a curve fitted to the data by <a class="reference external" href="https://en.wikipedia.org/wiki/Local_regression">local regression</a>.</p>
<div class="cell tag_hide-input tag_hide-cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">utils</span> <span class="kn">import</span> <span class="n">plot_series_lowess</span>

<span class="n">plot_series_lowess</span><span class="p">(</span><span class="n">series</span><span class="p">,</span> <span class="s1">&#39;C1&#39;</span><span class="p">)</span>

<span class="n">decorate</span><span class="p">(</span><span class="n">xlabel</span><span class="o">=</span><span class="s1">&#39;Actual ability&#39;</span><span class="p">,</span>
         <span class="n">ylabel</span><span class="o">=</span><span class="s1">&#39;Standard deviation of posterior&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/chap10_149_0.png" src="_images/chap10_149_0.png" />
</div>
</div>
<p>The test is most precise for people with ability between <code class="docutils literal notranslate"><span class="pre">500</span></code> and <code class="docutils literal notranslate"><span class="pre">600</span></code>, less precise for people at the high end of the range, and even worse for people at the low end.</p>
<p>When all the questions have difficulty <code class="docutils literal notranslate"><span class="pre">500</span></code>, a person with <code class="docutils literal notranslate"><span class="pre">ability=800</span></code> has a high probability of getting them right.  So when they do, we don’t learn very much about them.</p>
<p>If the test includes questions with a range of difficulty, it provides more information about people at the high and low ends of the range.</p>
<p>As an exercise at the end of the chapter, you’ll have a chance to try out other strategies, including adaptive strategies that choose each question based on previous outcomes.</p>
</div>
<div class="section" id="discriminatory-power">
<h2>Discriminatory Power<a class="headerlink" href="#discriminatory-power" title="Permalink to this headline">¶</a></h2>
<p>In the previous section we used the standard deviation of the posterior distribution to quantify the precision of the estimates.  Another way to describe the performance of the test (as opposed to the performance of the test-takers) is to measure “discriminatory power”, which is the ability of the test to distinguish correctly between test-takers with different ability.</p>
<p>To measure discriminatory power, I’ll simulate a person taking the test 100 times; after each simulation, I’ll use the mean of the posterior distribution as their “score”.</p>
<div class="cell tag_hide-cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">sample_posterior</span><span class="p">(</span><span class="n">actual_ability</span><span class="p">,</span> <span class="n">iters</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Simulate multiple tests and compute posterior means.</span>
<span class="sd">    </span>
<span class="sd">    actual_ability: number</span>
<span class="sd">    iters: number of simulated tests</span>
<span class="sd">    </span>
<span class="sd">    returns: array of scores</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">scores</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">iters</span><span class="p">):</span>
        <span class="n">belief</span><span class="p">,</span> <span class="n">trace</span> <span class="o">=</span> <span class="n">simulate_test</span><span class="p">(</span><span class="n">actual_ability</span><span class="p">)</span>
        <span class="n">score</span> <span class="o">=</span> <span class="n">belief</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
        <span class="n">scores</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">score</span><span class="p">)</span>
        
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">scores</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Here are samples of scores for people with several levels of ability.</p>
<div class="cell tag_hide-cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">sample_500</span> <span class="o">=</span> <span class="n">sample_posterior</span><span class="p">(</span><span class="mi">500</span><span class="p">,</span> <span class="n">iters</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell tag_hide-cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">sample_600</span> <span class="o">=</span> <span class="n">sample_posterior</span><span class="p">(</span><span class="mi">600</span><span class="p">,</span> <span class="n">iters</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell tag_hide-cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">sample_700</span> <span class="o">=</span> <span class="n">sample_posterior</span><span class="p">(</span><span class="mi">700</span><span class="p">,</span> <span class="n">iters</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell tag_hide-cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">sample_800</span> <span class="o">=</span> <span class="n">sample_posterior</span><span class="p">(</span><span class="mi">800</span><span class="p">,</span> <span class="n">iters</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Here’s what the distributions of scores look like.</p>
<div class="cell tag_hide-cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">empiricaldist</span> <span class="kn">import</span> <span class="n">Cdf</span>

<span class="n">cdf_500</span> <span class="o">=</span> <span class="n">Cdf</span><span class="o">.</span><span class="n">from_seq</span><span class="p">(</span><span class="n">sample_500</span><span class="p">)</span>
<span class="n">cdf_600</span> <span class="o">=</span> <span class="n">Cdf</span><span class="o">.</span><span class="n">from_seq</span><span class="p">(</span><span class="n">sample_600</span><span class="p">)</span>
<span class="n">cdf_700</span> <span class="o">=</span> <span class="n">Cdf</span><span class="o">.</span><span class="n">from_seq</span><span class="p">(</span><span class="n">sample_700</span><span class="p">)</span>
<span class="n">cdf_800</span> <span class="o">=</span> <span class="n">Cdf</span><span class="o">.</span><span class="n">from_seq</span><span class="p">(</span><span class="n">sample_800</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell tag_hide-cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">cdf_500</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">label</span><span class="o">=</span><span class="s1">&#39;ability=500&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;C1&#39;</span><span class="p">,</span>
            <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;dashed&#39;</span><span class="p">)</span>
<span class="n">cdf_600</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">label</span><span class="o">=</span><span class="s1">&#39;ability=600&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;C3&#39;</span><span class="p">)</span>
<span class="n">cdf_700</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">label</span><span class="o">=</span><span class="s1">&#39;ability=700&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;C2&#39;</span><span class="p">,</span>
            <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;dashed&#39;</span><span class="p">)</span>
<span class="n">cdf_800</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">label</span><span class="o">=</span><span class="s1">&#39;ability=800&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;C0&#39;</span><span class="p">)</span>

<span class="n">decorate</span><span class="p">(</span><span class="n">xlabel</span><span class="o">=</span><span class="s1">&#39;Test score&#39;</span><span class="p">,</span>
         <span class="n">ylabel</span><span class="o">=</span><span class="s1">&#39;CDF&#39;</span><span class="p">,</span>
         <span class="n">title</span><span class="o">=</span><span class="s1">&#39;Sampling distribution of test scores&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/chap10_160_0.png" src="_images/chap10_160_0.png" />
</div>
</div>
<p>On average, people with higher ability get higher scores, but anyone can have a bad day, or a good day, so there is some overlap between the distributions.</p>
<p>For people with ability between <code class="docutils literal notranslate"><span class="pre">500</span></code> and <code class="docutils literal notranslate"><span class="pre">600</span></code>, where the precision of the test is highest, the discriminatory power of the test is also high.</p>
<p>If people with abilities <code class="docutils literal notranslate"><span class="pre">500</span></code> and <code class="docutils literal notranslate"><span class="pre">600</span></code> take the test, it is almost certain that the person with higher ability will get a higher score.</p>
<div class="cell tag_hide-cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">sample_600</span> <span class="o">&gt;</span> <span class="n">sample_500</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>0.98
</pre></div>
</div>
</div>
</div>
<p>Between people with abilities <code class="docutils literal notranslate"><span class="pre">600</span></code> and <code class="docutils literal notranslate"><span class="pre">700</span></code>, it is less certain.</p>
<div class="cell tag_hide-cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">sample_700</span> <span class="o">&gt;</span> <span class="n">sample_600</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>0.95
</pre></div>
</div>
</div>
</div>
<p>And between people with abilities <code class="docutils literal notranslate"><span class="pre">700</span></code> and <code class="docutils literal notranslate"><span class="pre">800</span></code>, it is not certain at all.</p>
<div class="cell tag_hide-cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">sample_800</span> <span class="o">&gt;</span> <span class="n">sample_700</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>0.85
</pre></div>
</div>
</div>
</div>
<p>But remember that these results are based on a test where all questions are equally difficult.
If you do the exercises at the end of the chapter, you’ll see that the performance of the test is better if it includes questions with a range of difficulties, and even better if the test it is adaptive.</p>
<p>Go back and modify <code class="docutils literal notranslate"><span class="pre">choose</span></code>, which is the function that chooses the difficulty of the next question.</p>
<ol class="simple">
<li><p>Write a version of <code class="docutils literal notranslate"><span class="pre">choose</span></code> that returns a range of difficulties by using <code class="docutils literal notranslate"><span class="pre">i</span></code> as an index into a sequence of difficulties.</p></li>
<li><p>Write a version of <code class="docutils literal notranslate"><span class="pre">choose</span></code> that is adaptive, so it choose the difficulty of the next question based <code class="docutils literal notranslate"><span class="pre">belief</span></code>, which is the posterior distribution of the test-taker’s ability, based on the outcome of previous responses.</p></li>
</ol>
<p>For both new versions, run the simulations again to quantify the precision of the test and its discriminatory power.</p>
<p>For the first version of <code class="docutils literal notranslate"><span class="pre">choose</span></code>, what is the ideal distribution of difficulties?</p>
<p>For the second version, what is the adaptive strategy that maximizes the precision of the test over the range of abilities?</p>
<div class="cell tag_hide-cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Solution</span>

<span class="c1"># I don&#39;t know what the optimal distribution of questions</span>
<span class="c1"># is, but my guess is that it would follow the distribution</span>
<span class="c1"># of ability.</span>

<span class="c1"># But as a simplification, I used a uniform distribution</span>
<span class="c1"># from 200 to 800.</span>

<span class="c1"># It works pretty well (and substantially better than the</span>
<span class="c1"># test where all questions are equally difficult.)</span>

<span class="n">num_questions</span> <span class="o">=</span> <span class="mi">51</span>
<span class="n">difficulties</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">200</span><span class="p">,</span> <span class="mi">800</span><span class="p">,</span> <span class="n">num_questions</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">choose</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">belief</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Choose the difficulty of the next question.</span>
<span class="sd">    </span>
<span class="sd">    i: index from [0..num_questions-1]</span>
<span class="sd">    belief: Pmf representing current estimate of ability</span>
<span class="sd">    </span>
<span class="sd">    returns: difficulty</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">difficulties</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<div class="cell tag_hide-cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Solution</span>

<span class="c1"># I suspect that the optimal strategy is to choose</span>
<span class="c1"># a question so that the test-taker has a 50% chance</span>
<span class="c1"># of getting it right.</span>

<span class="c1"># As rough approximation of that, I choose a question</span>
<span class="c1"># with difficulty equal to the posterior mean of ability.</span>

<span class="c1"># It works quite well (and substantially better than</span>
<span class="c1"># the previous version).</span>

<span class="k">def</span> <span class="nf">choose</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">belief</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Choose the difficulty of the next question.</span>
<span class="sd">    </span>
<span class="sd">    i: index from [0..num_questions-1]</span>
<span class="sd">    belief: Pmf representing current estimate of ability</span>
<span class="sd">    </span>
<span class="sd">    returns: difficulty</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">belief</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        
            



<div class='prev-next-bottom'>
    
    <div id="prev">
        <a class="left-prev" href="chap09.html" title="previous page">
            <i class="prevnext-label fas fa-angle-left"></i>
            <div class="prevnext-info">
                <p class="prevnext-label">previous</p>
                <p class="prevnext-title">Decision Analysis</p>
            </div>
        </a>
    </div>
     <div id="next">
        <a class="right-next" href="chap11.html" title="next page">
            <div class="prevnext-info">
                <p class="prevnext-label">next</p>
                <p class="prevnext-title">Comparison</p>
            </div>
            <i class="prevnext-label fas fa-angle-right"></i>
        </a>
     </div>

</div>
        
        </div>
    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By Allen B. Downey<br/>
        
            &copy; Copyright 2021.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>
  
  <script src="_static/js/index.1c5a1a01449ed65a7b51.js"></script>

  
  </body>
</html>