{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conjugate Priors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Think Bayes, Second Edition\n",
    "\n",
    "Copyright 2020 Allen B. Downey\n",
    "\n",
    "License: [Attribution-NonCommercial-ShareAlike 4.0 International (CC BY-NC-SA 4.0)](https://creativecommons.org/licenses/by-nc-sa/4.0/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# If we're running on Colab, install empiricaldist\n",
    "# https://pypi.org/project/empiricaldist/\n",
    "\n",
    "import sys\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "    !pip install empiricaldist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get utils.py\n",
    "\n",
    "import os\n",
    "\n",
    "if not os.path.exists('utils.py'):\n",
    "    !wget https://github.com/AllenDowney/ThinkBayes2/raw/master/soln/utils.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from utils import set_pyplot_params\n",
    "set_pyplot_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Grid algorithms... three alternative:\n",
    "\n",
    "1. Conjugate priors\n",
    "\n",
    "2. ABC\n",
    "\n",
    "3. MCMC\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Poisson processes\n",
    "\n",
    "In Chapter xxx, we solved the World Cup problem using a Poisson process to model goals in a soccer game as random events that are equally likely to occur at any point during a game.\n",
    "\n",
    "We used a gamma distribution to represent the prior distribution of $\\lambda$, the goal-scoring rate.  And we used a Poisson distribution to compute the probability of the data, the observed number of goals in a game.\n",
    "\n",
    "Here's the prior distribution again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "alpha = 1.4\n",
    "lams = np.linspace(0, 10, 101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import make_pmf\n",
    "from scipy.stats import gamma\n",
    "\n",
    "dist = gamma(alpha)\n",
    "prior = make_pmf(dist, lams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here's the likelihood of scoring 4 goals for each possible value of `lam`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import poisson\n",
    "\n",
    "k = 4\n",
    "likelihood = poisson(lams).pmf(k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We did the update by multiplying the prior by the likelihood and normalizing the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "posterior = prior * likelihood\n",
    "posterior.normalize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here's the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from utils import decorate\n",
    "\n",
    "def decorate_rate(title=''):\n",
    "    decorate(xlabel='Goal scoring rate (lam)',\n",
    "             ylabel='PMF',\n",
    "             title=title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prior.plot(label='prior', color='C5')\n",
    "posterior.plot(label='posterior', color='C1')\n",
    "\n",
    "decorate_rate('Posterior distribution')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The conjugate prior\n",
    "\n",
    "At the time, I said that I chose the gamma distribution for three reasons:\n",
    "\n",
    "1. The goal scoring rate is continuous and cannot be less than 0, and the gamma distribution is appropriate for this kind of quantity.\n",
    "\n",
    "2. The gamma distribution has only one parameter, `alpha`, which is the mean.  So it's easy to construct a gamma distribution with the mean we want.\n",
    "\n",
    "3. The shape of the Gamma distribution is a reasonable choice, given what we know about soccer.\n",
    "\n",
    "And I said there was a fourth reason that I would reveal later.\n",
    "Well, now is the time.\n",
    "\n",
    "The other reason I chose the gamma distribution is that it is the \"conjugate prior\" of the Poisson distribution, so-called because the two distributions are connected or coupled, which is what \"conjugate\" means.\n",
    "\n",
    "In the next section I'll explain *how* they are connected, but first I'll show you the consequence of this connection, which is that there is a remarkably simple way to compute the posterior distribution.\n",
    "\n",
    "However, in order to demonstrate it, we have to switch from the one-parameter version of the gamma distribution to the two-parameter version.  Since the first parameter is called `alpha`, you might guess that the second parameter is called `beta`.\n",
    "\n",
    "The following function takes `alpha` and `beta` and makes an object that represents a gamma distribution with those parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_gamma_dist(alpha, beta):\n",
    "    \"\"\"Makes a gamma object.\n",
    "    \n",
    "    alpha: shape parameter\n",
    "    beta: scale parameter\n",
    "    \n",
    "    returns: gamma object\n",
    "    \"\"\"\n",
    "    dist = gamma(alpha, scale=1/beta)\n",
    "    dist.alpha = alpha\n",
    "    dist.beta = beta\n",
    "    return dist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's the prior distribution with `alpha=1.4` again and `beta=1`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 1.4\n",
    "beta = 1\n",
    "\n",
    "prior_gamma = make_gamma_dist(alpha, beta)\n",
    "prior_gamma.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to do a Bayesian update, I claim, all we have to do is make a gamma distribution with parameters `alpha+k` and `beta+1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_gamma(prior, data):\n",
    "    \"\"\"Update a gamma prior.\n",
    "    \n",
    "    prior: Pmf\n",
    "    data: tuple of (k goals, t games)\n",
    "    \n",
    "    returns: scipy.stats.dist\n",
    "    \"\"\"\n",
    "    k, t = data\n",
    "    alpha = prior.alpha + k\n",
    "    beta = prior.beta + t\n",
    "    return make_gamma_dist(alpha, beta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's how we update it with the data, which is 4 goals in 1 game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = 4, 1\n",
    "posterior_gamma = update_gamma(prior_gamma, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After all the work we did in Chapter 7, it might seem absurd that we can do a Bayesian update by adding two pairs of numbers.\n",
    "So let's confirm that it works.\n",
    "\n",
    "I'll evaluate the posterior distribution for each quantity in `lams` and put the results in a `Pmf` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "posterior_conjugate = make_pmf(posterior_gamma, lams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So `posterior_conjugate` contains a discrete approximation of the posterior distribution we just computed using the conjugate prior.\n",
    "The following figure shows the result along with the posterior we computed using the grid algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "posterior.plot(label='grid posterior', color='C1')\n",
    "posterior_conjugate.plot(label='conjugate posterior', \n",
    "                         color='C4', linestyle='dotted')\n",
    "\n",
    "decorate_rate('Posterior distribution')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "They are the same other than small differences due to floating-point approximations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.allclose(posterior, posterior_conjugate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What the Actual?\n",
    "\n",
    "To understand how that works, we'll write the PDF of the gamma prior and the PMF of the Poisson likelihood, then multiply them together, because that's what the Bayesian update does.\n",
    "We'll see that the result is a gamma distribution, and we'll derive its parameters.\n",
    "\n",
    "Here's the PDF of the gamma prior, which is the probability density for each value of $\\lambda$, given parameters $\\alpha$ and $\\beta$:\n",
    "\n",
    "$$\\lambda^{\\alpha-1}e^{-\\beta \\lambda}$$\n",
    "\n",
    "I have omitted the normalizing factor; since we are planning to normalize the posterior distribution anyway, we don't really need it.\n",
    "\n",
    "Now suppose a team scores $k$ goals in $t$ games.\n",
    "The probability of this data is given by PMF of the Poisson distribution, which is a function of $k$ with $\\lambda$ and $t$ as a parameter.\n",
    "\n",
    "$$\\lambda^k e^{-\\lambda t}$$\n",
    "\n",
    "Again, I have omitted the normalizing factor, which makes it clearer that the gamma and Poisson distributions have the same functional form.\n",
    "When we multiply them together, we can pair up the factors and add up the exponents.\n",
    "The result is the unnormalized posterior distribution,\n",
    "\n",
    "$$\\lambda^{\\alpha-1+k} e^{-(\\beta + t) \\lambda}$$\n",
    "\n",
    "which we can recognize as an unnormalized gamma distribution with parameters $\\alpha + k$ and $\\beta + t$.\n",
    "\n",
    "This derivation provides insight into what the parameters of the posterior distribution mean: $\\alpha$ reflects the number of events that have occurred; $\\beta$ reflects the elapsed time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binomial likelihood\n",
    "\n",
    "As a second example, let's look again at the Euro problem.\n",
    "When we solved it with a grid algorithm, we started with a uniform prior:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import make_uniform\n",
    "\n",
    "xs = np.linspace(0, 1, 101)\n",
    "uniform = make_uniform(xs, 'uniform')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We used the binomial distribution to compute the likelihood of the data, which was 140 heads out of 250 attempts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import binom\n",
    "\n",
    "k, n = 140, 250\n",
    "xs = uniform.qs\n",
    "\n",
    "likelihood = binom.pmf(k, n, xs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we computed the posterior distribution in the usual way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "posterior = uniform * likelihood\n",
    "posterior.normalize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can solve this problem more efficiently using the conjugate prior of the binomial distribution, which is the beta distribution.\n",
    "\n",
    "The beta distribution is bounded between 0 and 1, so it works well for representing the distribution of a probability like `x`.\n",
    "It has two parameters, called `alpha` and `beta`, that determine the shape of the distribution.\n",
    "\n",
    "SciPy provides an object called `beta` that represents a beta distribution.\n",
    "The following function takes `alpha` and `beta` and returns a new `beta` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats\n",
    "\n",
    "def make_beta(alpha, beta):\n",
    "    \"\"\"Makes a beta object.\n",
    "    \n",
    "    alpha: shape parameter\n",
    "    beta: scale parameter\n",
    "    \n",
    "    returns: scipt.stats.beta object\n",
    "    \"\"\"\n",
    "    dist = scipy.stats.beta(alpha, beta)\n",
    "    dist.alpha = alpha\n",
    "    dist.beta = beta\n",
    "    return dist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It turns out that the uniform distribution, which we used as a prior, is the beta distribution with parameters `alpha=1` and `beta=1`.\n",
    "So we can make a `beta` object that represents a uniform distribution, like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 1\n",
    "beta = 1\n",
    "\n",
    "prior_beta = make_beta(alpha, beta)\n",
    "prior_beta.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's figure out how to do the update.  As in the previous example, we'll write the PDF of the prior distribution and the PMF of the likelihood function, and multiply them together.  We'll see that the product has the same form as the prior, and we'll derive its parameters.\n",
    "\n",
    "Here is the PDF of the beta distribution, which is a function of $x$ with $\\alpha$ and $\\beta$ as parameters.\n",
    "\n",
    "$$x^{\\alpha-1} (1-x)^{\\beta-1}$$\n",
    "\n",
    "Again, I have omitted the normalizing factor, which we don't need because we are going to normalize the distribution after the update.\n",
    "\n",
    "The beta PDF might look familiar: the PMF of the binomial distribution has the same form, but we interpret it as a function of $k$ with $n$ and $x$ as parameters.\n",
    "\n",
    "$$x^{k} (1-x)^{n-k}$$\n",
    "\n",
    "Again, I have omitted the normalizing factor.\n",
    "Now when we multiply the beta prior and the binomial likelihood, the result is\n",
    "\n",
    "$$x^{\\alpha-1+k} (1-x)^{\\beta-1+n-k}$$\n",
    "\n",
    "which we recognize as an unnormalized beta distribution with parameters $\\alpha+k$ and $\\beta+n-k$.\n",
    "\n",
    "So, to update the beta prior, all we have to do is make a beta distribution with parameters `alpha+k` and `beta+n-k`, as in the following function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_beta(prior, data):\n",
    "    \"\"\"Update a beta distribution.\n",
    "    \n",
    "    prior: SciPy beta object\n",
    "    data: number of successes and number of trials (k, n)\n",
    "    \n",
    "    returns: SciPy beta object\n",
    "    \"\"\"\n",
    "    k, n = data\n",
    "    alpha = prior.alpha + k\n",
    "    beta = prior.beta + n - k\n",
    "    return make_beta(alpha, beta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, the conjugate prior gives us insight into the meaning of the parameters; $\\alpha$ is related to the number of observed successes; $\\beta$ is related to the number of failures.\n",
    "\n",
    "Here's how we do the update with the observed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = 140, 250\n",
    "posterior_beta = update_beta(prior_beta, data)\n",
    "posterior_beta.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To confirm that it works, I'll evaluate the posterior distribution for the possible values of `xs` and put the results in a `Pmf`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "posterior_conjugate = make_pmf(posterior_beta, xs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can compare the posterior distribution we just computed with the results from the grid algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def decorate_euro(title):\n",
    "    decorate(xlabel='Proportion of heads (x)',\n",
    "             ylabel='Probability',\n",
    "             title=title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "posterior.plot(label='grid posterior', color='C1')\n",
    "posterior_conjugate.plot(label='conjugate posterior',\n",
    "                        color='C4', linestyle='dotted')\n",
    "\n",
    "decorate_euro(title='Posterior distribution of x')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "They are the same other than small differences due to floating-point approximations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.allclose(posterior, posterior_conjugate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lions and tigers and bears\n",
    "\n",
    "Suppose we visit a wild animal preserve where we know that the only animals are lions and tigers and bears, but we don't know how many of each there are.\n",
    "\n",
    "During the tour, we see 3 lions, 2 tigers, and one bear. Assuming that every animal had an equal chance to appear in our sample, what is the probability that the next animal we see is a bear?\n",
    "\n",
    "To answer this question, we'll use the data to estimate the prevalence of each species, that is, what fraction of the animals belong to each species.\n",
    "\n",
    "If we know the prevalences, we can use the multinomial distribution to compute the probability of the data.\n",
    "For example, suppose we know that the fraction of lions, tigers, and bears is 0.4, 0.3, and 0.3, respectively.\n",
    "\n",
    "In that case the probability of the data is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import multinomial\n",
    "\n",
    "data = np.array([3, 2, 1])\n",
    "n = data.sum()\n",
    "ps = 0.4, 0.3, 0.3\n",
    "\n",
    "multinomial.pmf(data, n, ps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we could choose a prior for the prevalences and do a Bayesian update using the multinomial distribution to compute the probability of the data.\n",
    "\n",
    "But there's an easier way, because the multinomial distribution has a conjugate prior: the Dirichlet distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Dirichlet distribution\n",
    "\n",
    "The Dirichlet distribution is a multivariate distribution, like the multivariate normal distribution we used in Chapter xxx to describe the distribution of penguin measurements.  \n",
    "\n",
    "In that example, the quantities in the distribution are pairs of flipper length and culmen length, and the parameters of the distribution are a vector of means and a matrix of covariances.\n",
    "\n",
    "In a Dirichlet distribution, the quantities are vectors of probabilities, $\\pmb{x}$, and the parameter is a vector, $\\pmb{\\alpha}$.\n",
    "\n",
    "An example will make that clearer.  SciPy provides a `dirichlet` object that represents a Dirichlet distribution.\n",
    "Here's an instance with $\\pmb{\\alpha} = 1, 2, 3$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import dirichlet\n",
    "\n",
    "alpha = np.array([1, 2, 3])\n",
    "dist = dirichlet(alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we provided three parameters, the result is a distribution of three variables.\n",
    "If we draw a random value from this distribution, like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist.rvs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is an array of three values.\n",
    "\n",
    "The values are bounded between 0 and 1, so they can be interpreted as probabilities.\n",
    "Even better, they always add up to 1, so they can be interpreted as the probabilities of a set of outcomes that are mutually exclusive and collectively exhaustive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist.rvs().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what the distributions of these values look like.  I'll draw 1000 random vectors from this distribution, like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = dist.rvs(1000)\n",
    "sample.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is an array with 1000 rows and three columns.  I'll compute the `Cdf` of the values in each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from empiricaldist import Cdf\n",
    "\n",
    "cdfs = [Cdf.from_seq(col) for col in sample.transpose()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the marginal distributions of the three variables.  Here's what they look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, cdf in enumerate(cdfs):\n",
    "    label = f'Column {i}'\n",
    "    cdf.plot(label=label)\n",
    "    \n",
    "decorate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Column 0, which corresponds to the lowest parameter, contains the lowest probabilities.\n",
    "Column 2, which corresponds to the highest parameter, contains the highest probabilities.\n",
    "\n",
    "We can compute the means of the marginal distributions like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha / alpha.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the `dist` object can do it for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare those to the means of the samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample.mean(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's pretty close.\n",
    "\n",
    "As it turns out, these marginal distributions are beta distributions.\n",
    "The following function takes the vector of parameters, `alpha`, and computes the marginal distribution of variable `i`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def marginal_beta(alpha, i):\n",
    "    \"\"\"Compute the ith marginal of a Dirichlet distribution.\n",
    "    \n",
    "    alpha: vector parameter of the distribution\n",
    "    i: index of the marginal\n",
    "    \"\"\"\n",
    "    total = alpha.sum()\n",
    "    return make_beta(alpha[i], total-alpha[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following plot uses this function to plot the marginal beta distributions and compares them to the distributions of the columns in the sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = np.linspace(0, 1, 101)\n",
    "\n",
    "for i, cdf in enumerate(cdfs):\n",
    "    label = f'Column {i}'\n",
    "    \n",
    "    marginal = marginal_beta(alpha, i)\n",
    "    make_pmf(marginal, xs).make_cdf().plot(color='C5')\n",
    "    cdf.plot(label=label, linestyle='dotted')\n",
    "\n",
    "decorate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So that checks out.\n",
    "\n",
    "This time I will not derive the relationship between the Dirichlet prior and the multinomial likelihood function, but I will tell you how to use this relationship to do a Bayesian update.\n",
    "\n",
    "If the prior distribution is Dirichlet with parameter vector `alpha` and the data is a vector of observations, `data`, the posterior distribution is Dirichlet with parameter vector `alpha + data`.\n",
    "\n",
    "As an exercise at the end of this chapter, you can use this method to solve the Lions and Tigers and Bears problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "After reading this chapter, if you feel like you've been tricked, I understand.  It turns out that many of the problems in this book can be solved, using conjugate priors, with just a few arithmetic operations.  So why did we go to all the trouble of using grid algorithms?\n",
    "\n",
    "Sadly, there are only a few problems we can solve with conjugate priors; in fact, this chapter includes most of the ones that are useful in practice.\n",
    "\n",
    "For most problems, there is no conjugate prior and no shortcut to compute the posterior distribution.  That's why we need grid algorithms and the methods in the next two chapters, approximate Bayesian computation (ABC) and Markov chain Monte Carlo methods (MCMC)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "**Exercise:** In the second version of the World Cup problem, the data we use for the update is not the number of goals in a game, but the time until the first goal.\n",
    "So the probability of the data is given by the exponential distribution rather than the Poisson distribution. \n",
    "\n",
    "But it turns out that the gamma distribution is *also* the conjugate prior of the exponential distribution, so there is a simple closed-form way to compute this update, too.\n",
    "\n",
    "The PDF of the exponential distribution is a function of $t$ with $\\lambda$ as a parameter.\n",
    "\n",
    "$$\\lambda e^{-\\lambda t}$$\n",
    "\n",
    "Multiply the PDF of the gamma prior by this likelihood, confirm that the result is an unnormalized gamma distribution, and see if you can derive its parameters.\n",
    "\n",
    "Write a few lines of code to update `prior_gamma` with the data from this version of the problem, which was a first goal after 11 minutes and a second goal after an additional 12 minutes.\n",
    "\n",
    "Remember to express these quantities in units of games, which are approximately 90 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution goes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution goes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution goes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And here's what the posteriors look like\n",
    "\n",
    "make_pmf(prior_gamma, lams).plot(color='C5', label='prior')\n",
    "make_pmf(posterior1, lams).plot(label='after 1 goal')\n",
    "make_pmf(posterior2, lams).plot(label='after 2 goals')\n",
    "\n",
    "decorate_rate(title='World Cup Problem, Germany v Brazil')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** For problems like the Euro problem where the likelihood function is binomial, we can do a Bayesian update with just a few arithmetic operations, but only if the prior is a beta distribution.\n",
    "\n",
    "As we saw, the uniform distribution is a beta distribution with particular parameters, but what can we do if the prior distribution we want is not a beta distribution?  For example, in Chapter xxx we also solved the Euro problem with a triangle prior, and a beta distribution cannot be a triangle.\n",
    "\n",
    "Even so, we can often find a beta distribution that is a good-enough approximation for the prior we want.\n",
    "\n",
    "Here's the triangle prior again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from empiricaldist import Pmf\n",
    "\n",
    "ramp_up = np.arange(50)\n",
    "ramp_down = np.arange(50, -1, -1)\n",
    "\n",
    "a = np.append(ramp_up, ramp_down)\n",
    "xs = uniform.qs\n",
    "\n",
    "triangle = Pmf(a, xs, name='triangle')\n",
    "triangle.normalize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here's the update."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "k, n = 140, 250\n",
    "likelihood = binom.pmf(k, n, xs)\n",
    "\n",
    "posterior = triangle * likelihood\n",
    "posterior.normalize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See if you can find a beta distribution that fits the triangle prior, then update it using `update_beta`.\n",
    "\n",
    "Use `make_pmf` to make a `Pmf` that approximates the posterior distribution and compare it to the posterior we just computed using a grid algorithm.  How big is the largest difference between them?\n",
    "\n",
    "To get you started, here's the beta distribution that we used as a uniform prior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 1\n",
    "beta = 1\n",
    "\n",
    "prior_beta = make_beta(alpha, beta)\n",
    "prior_beta.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here's what it looks like compared to the triangle prior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "prior_pmf = make_pmf(prior_beta, xs)\n",
    "\n",
    "triangle.plot(label='triangle')\n",
    "prior_pmf.plot(label='beta')\n",
    "\n",
    "decorate_euro('Prior distributions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution goes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution goes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution goes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:**  [3Blue1Brown](https://en.wikipedia.org/wiki/3Blue1Brown) is a YouTube channel about math; if you are not already aware of it, I recommend it highly.\n",
    "\n",
    "In [this video](https://www.youtube.com/watch?v=8idr1WZ1A7Q) the narrator presents this problem:\n",
    "\n",
    "> You are buying a product online and you see three sellers offering the same product at the same price.  One of them has a 100% positive rating, but with only 10 reviews.  Another has a 96% positive rating with 50 total reviews.  And yet another has a 93% positive rating, but with 200 total reviews.\n",
    ">\n",
    ">Which one should you buy from?\n",
    "\n",
    "Let's think about how to model this scenario.  Suppose each seller has some unknown probability, `x`, of providing satisfactory service and getting a positive rating, and we want to choose the seller with the highest value of `x`.\n",
    "\n",
    "This is not the only model for this scenario, and it is not necessarily the best.  An alternative would be something like item response theory, where sellers have varying ability to provide satisfactory service and customers have varying difficulty of being satisfied.\n",
    "\n",
    "But the first model has the virtue of simplicity, so let's see where it gets us.\n",
    "\n",
    "As a prior, I suggest we use a beta distribution with `alpha=8` and `beta=2`.  Here's what it looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "prior = make_beta(8, 2)\n",
    "\n",
    "xs = np.linspace(0.005, 0.995, 199)\n",
    "prior_pmf = make_pmf(prior, xs)\n",
    "prior_pmf.plot(color='C5', label='prior')\n",
    "\n",
    "decorate(xlabel='Probability of positive rating',\n",
    "         ylabel='PDF')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This prior implies that most sellers are satisfactory most of the time, but none are perfect.\n",
    "\n",
    "1. Use the data to update this prior for the three sellers and plot the posterior distributions.  Which seller has the highest posterior mean?\n",
    "\n",
    "2. How confident should we be about our choice?  That is, what is the probability that the seller with the highest posterior mean actually has the highest value of `x`?\n",
    "\n",
    "3. Consider an alternative prior with `alpha=0.7` and `beta=0.5`.  What does this prior look like and what does it imply about sellers?\n",
    "\n",
    "4. Run the analysis again with the alternative prior and see what effect it has on the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might notice that I restricted the range of `xs` so it does not include 0 and 1.  I did that because when the parameters of the beta distribution are less than 1, the probability density goes to infinity at 0 and 1.  From a mathematical point of view, that's not a problem; it is still a proper probability distribution.  But from a computational point of view, it means we have to avoid evaluating the PDF at 0 and 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution goes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution goes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution goes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution goes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution goes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution goes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Use a Dirichlet prior with parameter vector `alpha = [1, 1, 1]` to solve the Lions and Tigers and Bears problem:\n",
    "\n",
    ">Suppose we visit a wild animal preserve where we know that the only animals are lions and tigers and bears, but we don't know how many of each there are.\n",
    ">\n",
    ">During the tour, we see 3 lions, 2 tigers, and one bear. Assuming that every animal had an equal chance to appear in our sample, estimate the prevalence of each species.\n",
    ">\n",
    ">What is the probability that the next animal we see is a bear?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution goes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution goes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution goes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution goes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
