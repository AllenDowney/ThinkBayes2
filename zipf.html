
<!DOCTYPE html>


<html lang="en" data-content_root="./" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>What’s a chartist? &#8212; Think Bayes</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=03e43079" />
    <link rel="stylesheet" type="text/css" href="_static/styles/sphinx-book-theme.css?v=eba8b062" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.8ecb98da25f57f5357bf6f572d296f466b2cfe2517ffebfabe82451661e28f02.css?v=6644e6bb" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-design.min.css?v=95c83b7e" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="_static/doctools.js?v=9a2dae69"></script>
    <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="_static/copybutton.js?v=f281be69"></script>
    <script src="_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="_static/design-tabs.js?v=36754332"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"tex2jax": {"inlineMath": [["$", "$"], ["\\(", "\\)"]]}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'zipf';</script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="The Poincaré Problem" href="bread.html" />
    <link rel="prev" title="The All-Knowing Cube of Probability" href="beta_binomial.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="index.html">
  
  
  
  
  
  
    <p class="title logo__title">Think Bayes</p>
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="index.html">
                    Think Bayes 2
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Front Matter</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="preface.html">Preface</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Chapters</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="chap01.html">1. Probability</a></li>
<li class="toctree-l1"><a class="reference internal" href="chap02.html">2. Bayes’s Theorem</a></li>
<li class="toctree-l1"><a class="reference internal" href="chap03.html">3. Distributions</a></li>
<li class="toctree-l1"><a class="reference internal" href="chap04.html">4. Estimating Proportions</a></li>
<li class="toctree-l1"><a class="reference internal" href="chap05.html">5. Estimating Counts</a></li>
<li class="toctree-l1"><a class="reference internal" href="chap06.html">6. Odds and Addends</a></li>
<li class="toctree-l1"><a class="reference internal" href="chap07.html">7. Minimum, Maximum, and Mixture</a></li>
<li class="toctree-l1"><a class="reference internal" href="chap08.html">8. Poisson Processes</a></li>
<li class="toctree-l1"><a class="reference internal" href="chap09.html">9. Decision Analysis</a></li>
<li class="toctree-l1"><a class="reference internal" href="chap10.html">10. Testing</a></li>
<li class="toctree-l1"><a class="reference internal" href="chap11.html">11. Comparison</a></li>
<li class="toctree-l1"><a class="reference internal" href="chap12.html">12. Classification</a></li>
<li class="toctree-l1"><a class="reference internal" href="chap13.html">13. Inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="chap14.html">14. Survival Analysis</a></li>
<li class="toctree-l1"><a class="reference internal" href="chap15.html">15. Mark and Recapture</a></li>
<li class="toctree-l1"><a class="reference internal" href="chap16.html">16. Logistic Regression</a></li>
<li class="toctree-l1"><a class="reference internal" href="chap17.html">17. Regression</a></li>
<li class="toctree-l1"><a class="reference internal" href="chap18.html">18. Conjugate Priors</a></li>
<li class="toctree-l1"><a class="reference internal" href="chap19_v3.html">19. MCMC</a></li>
<li class="toctree-l1"><a class="reference internal" href="chap20.html">20. Approximate Bayesian Computation</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Examples</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="redline.html">The Red Line Problem</a></li>
<li class="toctree-l1"><a class="reference internal" href="redline_pymc.html">The Red Line Problem</a></li>
<li class="toctree-l1"><a class="reference internal" href="vaccine2.html">Estimating vaccine efficacy</a></li>
<li class="toctree-l1"><a class="reference internal" href="usb.html">Flipping USB Connectors</a></li>
<li class="toctree-l1"><a class="reference internal" href="sister.html">The Left Handed Sister Problem</a></li>
<li class="toctree-l1"><a class="reference internal" href="bayes_dice.html">Bayesian Dice</a></li>
<li class="toctree-l1"><a class="reference internal" href="radiation.html">The Emitter-Detector Problem</a></li>
<li class="toctree-l1"><a class="reference internal" href="hospital.html">Grid algorithms for hierarchical models</a></li>
<li class="toctree-l1"><a class="reference internal" href="hospital_birth_rate.html">Comparing birth rates</a></li>
<li class="toctree-l1"><a class="reference internal" href="ok.html">How Many Typos?</a></li>
<li class="toctree-l1"><a class="reference internal" href="bookstore.html">How Many Books?</a></li>
<li class="toctree-l1"><a class="reference internal" href="beta_binomial.html">The All-Knowing Cube of Probability</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">What’s a chartist?</a></li>
<li class="toctree-l1"><a class="reference internal" href="bread.html">The Poincaré Problem</a></li>
<li class="toctree-l1"><a class="reference internal" href="cancer.html">Cancer Survival Rates Are Misleading</a></li>
<li class="toctree-l1"><a class="reference internal" href="raven.html">The Raven Paradox</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">


<a href="https://github.com/AllenDowney/ThinkBayes2" target="_blank"
   class="btn btn-sm btn-source-repository-button"
   title="Source repository"
   data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>

</a>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="_sources/zipf.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>What’s a chartist?</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#word-frequencies">Word Frequencies</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#zipfs-law">Zipf’s Law</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#tail-distribution">Tail Distribution</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#fitting-a-model">Fitting a Model</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-update">The Update</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section id="whats-a-chartist">
<h1>What’s a chartist?<a class="headerlink" href="#whats-a-chartist" title="Link to this heading">#</a></h1>
<p>Recently I heard the word “chartist” for the first time in my life (that I recall).
And then later the same day, I heard it again.
So that raises two questions:</p>
<ul class="simple">
<li><p>What are the chances of going 57 years without hearing a word, and then hearing it twice in one day?</p></li>
<li><p>Also, what’s a chartist?</p></li>
</ul>
<p>To answer the second question first, it’s someone who supported chartism, which was “a working-class movement for political reform in the United Kingdom that erupted from 1838 to 1857”, quoth <a class="reference external" href="https://en.wikipedia.org/wiki/Chartism">Wikipedia</a>.  The name comes from the People’s Charter of 1838, which called for voting rights for unpropertied men, among other reforms.</p>
<p>To answer the first question, we’ll do some Bayesian statistics.
My solution is based on a model that’s not very realistic, so we should not take the result too seriously, but it demonstrates some interesting methods, I think.
And as you’ll see, there is a connection to Zipf’s law, <a class="reference external" href="https://www.allendowney.com/blog/2024/11/10/zipfs-law/">which I wrote about last week</a>.</p>
<p>Since last week’s post was at the beginner level, I should warn you that this one is more advanced – in rapid succession, it involves the beta distribution, the <span class="math notranslate nohighlight">\(t\)</span> distribution, the negative binomial, and the binomial.</p>
<p>This post is based on <em>Think Bayes 2e</em>, which is available from
<a class="reference external" href="https://bookshop.org/a/98697/9781492089469">Bookshop.org</a> and
<a class="reference external" href="https://amzn.to/334eqGo">Amazon</a>.</p>
<p><a class="reference external" href="https://colab.research.google.com/github/AllenDowney/ThinkBayes2/blob/master/examples/zipf.ipynb">Click here to run this notebook on Colab</a>.</p>
<div class="cell tag_hide-cell docutils container">
<details class="admonition hide above-input">
<summary aria-label="Toggle hidden content">
<p class="collapsed admonition-title">Show code cell content</p>
<p class="expanded admonition-title">Hide code cell content</p>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">try</span><span class="p">:</span>
    <span class="kn">import</span><span class="w"> </span><span class="nn">empiricaldist</span>
<span class="k">except</span> <span class="ne">ImportError</span><span class="p">:</span>
    <span class="o">!</span>pip<span class="w"> </span>install<span class="w"> </span>empiricaldist
</pre></div>
</div>
</div>
</details>
</div>
<div class="cell tag_hide-cell docutils container">
<details class="admonition hide above-input">
<summary aria-label="Toggle hidden content">
<p class="collapsed admonition-title">Show code cell content</p>
<p class="expanded admonition-title">Hide code cell content</p>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># download thinkdsp.py</span>

<span class="kn">from</span><span class="w"> </span><span class="nn">os.path</span><span class="w"> </span><span class="kn">import</span> <span class="n">basename</span><span class="p">,</span> <span class="n">exists</span>


<span class="k">def</span><span class="w"> </span><span class="nf">download</span><span class="p">(</span><span class="n">url</span><span class="p">):</span>
    <span class="n">filename</span> <span class="o">=</span> <span class="n">basename</span><span class="p">(</span><span class="n">url</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">exists</span><span class="p">(</span><span class="n">filename</span><span class="p">):</span>
        <span class="kn">from</span><span class="w"> </span><span class="nn">urllib.request</span><span class="w"> </span><span class="kn">import</span> <span class="n">urlretrieve</span>

        <span class="n">local</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">urlretrieve</span><span class="p">(</span><span class="n">url</span><span class="p">,</span> <span class="n">filename</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Downloaded &quot;</span> <span class="o">+</span> <span class="n">local</span><span class="p">)</span>


<span class="n">download</span><span class="p">(</span><span class="s2">&quot;https://github.com/AllenDowney/ThinkBayes2/raw/master/soln/utils.py&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</details>
</div>
<div class="cell tag_hide-cell docutils container">
<details class="admonition hide above-input">
<summary aria-label="Toggle hidden content">
<p class="collapsed admonition-title">Show code cell content</p>
<p class="expanded admonition-title">Hide code cell content</p>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">pandas</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">pd</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>

<span class="kn">from</span><span class="w"> </span><span class="nn">empiricaldist</span><span class="w"> </span><span class="kn">import</span> <span class="n">Pmf</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">utils</span><span class="w"> </span><span class="kn">import</span> <span class="n">decorate</span>

<span class="n">plt</span><span class="o">.</span><span class="n">rcParams</span><span class="p">[</span><span class="s2">&quot;figure.dpi&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="mi">75</span>
<span class="n">plt</span><span class="o">.</span><span class="n">rcParams</span><span class="p">[</span><span class="s2">&quot;figure.figsize&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="mi">6</span><span class="p">,</span> <span class="mf">3.5</span><span class="p">]</span>
</pre></div>
</div>
</div>
</details>
</div>
<section id="word-frequencies">
<h2>Word Frequencies<a class="headerlink" href="#word-frequencies" title="Link to this heading">#</a></h2>
<p>If you don’t hear a word for more than 50 years, that suggests it is not a common word.
We can use Bayes’s theorem to quantify this intuition.
First we’ll compute the posterior distribution of the word’s frequency, then the posterior predictive distribution of hearing it again within a day.</p>
<p>Because we have only one piece of data – the time until first appearance – we’ll need a good prior distribution.
Which means we’ll need a large, good quality sample of English text.
For that, I’ll use a free sample of the COCA dataset from <a class="reference external" href="https://www.corpusdata.org/formats.asp">CorpusData.org</a>. The following cells download and read the data.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">download</span><span class="p">(</span><span class="s2">&quot;https://www.corpusdata.org/coca/samples/coca-samples-text.zip&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">zipfile</span>


<span class="k">def</span><span class="w"> </span><span class="nf">generate_lines</span><span class="p">(</span><span class="n">zip_path</span><span class="o">=</span><span class="s2">&quot;coca-samples-text.zip&quot;</span><span class="p">):</span>
    <span class="k">with</span> <span class="n">zipfile</span><span class="o">.</span><span class="n">ZipFile</span><span class="p">(</span><span class="n">zip_path</span><span class="p">,</span> <span class="s2">&quot;r&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">zip_file</span><span class="p">:</span>
        <span class="n">file_list</span> <span class="o">=</span> <span class="n">zip_file</span><span class="o">.</span><span class="n">namelist</span><span class="p">()</span>
        <span class="k">for</span> <span class="n">file_name</span> <span class="ow">in</span> <span class="n">file_list</span><span class="p">:</span>
            <span class="k">with</span> <span class="n">zip_file</span><span class="o">.</span><span class="n">open</span><span class="p">(</span><span class="n">file_name</span><span class="p">)</span> <span class="k">as</span> <span class="n">file</span><span class="p">:</span>
                <span class="n">lines</span> <span class="o">=</span> <span class="n">file</span><span class="o">.</span><span class="n">readlines</span><span class="p">()</span>
                <span class="k">for</span> <span class="n">line</span> <span class="ow">in</span> <span class="n">lines</span><span class="p">:</span>
                    <span class="k">yield</span> <span class="p">(</span><span class="n">line</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="s2">&quot;utf-8&quot;</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<p>We’ll use a <code class="docutils literal notranslate"><span class="pre">Counter</span></code> to count the number of times each word appears.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">re</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">collections</span><span class="w"> </span><span class="kn">import</span> <span class="n">Counter</span>

<span class="n">pattern</span> <span class="o">=</span> <span class="sa">r</span><span class="s2">&quot;[ /\n]+|--&quot;</span>

<span class="n">counter</span> <span class="o">=</span> <span class="n">Counter</span><span class="p">()</span>
<span class="k">for</span> <span class="n">line</span> <span class="ow">in</span> <span class="n">generate_lines</span><span class="p">():</span>
    <span class="n">words</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">pattern</span><span class="p">,</span> <span class="n">line</span><span class="p">)[</span><span class="mi">1</span><span class="p">:]</span>
    <span class="n">counter</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">word</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span> <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">words</span> <span class="k">if</span> <span class="n">word</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>The dataset includes about 188,000 unique strings, but not all of them are what we would consider words.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">len</span><span class="p">(</span><span class="n">counter</span><span class="p">),</span> <span class="n">counter</span><span class="o">.</span><span class="n">total</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(188086, 11503819)
</pre></div>
</div>
</div>
</div>
<p>To narrow it down, I’ll remove anything that starts or ends with a non-alphabetical character – so hyphens and apostrophes are allowed in the middle of a word.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="nb">list</span><span class="p">(</span><span class="n">counter</span><span class="o">.</span><span class="n">keys</span><span class="p">()):</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">s</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">isalpha</span><span class="p">()</span> <span class="ow">or</span> <span class="ow">not</span> <span class="n">s</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">isalpha</span><span class="p">():</span>
        <span class="k">del</span> <span class="n">counter</span><span class="p">[</span><span class="n">s</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<p>This filter reduces the number of unique words to about 151,000.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">num_words</span> <span class="o">=</span> <span class="n">counter</span><span class="o">.</span><span class="n">total</span><span class="p">()</span>
<span class="nb">len</span><span class="p">(</span><span class="n">counter</span><span class="p">),</span> <span class="n">num_words</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(151414, 8889694)
</pre></div>
</div>
</div>
</div>
<p>The most common words are what you would expect, with the exception of “n’t”, which is there because the COCA corpus treats it as a separate word.</p>
<p>Of the 50 most common words, all of them have one syllable except number 38.
Before you look at the list, can you guess the most common two-syllable word?
Here’s a theory about <a class="reference external" href="https://news.mit.edu/2011/words-count-0210">why common words are short</a>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">word</span><span class="p">,</span> <span class="n">freq</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">counter</span><span class="o">.</span><span class="n">most_common</span><span class="p">(</span><span class="mi">50</span><span class="p">)):</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="se">\t</span><span class="si">{</span><span class="n">word</span><span class="si">}</span><span class="se">\t</span><span class="si">{</span><span class="n">freq</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>1	the	461991
2	to	237929
3	and	231459
4	of	217363
5	a	203302
6	in	153323
7	i	137931
8	that	123818
9	you	109635
10	it	103712
11	is	93996
12	for	78755
13	on	64869
14	was	64388
15	with	59724
16	he	57684
17	this	51879
18	as	51202
19	n&#39;t	49291
20	we	47694
21	are	47192
22	have	46963
23	be	46563
24	not	43872
25	but	42434
26	they	42411
27	at	42017
28	do	41568
29	what	35637
30	from	34557
31	his	33578
32	by	32583
33	or	32146
34	she	29945
35	all	29391
36	my	29390
37	an	28580
38	about	27804
39	there	27291
40	so	27081
41	her	26363
42	one	26022
43	had	25656
44	if	25373
45	your	24641
46	me	24551
47	who	23500
48	can	23311
49	their	23221
50	out	22902
</pre></div>
</div>
</div>
</div>
<p>There are about 72,000 words that only appear once in the corpus, technically known as <a class="reference external" href="https://en.wikipedia.org/wiki/Hapax_legomenon">hapax legomena</a>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">singletons</span> <span class="o">=</span> <span class="p">[</span><span class="n">word</span> <span class="k">for</span> <span class="p">(</span><span class="n">word</span><span class="p">,</span> <span class="n">freq</span><span class="p">)</span> <span class="ow">in</span> <span class="n">counter</span><span class="o">.</span><span class="n">items</span><span class="p">()</span> <span class="k">if</span> <span class="n">freq</span> <span class="o">==</span> <span class="mi">1</span><span class="p">]</span>
<span class="nb">len</span><span class="p">(</span><span class="n">singletons</span><span class="p">),</span> <span class="nb">len</span><span class="p">(</span><span class="n">singletons</span><span class="p">)</span> <span class="o">/</span> <span class="n">counter</span><span class="o">.</span><span class="n">total</span><span class="p">()</span> <span class="o">*</span> <span class="mi">100</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(72159, 0.811715228893143)
</pre></div>
</div>
</div>
</div>
<p>Here’s a random selection of them. Many are proper names, typos, or other non-words, but some are legitimate but rare words.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">singletons</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([&#39;laneer&#39;, &#39;emc&#39;, &#39;literature-like&#39;, &#39;tomyworld&#39;, &#39;roald&#39;,
       &#39;unreleased&#39;, &#39;basemen&#39;, &#39;kielhau&#39;, &#39;clobber&#39;, &#39;feydeau&#39;,
       &#39;symptomless&#39;, &#39;channelmaster&#39;, &#39;v-i&#39;, &#39;tipsha&#39;, &#39;mjlkdroppen&#39;,
       &#39;harlots&#39;, &#39;phaetons&#39;, &#39;grlinger&#39;, &#39;naniwa&#39;, &#39;dadian&#39;,
       &#39;banafionen&#39;, &#39;ceramaseal&#39;, &#39;vine-covered&#39;, &#39;terrafirmahome.com&#39;,
       &#39;hesten&#39;, &#39;undertheorized&#39;, &#39;fantastycznie&#39;, &#39;kaido&#39;, &#39;noughts&#39;,
       &#39;hannelie&#39;, &#39;cacoa&#39;, &#39;subelement&#39;, &#39;mestothelioma&#39;, &#39;gut-level&#39;,
       &#39;abis&#39;, &#39;potterville&#39;, &#39;quarter-to-quarter&#39;, &#39;lokkii&#39;, &#39;telemed&#39;,
       &#39;whitewood&#39;, &#39;dualmode&#39;, &#39;plebiscites&#39;, &#39;loubrutton&#39;,
       &#39;off-loading&#39;, &#39;abbot-t-t&#39;, &#39;whackaloons&#39;, &#39;tuinal&#39;, &#39;guyi&#39;,
       &#39;samanthalaughs&#39;, &#39;editor-sponsored&#39;, &#39;neurosciences&#39;, &#39;lunched&#39;,
       &#39;chicken-and-brisket&#39;, &#39;korekane&#39;, &#39;ruby-colored&#39;,
       &#39;double-elimination&#39;, &#39;cornhusker&#39;, &#39;wjounds&#39;, &#39;mendy&#39;, &#39;red.ooh&#39;,
       &#39;delighters&#39;, &#39;tuviera&#39;, &#39;spot-lit&#39;, &#39;tuskarr&#39;, &#39;easy-many&#39;,
       &#39;timepoint&#39;, &#39;mouthfuls&#39;, &#39;catchy-titled&#39;, &#39;b.l&#39;, &#39;four-ply&#39;,
       &quot;sa&#39;ud&quot;, &#39;millenarianism&#39;, &#39;gelder&#39;, &#39;cinnam&#39;,
       &#39;documentary-filmmaking&#39;, &#39;huviesen&#39;, &#39;by-gone&#39;, &#39;boy-friend&#39;,
       &#39;heartlight&#39;, &#39;farecompare.com&#39;, &#39;nurya&#39;, &#39;overstaying&#39;,
       &#39;johnny-turn&#39;, &#39;rashness&#39;, &#39;mestier&#39;, &#39;trivedi&#39;, &#39;koshanska&#39;,
       &#39;tremulousness&#39;, &#39;movies-another&#39;, &#39;womenfolks&#39;, &#39;bawdy&#39;,
       &#39;all-her-life&#39;, &#39;lakhani&#39;, &#39;screeeeaming&#39;, &#39;marketings&#39;, &#39;girthy&#39;,
       &#39;non-discriminatory&#39;, &#39;chumpy&#39;, &#39;resque&#39;, &#39;lysing&#39;], dtype=&#39;&lt;U24&#39;)
</pre></div>
</div>
</div>
</div>
<p>As it turns out, “chartist” and “chartism” do not appear in this corpus, confirming that they are rare words.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">counter</span><span class="p">[</span><span class="s1">&#39;chartist&#39;</span><span class="p">],</span> <span class="n">counter</span><span class="p">[</span><span class="s1">&#39;chartism&#39;</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>0
</pre></div>
</div>
</div>
</div>
<p>Now let’s see what the distribution of word frequencies looks like.</p>
</section>
<section id="zipfs-law">
<h2>Zipf’s Law<a class="headerlink" href="#zipfs-law" title="Link to this heading">#</a></h2>
<p>One way to visualize the distribution is a Zipf plot, which shows the ranks on the x-axis and the frequencies on the y-axis.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">freqs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="nb">sorted</span><span class="p">(</span><span class="n">counter</span><span class="o">.</span><span class="n">values</span><span class="p">(),</span> <span class="n">reverse</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">n</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">freqs</span><span class="p">)</span>
<span class="n">ranks</span> <span class="o">=</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">n</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Here’s what it looks like on a log-log scale.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">ranks</span><span class="p">,</span> <span class="n">freqs</span><span class="p">)</span>

<span class="n">decorate</span><span class="p">(</span>
    <span class="n">title</span><span class="o">=</span><span class="s2">&quot;Zipf plot&quot;</span><span class="p">,</span> <span class="n">xlabel</span><span class="o">=</span><span class="s2">&quot;Rank&quot;</span><span class="p">,</span> <span class="n">ylabel</span><span class="o">=</span><span class="s2">&quot;Frequency&quot;</span><span class="p">,</span> <span class="n">xscale</span><span class="o">=</span><span class="s2">&quot;log&quot;</span><span class="p">,</span> <span class="n">yscale</span><span class="o">=</span><span class="s2">&quot;log&quot;</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/204f4aae3fe537fefdbe43abadd4be2a854bc627c7d5e064d9efebd9cc6a58df.png" src="_images/204f4aae3fe537fefdbe43abadd4be2a854bc627c7d5e064d9efebd9cc6a58df.png" />
</div>
</div>
<p>Zipf’s law suggest that the result should be a straight line with slope close to -1.
It’s not exactly a straight line, but it’s close, and the slope is about -1.1.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">rise</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">log10</span><span class="p">(</span><span class="n">freqs</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">log10</span><span class="p">(</span><span class="n">freqs</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="n">rise</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>-5.664633515191604
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">run</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">log10</span><span class="p">(</span><span class="n">ranks</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">log10</span><span class="p">(</span><span class="n">ranks</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="n">run</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>5.180166032638616
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">rise</span> <span class="o">/</span> <span class="n">run</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>-1.0935235433575892
</pre></div>
</div>
</div>
</div>
<p>The Zipf plot is a well-known visual representation of the distribution of frequencies, but for the current problem, we’ll switch to a different representation.</p>
</section>
<section id="tail-distribution">
<h2>Tail Distribution<a class="headerlink" href="#tail-distribution" title="Link to this heading">#</a></h2>
<p>Given the number of times each word appear in the corpus, we can compute the rates, which is the number of times we expect each word to appear in a sample of a given size, and the inverse rates, which are the number of words we need to see before we expect a given word to appear.</p>
<p>We will find it most convenient to work with the distribution of inverse rates on a log scale.
The first step is to use the observed frequencies to estimate word rates – we’ll estimate the rate at which each word would appear in a random sample.</p>
<p>We’ll do that by creating a beta distribution that represents the posterior distribution of word rates, given the observed frequencies (see <a class="reference external" href="https://allendowney.github.io/ThinkBayes2/chap18.html#the-conjugate-prior">this section of <em>Think Bayes</em></a>) – and then drawing a random sample from the posterior.
So words that have the same frequency will not generally have the same inferred rate.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">scipy.stats</span><span class="w"> </span><span class="kn">import</span> <span class="n">beta</span>

<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">17</span><span class="p">)</span>
<span class="n">alphas</span> <span class="o">=</span> <span class="n">freqs</span> <span class="o">+</span> <span class="mi">1</span>
<span class="n">betas</span> <span class="o">=</span> <span class="n">num_words</span> <span class="o">-</span> <span class="n">freqs</span> <span class="o">+</span> <span class="mi">1</span>
<span class="n">inferred_rates</span> <span class="o">=</span> <span class="n">beta</span><span class="p">(</span><span class="n">alphas</span><span class="p">,</span> <span class="n">betas</span><span class="p">)</span><span class="o">.</span><span class="n">rvs</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<p>Now we can compute the inverse rates, which are the number of words we have to sample before we expect to see each word once.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">inverse_rates</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">/</span> <span class="n">inferred_rates</span>
</pre></div>
</div>
</div>
</div>
<p>And here are their magnitudes, expressed as logarithms base 10.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">mags</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">log10</span><span class="p">(</span><span class="n">inverse_rates</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>To represent the distribution of these magnitudes, we’ll use a <code class="docutils literal notranslate"><span class="pre">Surv</span></code> object, which represents survival functions, but we’ll use a variation of the survival function which is the probability that a randomly-chosen value is greater than or equal to a given quantity.
The following function computes this version of a survival function, which is called a tail probability.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">empiricaldist</span><span class="w"> </span><span class="kn">import</span> <span class="n">Surv</span>


<span class="k">def</span><span class="w"> </span><span class="nf">make_surv</span><span class="p">(</span><span class="n">seq</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Make a non-standard survival function, P(X&gt;=x)&quot;&quot;&quot;</span>
    <span class="n">pmf</span> <span class="o">=</span> <span class="n">Pmf</span><span class="o">.</span><span class="n">from_seq</span><span class="p">(</span><span class="n">seq</span><span class="p">)</span>
    <span class="n">surv</span> <span class="o">=</span> <span class="n">pmf</span><span class="o">.</span><span class="n">make_surv</span><span class="p">()</span> <span class="o">+</span> <span class="n">pmf</span>

    <span class="c1"># correct for numerical error</span>
    <span class="n">surv</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="k">return</span> <span class="n">Surv</span><span class="p">(</span><span class="n">surv</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Here’s how we make the survival function.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">surv</span> <span class="o">=</span> <span class="n">make_surv</span><span class="p">(</span><span class="n">mags</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>And here’s what it looks like.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">options</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="n">marker</span><span class="o">=</span><span class="s2">&quot;.&quot;</span><span class="p">,</span> <span class="n">ms</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;data&quot;</span><span class="p">)</span>
<span class="n">surv</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="o">**</span><span class="n">options</span><span class="p">)</span>
<span class="n">decorate</span><span class="p">(</span><span class="n">xlabel</span><span class="o">=</span><span class="s2">&quot;Inverse rate (log10 words per appearance)&quot;</span><span class="p">,</span> <span class="n">ylabel</span><span class="o">=</span><span class="s2">&quot;Tail probability&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/a85267e340ee9e92988ee9ce9ec80c2edcf7c526f04c09a28b53b4ca43f0fa1a.png" src="_images/a85267e340ee9e92988ee9ce9ec80c2edcf7c526f04c09a28b53b4ca43f0fa1a.png" />
</div>
</div>
<p>The tail distribution has the sigmoid shape that is characteristic of normal distributions and <span class="math notranslate nohighlight">\(t\)</span> distributions, although it is notably asymmetric.</p>
<p>And here’s what the tail probabilities look like on a log-y scale.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">surv</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="o">**</span><span class="n">options</span><span class="p">)</span>
<span class="n">decorate</span><span class="p">(</span><span class="n">xlabel</span><span class="o">=</span><span class="s2">&quot;Inverse rate (words per appearance)&quot;</span><span class="p">,</span> <span class="n">yscale</span><span class="o">=</span><span class="s2">&quot;log&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/ad17e7857447f99903d3a718f91bcaae09fdcf067df4d4f48f5127d3e8151c5d.png" src="_images/ad17e7857447f99903d3a718f91bcaae09fdcf067df4d4f48f5127d3e8151c5d.png" />
</div>
</div>
<p>If this distribution were normal, we would expect this curve to drop off with increasing slope.
But for the words with the lowest frequencies – that is, the highest inverse rates – it is almost a straight line.
And that suggests that a <span class="math notranslate nohighlight">\(t\)</span> distribution might be a good model for this data.</p>
</section>
<section id="fitting-a-model">
<h2>Fitting a Model<a class="headerlink" href="#fitting-a-model" title="Link to this heading">#</a></h2>
<p>To estimate the frequency of rare words, we will need to model the tail behavior of this distribution and extrapolate it beyond the data.
So let’s fit a <span class="math notranslate nohighlight">\(t\)</span> distribution and see how it looks.
I’ll use code from <a class="reference external" href="https://allendowney.github.io/ProbablyOverthinkingIt/longtail.html">Chapter 8 of <em>Probably Overthinking It</em></a>, which is all about these long-tailed distributions.</p>
<p>The following function makes a <code class="docutils literal notranslate"><span class="pre">Surv</span></code> object that represents a <span class="math notranslate nohighlight">\(t\)</span> distribution with the given parameters.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">scipy.stats</span><span class="w"> </span><span class="kn">import</span> <span class="n">t</span> <span class="k">as</span> <span class="n">t_dist</span>


<span class="k">def</span><span class="w"> </span><span class="nf">truncated_t_sf</span><span class="p">(</span><span class="n">qs</span><span class="p">,</span> <span class="n">df</span><span class="p">,</span> <span class="n">mu</span><span class="p">,</span> <span class="n">sigma</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Makes Surv object for a t distribution.</span>
<span class="sd">    </span>
<span class="sd">    Truncated on the left, assuming all values are greater than min(qs)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">ps</span> <span class="o">=</span> <span class="n">t_dist</span><span class="o">.</span><span class="n">sf</span><span class="p">(</span><span class="n">qs</span><span class="p">,</span> <span class="n">df</span><span class="p">,</span> <span class="n">mu</span><span class="p">,</span> <span class="n">sigma</span><span class="p">)</span>
    <span class="n">surv_model</span> <span class="o">=</span> <span class="n">Surv</span><span class="p">(</span><span class="n">ps</span> <span class="o">/</span> <span class="n">ps</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">qs</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">surv_model</span>
</pre></div>
</div>
</div>
</div>
<p>If we are given the <code class="docutils literal notranslate"><span class="pre">df</span></code> parameter, we can use the following function to find the values of <code class="docutils literal notranslate"><span class="pre">mu</span></code> and <code class="docutils literal notranslate"><span class="pre">sigma</span></code> that best fit the data, focusing on the central part of the distribution.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">scipy.optimize</span><span class="w"> </span><span class="kn">import</span> <span class="n">least_squares</span>


<span class="k">def</span><span class="w"> </span><span class="nf">fit_truncated_t</span><span class="p">(</span><span class="n">df</span><span class="p">,</span> <span class="n">surv</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Given df, find the best values of mu and sigma.&quot;&quot;&quot;</span>
    <span class="n">low</span><span class="p">,</span> <span class="n">high</span> <span class="o">=</span> <span class="n">surv</span><span class="o">.</span><span class="n">qs</span><span class="o">.</span><span class="n">min</span><span class="p">(),</span> <span class="n">surv</span><span class="o">.</span><span class="n">qs</span><span class="o">.</span><span class="n">max</span><span class="p">()</span>
    <span class="n">qs_model</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">low</span><span class="p">,</span> <span class="n">high</span><span class="p">,</span> <span class="mi">2000</span><span class="p">)</span>
    <span class="n">ps</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">,</span> <span class="mi">20</span><span class="p">)</span>
    <span class="n">qs</span> <span class="o">=</span> <span class="n">surv</span><span class="o">.</span><span class="n">inverse</span><span class="p">(</span><span class="n">ps</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">error_func_t</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">df</span><span class="p">,</span> <span class="n">surv</span><span class="p">):</span>
        <span class="n">mu</span><span class="p">,</span> <span class="n">sigma</span> <span class="o">=</span> <span class="n">params</span>
        <span class="n">surv_model</span> <span class="o">=</span> <span class="n">truncated_t_sf</span><span class="p">(</span><span class="n">qs_model</span><span class="p">,</span> <span class="n">df</span><span class="p">,</span> <span class="n">mu</span><span class="p">,</span> <span class="n">sigma</span><span class="p">)</span>

        <span class="n">error</span> <span class="o">=</span> <span class="n">surv</span><span class="p">(</span><span class="n">qs</span><span class="p">)</span> <span class="o">-</span> <span class="n">surv_model</span><span class="p">(</span><span class="n">qs</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">error</span>

    <span class="n">pmf</span> <span class="o">=</span> <span class="n">surv</span><span class="o">.</span><span class="n">make_pmf</span><span class="p">()</span>
    <span class="n">pmf</span><span class="o">.</span><span class="n">normalize</span><span class="p">()</span>
    <span class="n">params</span> <span class="o">=</span> <span class="n">pmf</span><span class="o">.</span><span class="n">mean</span><span class="p">(),</span> <span class="n">pmf</span><span class="o">.</span><span class="n">std</span><span class="p">()</span>
    <span class="n">res</span> <span class="o">=</span> <span class="n">least_squares</span><span class="p">(</span><span class="n">error_func_t</span><span class="p">,</span> <span class="n">x0</span><span class="o">=</span><span class="n">params</span><span class="p">,</span> <span class="n">args</span><span class="o">=</span><span class="p">(</span><span class="n">df</span><span class="p">,</span> <span class="n">surv</span><span class="p">),</span> <span class="n">xtol</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">)</span>
    <span class="k">assert</span> <span class="n">res</span><span class="o">.</span><span class="n">success</span>
    <span class="k">return</span> <span class="n">res</span><span class="o">.</span><span class="n">x</span>
</pre></div>
</div>
</div>
</div>
<p>But since we are not given <code class="docutils literal notranslate"><span class="pre">df</span></code>, we can use the following function to search for the value that best fits the tail of the distribution.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">scipy.optimize</span><span class="w"> </span><span class="kn">import</span> <span class="n">minimize</span>


<span class="k">def</span><span class="w"> </span><span class="nf">minimize_df</span><span class="p">(</span><span class="n">df0</span><span class="p">,</span> <span class="n">surv</span><span class="p">,</span> <span class="n">bounds</span><span class="o">=</span><span class="p">[(</span><span class="mi">1</span><span class="p">,</span> <span class="mf">1e3</span><span class="p">)],</span> <span class="n">ps</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="n">low</span><span class="p">,</span> <span class="n">high</span> <span class="o">=</span> <span class="n">surv</span><span class="o">.</span><span class="n">qs</span><span class="o">.</span><span class="n">min</span><span class="p">(),</span> <span class="n">surv</span><span class="o">.</span><span class="n">qs</span><span class="o">.</span><span class="n">max</span><span class="p">()</span>
    <span class="n">qs_model</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">low</span><span class="p">,</span> <span class="n">high</span> <span class="o">*</span> <span class="mf">1.2</span><span class="p">,</span> <span class="mi">2000</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">ps</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">t</span> <span class="o">=</span> <span class="n">surv</span><span class="o">.</span><span class="n">ps</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">surv</span><span class="o">.</span><span class="n">ps</span><span class="p">[</span><span class="o">-</span><span class="mi">5</span><span class="p">]</span>
        <span class="n">low</span><span class="p">,</span> <span class="n">high</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">log10</span><span class="p">(</span><span class="n">t</span><span class="p">)</span>
        <span class="n">ps</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">logspace</span><span class="p">(</span><span class="n">low</span><span class="p">,</span> <span class="n">high</span><span class="p">,</span> <span class="mi">30</span><span class="p">,</span> <span class="n">endpoint</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

    <span class="n">qs</span> <span class="o">=</span> <span class="n">surv</span><span class="o">.</span><span class="n">inverse</span><span class="p">(</span><span class="n">ps</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">error_func_tail</span><span class="p">(</span><span class="n">params</span><span class="p">):</span>
        <span class="p">(</span><span class="n">df</span><span class="p">,)</span> <span class="o">=</span> <span class="n">params</span>
        <span class="c1"># print(df)</span>
        <span class="n">mu</span><span class="p">,</span> <span class="n">sigma</span> <span class="o">=</span> <span class="n">fit_truncated_t</span><span class="p">(</span><span class="n">df</span><span class="p">,</span> <span class="n">surv</span><span class="p">)</span>
        <span class="n">surv_model</span> <span class="o">=</span> <span class="n">truncated_t_sf</span><span class="p">(</span><span class="n">qs_model</span><span class="p">,</span> <span class="n">df</span><span class="p">,</span> <span class="n">mu</span><span class="p">,</span> <span class="n">sigma</span><span class="p">)</span>

        <span class="n">errors</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">log10</span><span class="p">(</span><span class="n">surv</span><span class="p">(</span><span class="n">qs</span><span class="p">))</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">log10</span><span class="p">(</span><span class="n">surv_model</span><span class="p">(</span><span class="n">qs</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">errors</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>

    <span class="n">params</span> <span class="o">=</span> <span class="p">(</span><span class="n">df0</span><span class="p">,)</span>
    <span class="n">res</span> <span class="o">=</span> <span class="n">minimize</span><span class="p">(</span><span class="n">error_func_tail</span><span class="p">,</span> <span class="n">x0</span><span class="o">=</span><span class="n">params</span><span class="p">,</span> <span class="n">bounds</span><span class="o">=</span><span class="n">bounds</span><span class="p">,</span> <span class="n">tol</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">,</span> <span class="n">method</span><span class="o">=</span><span class="s2">&quot;Powell&quot;</span><span class="p">)</span>
    <span class="k">assert</span> <span class="n">res</span><span class="o">.</span><span class="n">success</span>
    <span class="k">return</span> <span class="n">res</span><span class="o">.</span><span class="n">x</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">df</span> <span class="o">=</span> <span class="n">minimize_df</span><span class="p">(</span><span class="mi">25</span><span class="p">,</span> <span class="n">surv</span><span class="p">)</span>
<span class="n">df</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([22.52401171])
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">mu</span><span class="p">,</span> <span class="n">sigma</span> <span class="o">=</span> <span class="n">fit_truncated_t</span><span class="p">(</span><span class="n">df</span><span class="p">,</span> <span class="n">surv</span><span class="p">)</span>
<span class="n">df</span><span class="p">,</span> <span class="n">mu</span><span class="p">,</span> <span class="n">sigma</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(array([22.52401171]), 6.433323515095857, 0.49070837962997577)
</pre></div>
</div>
</div>
</div>
<p>Here’s the <code class="docutils literal notranslate"><span class="pre">t</span></code> distribution that best fits the data.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">low</span><span class="p">,</span> <span class="n">high</span> <span class="o">=</span> <span class="n">surv</span><span class="o">.</span><span class="n">qs</span><span class="o">.</span><span class="n">min</span><span class="p">(),</span> <span class="n">surv</span><span class="o">.</span><span class="n">qs</span><span class="o">.</span><span class="n">max</span><span class="p">()</span>
<span class="n">qs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">low</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">2000</span><span class="p">)</span>
<span class="n">surv_model</span> <span class="o">=</span> <span class="n">truncated_t_sf</span><span class="p">(</span><span class="n">qs</span><span class="p">,</span> <span class="n">df</span><span class="p">,</span> <span class="n">mu</span><span class="p">,</span> <span class="n">sigma</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">surv_model</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">color</span><span class="o">=</span><span class="s2">&quot;gray&quot;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.4</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;model&quot;</span><span class="p">)</span>
<span class="n">surv</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="o">**</span><span class="n">options</span><span class="p">)</span>
<span class="n">decorate</span><span class="p">(</span><span class="n">xlabel</span><span class="o">=</span><span class="s2">&quot;Inverse rate (log10 words per appearance)&quot;</span><span class="p">,</span> <span class="n">ylabel</span><span class="o">=</span><span class="s2">&quot;Tail probability&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/6a78db0b7207d7492f70ad6ec717b9441d528c6bae9ad44e8e5673a7982f4777.png" src="_images/6a78db0b7207d7492f70ad6ec717b9441d528c6bae9ad44e8e5673a7982f4777.png" />
</div>
</div>
<p>With the y-axis on a linear scale, we can see that the model fits the data reasonably well, except for a range between 5 and 6 – that is for words that appear about 1 time in a million.</p>
<p>Here’s what the model looks like on a log-y scale.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">surv_model</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">color</span><span class="o">=</span><span class="s2">&quot;gray&quot;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.4</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;model&quot;</span><span class="p">)</span>
<span class="n">surv</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="o">**</span><span class="n">options</span><span class="p">)</span>
<span class="n">decorate</span><span class="p">(</span>
    <span class="n">xlabel</span><span class="o">=</span><span class="s2">&quot;Inverse rate (log10 words per appearance)&quot;</span><span class="p">,</span>
    <span class="n">ylabel</span><span class="o">=</span><span class="s2">&quot;Tail probability&quot;</span><span class="p">,</span>
    <span class="n">yscale</span><span class="o">=</span><span class="s2">&quot;log&quot;</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/8486cbced76150d86f5639eb3320a68b34fd31325bd8d20f9caf3d994dd0c669.png" src="_images/8486cbced76150d86f5639eb3320a68b34fd31325bd8d20f9caf3d994dd0c669.png" />
</div>
</div>
<p>The model fits the data well in the extreme tail, which is exactly where we need it.
And we can use the model to extrapolate a little beyond the data, to make sure we cover the range that will turn out to be likely in the scenario where we hear a word for this first time after 50 years.</p>
</section>
<section id="the-update">
<h2>The Update<a class="headerlink" href="#the-update" title="Link to this heading">#</a></h2>
<p>The model we’ve developed is the distribution of inverse rates for the words that appear in the corpus and, by extrapolation, for additional rare words that didn’t appear in the corpus.
This distribution will be the prior for the Bayesian update.
We just have to convert it from a survival function to a PMF (remembering that these are equivalent representations of the same distribution).</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">prior</span> <span class="o">=</span> <span class="n">surv_model</span><span class="o">.</span><span class="n">make_pmf</span><span class="p">()</span>
<span class="n">prior</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">label</span><span class="o">=</span><span class="s2">&quot;prior&quot;</span><span class="p">)</span>
<span class="n">decorate</span><span class="p">(</span>
    <span class="n">xlabel</span><span class="o">=</span><span class="s2">&quot;Inverse rate (log10 words per appearance)&quot;</span><span class="p">,</span>
    <span class="n">ylabel</span><span class="o">=</span><span class="s2">&quot;Density&quot;</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/baed114476768c8511a77423e7217807b462ce1d867fe098a75626df9ed59042.png" src="_images/baed114476768c8511a77423e7217807b462ce1d867fe098a75626df9ed59042.png" />
</div>
</div>
<p>To compute the likelihood of the observation, we have to transform the inverse rates to probabilities.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">ps</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">power</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="n">prior</span><span class="o">.</span><span class="n">qs</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Now suppose that in a given day, you read or hear 10,000 words in a context where you would notice if you heard a word for the first time.
Here’s the number of words you would hear in 50 years.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">words_per_day</span> <span class="o">=</span> <span class="mi">10_000</span>
<span class="n">days</span> <span class="o">=</span> <span class="mi">50</span> <span class="o">*</span> <span class="mi">365</span>
<span class="n">k</span> <span class="o">=</span> <span class="n">days</span> <span class="o">*</span> <span class="n">words_per_day</span>
<span class="n">k</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>182500000
</pre></div>
</div>
</div>
</div>
<p>Now, what’s the probability that you fail to encounter a word in <code class="docutils literal notranslate"><span class="pre">k</span></code> attempts and then encounter it on the next attempt?
We can answer that with the negative binomial distribution, which computes the probability of getting the <code class="docutils literal notranslate"><span class="pre">n</span></code>th success after <code class="docutils literal notranslate"><span class="pre">k</span></code> failures, for a given probability – or in this case, for  a sequence of possible probabilities.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">scipy.stats</span><span class="w"> </span><span class="kn">import</span> <span class="n">nbinom</span>

<span class="n">n</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">likelihood</span> <span class="o">=</span> <span class="n">nbinom</span><span class="o">.</span><span class="n">pmf</span><span class="p">(</span><span class="n">k</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">ps</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>With this likelihood and the prior, we can compute the posterior distribution in the usual way.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">posterior</span> <span class="o">=</span> <span class="n">prior</span> <span class="o">*</span> <span class="n">likelihood</span>
<span class="n">posterior</span><span class="o">.</span><span class="n">normalize</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>1.368245917258196e-11
</pre></div>
</div>
</div>
</div>
<p>And here’s what it looks like.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">prior</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;prior&quot;</span><span class="p">)</span>
<span class="n">posterior</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">label</span><span class="o">=</span><span class="s2">&quot;posterior&quot;</span><span class="p">)</span>
<span class="n">decorate</span><span class="p">(</span>
    <span class="n">xlabel</span><span class="o">=</span><span class="s2">&quot;Inverse rate (log10 words per appearance)&quot;</span><span class="p">,</span>
    <span class="n">ylabel</span><span class="o">=</span><span class="s2">&quot;Density&quot;</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/9ee20cb48e5268c6716d40b69fd8eede6f4a244ae418222ecfaaa83a632dd371.png" src="_images/9ee20cb48e5268c6716d40b69fd8eede6f4a244ae418222ecfaaa83a632dd371.png" />
</div>
</div>
<p>If you go 50 years without hearing a word, that suggests that it is a rare word, and the posterior distribution reflects that logic.</p>
<p>The posterior distribution represents a range of possible values for the inverse rate of the word you heard.
Now we can use it to answer the question we started with: what is the probability of hearing the same word again on the same day – that is, within the next 10,000 words you hear?</p>
<p>To answer that, we can use the survival function of the <a class="reference external" href="https://allendowney.github.io/ThinkBayes2/chap18.html?highlight=binomial#binomial-likelihood">binomial distribution</a> to compute the probability of more than 0 successes in the next <code class="docutils literal notranslate"><span class="pre">n_pred</span></code> attempts.
We’ll compute this probability for each of the <code class="docutils literal notranslate"><span class="pre">ps</span></code> that correspond to the inverse rates in the posterior.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">scipy.stats</span><span class="w"> </span><span class="kn">import</span> <span class="n">binom</span>

<span class="n">n_pred</span> <span class="o">=</span> <span class="n">words_per_day</span>
<span class="n">ps_pred</span> <span class="o">=</span> <span class="n">binom</span><span class="o">.</span><span class="n">sf</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">n_pred</span><span class="p">,</span> <span class="n">ps</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>And we can use the probabilities in the posterior to compute the expected value – by the law of total probability, the result is the probability of hearing the same word again within a day.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">p</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">posterior</span> <span class="o">*</span> <span class="n">ps_pred</span><span class="p">)</span>
<span class="n">p</span><span class="p">,</span> <span class="mi">1</span> <span class="o">/</span> <span class="n">p</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(0.00016019406802217392, 6242.42840166579)
</pre></div>
</div>
</div>
</div>
<p>The result is about 1 in 6000.</p>
<p>With all of the assumptions we made in this calculation, there’s no reason to be more precise than that.
And as I mentioned at the beginning, we should probably not take this conclusion to seriously.
If you hear a word for the first time after 50 years, there’s a good chance the word is “having a moment”, which greatly increases the chance you’ll hear it again.
I can’t think of why chartism might be in the news at the moment, but maybe this post will go viral and make it happen.</p>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="beta_binomial.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">The All-Knowing Cube of Probability</p>
      </div>
    </a>
    <a class="right-next"
       href="bread.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">The Poincaré Problem</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#word-frequencies">Word Frequencies</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#zipfs-law">Zipf’s Law</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#tail-distribution">Tail Distribution</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#fitting-a-model">Fitting a Model</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-update">The Update</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Allen B. Downey
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>