

<!DOCTYPE html>


<html lang="en" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

    <title>What’s a chartist? &#8212; Think Bayes</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=ac02cc09edc035673794" rel="stylesheet" />
<link href="_static/styles/bootstrap.css?digest=ac02cc09edc035673794" rel="stylesheet" />
<link href="_static/styles/pydata-sphinx-theme.css?digest=ac02cc09edc035673794" rel="stylesheet" />

  
  <link href="_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=ac02cc09edc035673794" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
    <link rel="stylesheet" href="_static/styles/sphinx-book-theme.css?digest=14f4ca6b54d191a8c7657f6c759bf11a5fb86285" type="text/css" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="_static/design-style.4045f2051d55cab465a707391d5b2007.min.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/bootstrap.js?digest=ac02cc09edc035673794" />
<link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=ac02cc09edc035673794" />
  <script src="_static/vendor/fontawesome/6.1.2/js/all.min.js?digest=ac02cc09edc035673794"></script>

    <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script src="_static/scripts/sphinx-book-theme.js?digest=5a5c038af52cf7bc1a1ec88eea08e6366ee68824"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"tex2jax": {"inlineMath": [["$", "$"], ["\\(", "\\)"]]}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'zipf';</script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="prev" title="The All-Knowing Cube of Probability" href="beta_binomial.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a class="skip-link" href="#main-content">Skip to main content</a>
  
  <div id="pst-scroll-pixel-helper"></div>

  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>
    Back to top
  </button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <nav class="bd-header navbar navbar-expand-lg bd-navbar">
    </nav>
  
  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">
  

<a class="navbar-brand logo" href="index.html">
  
  
  
  
  
    <p class="title logo__title">Think Bayes</p>
  
</a></div>
        <div class="sidebar-primary-item"><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="index.html">
                    Think Bayes 2
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Front Matter</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="preface.html">Preface</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Chapters</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="chap01.html">1. Probability</a></li>
<li class="toctree-l1"><a class="reference internal" href="chap02.html">2. Bayes’s Theorem</a></li>
<li class="toctree-l1"><a class="reference internal" href="chap03.html">3. Distributions</a></li>
<li class="toctree-l1"><a class="reference internal" href="chap04.html">4. Estimating Proportions</a></li>
<li class="toctree-l1"><a class="reference internal" href="chap05.html">5. Estimating Counts</a></li>
<li class="toctree-l1"><a class="reference internal" href="chap06.html">6. Odds and Addends</a></li>
<li class="toctree-l1"><a class="reference internal" href="chap07.html">7. Minimum, Maximum, and Mixture</a></li>
<li class="toctree-l1"><a class="reference internal" href="chap08.html">8. Poisson Processes</a></li>
<li class="toctree-l1"><a class="reference internal" href="chap09.html">9. Decision Analysis</a></li>
<li class="toctree-l1"><a class="reference internal" href="chap10.html">10. Testing</a></li>
<li class="toctree-l1"><a class="reference internal" href="chap11.html">11. Comparison</a></li>
<li class="toctree-l1"><a class="reference internal" href="chap12.html">12. Classification</a></li>
<li class="toctree-l1"><a class="reference internal" href="chap13.html">13. Inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="chap14.html">14. Survival Analysis</a></li>
<li class="toctree-l1"><a class="reference internal" href="chap15.html">15. Mark and Recapture</a></li>
<li class="toctree-l1"><a class="reference internal" href="chap16.html">16. Logistic Regression</a></li>
<li class="toctree-l1"><a class="reference internal" href="chap17.html">17. Regression</a></li>
<li class="toctree-l1"><a class="reference internal" href="chap18.html">18. Conjugate Priors</a></li>
<li class="toctree-l1"><a class="reference internal" href="chap19_v3.html">19. MCMC</a></li>
<li class="toctree-l1"><a class="reference internal" href="chap20.html">20. Approximate Bayesian Computation</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Examples</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="redline.html">The Red Line Problem</a></li>
<li class="toctree-l1"><a class="reference internal" href="redline_pymc.html">The Red Line Problem</a></li>
<li class="toctree-l1"><a class="reference internal" href="vaccine2.html">Estimating vaccine efficacy</a></li>
<li class="toctree-l1"><a class="reference internal" href="usb.html">Flipping USB Connectors</a></li>
<li class="toctree-l1"><a class="reference internal" href="sister.html">The Left Handed Sister Problem</a></li>
<li class="toctree-l1"><a class="reference internal" href="bayes_dice.html">Bayesian Dice</a></li>
<li class="toctree-l1"><a class="reference internal" href="radiation.html">The Emitter-Detector Problem</a></li>
<li class="toctree-l1"><a class="reference internal" href="hospital.html">Grid algorithms for hierarchical models</a></li>
<li class="toctree-l1"><a class="reference internal" href="hospital_birth_rate.html">Comparing birth rates</a></li>
<li class="toctree-l1"><a class="reference internal" href="ok.html">How Many Typos?</a></li>
<li class="toctree-l1"><a class="reference internal" href="bookstore.html">How Many Books?</a></li>
<li class="toctree-l1"><a class="reference internal" href="beta_binomial.html">The All-Knowing Cube of Probability</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">What’s a chartist?</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">


<a href="https://github.com/AllenDowney/ThinkBayes2" target="_blank"
   class="btn btn-sm btn-source-repository-button"
   title="Source repository"
   data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>

</a>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="_sources/zipf.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>


<script>
document.write(`
  <button class="btn btn-sm navbar-btn theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch nav-link" data-mode="light"><i class="fa-solid fa-sun fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="dark"><i class="fa-solid fa-moon fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="auto"><i class="fa-solid fa-circle-half-stroke fa-lg"></i></span>
  </button>
`);
</script>

<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>What’s a chartist?</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#word-frequencies">Word Frequencies</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#zipfs-law">Zipf’s Law</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#tail-distribution">Tail Distribution</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#fitting-a-model">Fitting a Model</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-update">The Update</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article" role="main">
                  
  <section id="whats-a-chartist">
<h1>What’s a chartist?<a class="headerlink" href="#whats-a-chartist" title="Permalink to this heading">#</a></h1>
<p>Recently I heard the word “chartist” for the first time in my life (that I recall).
And then later the same day, I heard it again.
So that raises two questions:</p>
<ul class="simple">
<li><p>What are the chances of going 57 years without hearing a word, and then hearing it twice in one day?</p></li>
<li><p>Also, what’s a chartist?</p></li>
</ul>
<p>To answer the second question first, it’s someone who supported chartism, which was “a working-class movement for political reform in the United Kingdom that erupted from 1838 to 1857”, quoth <a class="reference external" href="https://en.wikipedia.org/wiki/Chartism">Wikipedia</a>.  The name comes from the People’s Charter of 1838, which called for voting rights for unpropertied men, among other reforms.</p>
<p>To answer the first question, we’ll do some Bayesian statistics.
My solution is based on a model that’s not very realistic, so we should not take the result too seriously, but it demonstrates some interesting methods, I think.
And as you’ll see, there is a connection to Zipf’s law, <a class="reference external" href="https://www.allendowney.com/blog/2024/11/10/zipfs-law/">which I wrote about last week</a>.</p>
<p>Since last week’s post was at the beginner level, I should warn you that this one is more advanced – in rapid succession, it involves the beta distribution, the <span class="math notranslate nohighlight">\(t\)</span> distribution, the negative binomial, and the binomial.</p>
<p>This post is based on <em>Think Bayes 2e</em>, which is available from
<a class="reference external" href="https://bookshop.org/a/98697/9781492089469">Bookshop.org</a> and
<a class="reference external" href="https://amzn.to/334eqGo">Amazon</a>.</p>
<div class="cell tag_hide-cell docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell content</span>
<span class="expanded">Hide code cell content</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">try</span><span class="p">:</span>
    <span class="kn">import</span> <span class="nn">empiricaldist</span>
<span class="k">except</span> <span class="ne">ImportError</span><span class="p">:</span>
    <span class="o">!</span>pip<span class="w"> </span>install<span class="w"> </span>empiricaldist
</pre></div>
</div>
</div>
</details>
</div>
<div class="cell tag_hide-cell docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell content</span>
<span class="expanded">Hide code cell content</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># download thinkdsp.py</span>

<span class="kn">from</span> <span class="nn">os.path</span> <span class="kn">import</span> <span class="n">basename</span><span class="p">,</span> <span class="n">exists</span>


<span class="k">def</span> <span class="nf">download</span><span class="p">(</span><span class="n">url</span><span class="p">):</span>
    <span class="n">filename</span> <span class="o">=</span> <span class="n">basename</span><span class="p">(</span><span class="n">url</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">exists</span><span class="p">(</span><span class="n">filename</span><span class="p">):</span>
        <span class="kn">from</span> <span class="nn">urllib.request</span> <span class="kn">import</span> <span class="n">urlretrieve</span>

        <span class="n">local</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">urlretrieve</span><span class="p">(</span><span class="n">url</span><span class="p">,</span> <span class="n">filename</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Downloaded &quot;</span> <span class="o">+</span> <span class="n">local</span><span class="p">)</span>


<span class="n">download</span><span class="p">(</span><span class="s2">&quot;https://github.com/AllenDowney/ThinkBayes2/raw/master/soln/utils.py&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</details>
</div>
<div class="cell tag_hide-cell docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell content</span>
<span class="expanded">Hide code cell content</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="kn">from</span> <span class="nn">empiricaldist</span> <span class="kn">import</span> <span class="n">Pmf</span>
<span class="kn">from</span> <span class="nn">utils</span> <span class="kn">import</span> <span class="n">decorate</span>

<span class="n">plt</span><span class="o">.</span><span class="n">rcParams</span><span class="p">[</span><span class="s2">&quot;figure.dpi&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="mi">75</span>
<span class="n">plt</span><span class="o">.</span><span class="n">rcParams</span><span class="p">[</span><span class="s2">&quot;figure.figsize&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="mi">6</span><span class="p">,</span> <span class="mf">3.5</span><span class="p">]</span>
</pre></div>
</div>
</div>
</details>
</div>
<section id="word-frequencies">
<h2>Word Frequencies<a class="headerlink" href="#word-frequencies" title="Permalink to this heading">#</a></h2>
<p>If you don’t hear a word for more than 50 years, that suggests it is not a common word.
We can use Bayes’s theorem to quantify this intuition.
First we’ll compute the posterior distribution of the word’s frequency, then the posterior predictive distribution of hearing it again within a day.</p>
<p>Because we have only one piece of data – the time until first appearance – we’ll need a good prior distribution.
Which means we’ll need a large, good quality sample of English text.
For that, I’ll use a free sample of the COCA dataset from <a class="reference external" href="https://www.corpusdata.org/formats.asp">CorpusData.org</a>. The following cells download and read the data.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">download</span><span class="p">(</span><span class="s2">&quot;https://www.corpusdata.org/coca/samples/coca-samples-text.zip&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">zipfile</span>


<span class="k">def</span> <span class="nf">generate_lines</span><span class="p">(</span><span class="n">zip_path</span><span class="o">=</span><span class="s2">&quot;coca-samples-text.zip&quot;</span><span class="p">):</span>
    <span class="k">with</span> <span class="n">zipfile</span><span class="o">.</span><span class="n">ZipFile</span><span class="p">(</span><span class="n">zip_path</span><span class="p">,</span> <span class="s2">&quot;r&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">zip_file</span><span class="p">:</span>
        <span class="n">file_list</span> <span class="o">=</span> <span class="n">zip_file</span><span class="o">.</span><span class="n">namelist</span><span class="p">()</span>
        <span class="k">for</span> <span class="n">file_name</span> <span class="ow">in</span> <span class="n">file_list</span><span class="p">:</span>
            <span class="k">with</span> <span class="n">zip_file</span><span class="o">.</span><span class="n">open</span><span class="p">(</span><span class="n">file_name</span><span class="p">)</span> <span class="k">as</span> <span class="n">file</span><span class="p">:</span>
                <span class="n">lines</span> <span class="o">=</span> <span class="n">file</span><span class="o">.</span><span class="n">readlines</span><span class="p">()</span>
                <span class="k">for</span> <span class="n">line</span> <span class="ow">in</span> <span class="n">lines</span><span class="p">:</span>
                    <span class="k">yield</span> <span class="p">(</span><span class="n">line</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="s2">&quot;utf-8&quot;</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<p>We’ll use a <code class="docutils literal notranslate"><span class="pre">Counter</span></code> to count the number of times each word appears.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">re</span>
<span class="kn">from</span> <span class="nn">collections</span> <span class="kn">import</span> <span class="n">Counter</span>

<span class="n">counter</span> <span class="o">=</span> <span class="n">Counter</span><span class="p">()</span>

<span class="n">pattern</span> <span class="o">=</span> <span class="sa">r</span><span class="s2">&quot;[ /\n]+|--&quot;</span>

<span class="k">for</span> <span class="n">line</span> <span class="ow">in</span> <span class="n">generate_lines</span><span class="p">():</span>
    <span class="n">words</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">pattern</span><span class="p">,</span> <span class="n">line</span><span class="p">)[</span><span class="mi">1</span><span class="p">:]</span>
    <span class="n">counter</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">word</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span> <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">words</span> <span class="k">if</span> <span class="n">word</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>The dataset includes about 188,000 unique strings, but not all of them are what we would consider words.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">len</span><span class="p">(</span><span class="n">counter</span><span class="p">),</span> <span class="n">counter</span><span class="o">.</span><span class="n">total</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(188086, 11503819)
</pre></div>
</div>
</div>
</div>
<p>To narrow it down, I’ll remove anything that starts or ends with a non-alphabetical character – so hyphens and apostrophes are allowed in the middle of a word.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="nb">list</span><span class="p">(</span><span class="n">counter</span><span class="o">.</span><span class="n">keys</span><span class="p">()):</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">s</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">isalpha</span><span class="p">()</span> <span class="ow">or</span> <span class="ow">not</span> <span class="n">s</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">isalpha</span><span class="p">():</span>
        <span class="k">del</span> <span class="n">counter</span><span class="p">[</span><span class="n">s</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<p>This filter reduces the number of unique words to about 151,000.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">num_words</span> <span class="o">=</span> <span class="n">counter</span><span class="o">.</span><span class="n">total</span><span class="p">()</span>
<span class="nb">len</span><span class="p">(</span><span class="n">counter</span><span class="p">),</span> <span class="n">num_words</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(151414, 8889694)
</pre></div>
</div>
</div>
</div>
<p>The most common words are what you would expect, with the exception of “n’t”, which is there because the COCA corpus treats it as a separate word.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">counter</span><span class="o">.</span><span class="n">most_common</span><span class="p">(</span><span class="mi">20</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[(&#39;the&#39;, 461991),
 (&#39;to&#39;, 237929),
 (&#39;and&#39;, 231459),
 (&#39;of&#39;, 217363),
 (&#39;a&#39;, 203302),
 (&#39;in&#39;, 153323),
 (&#39;i&#39;, 137931),
 (&#39;that&#39;, 123818),
 (&#39;you&#39;, 109635),
 (&#39;it&#39;, 103712),
 (&#39;is&#39;, 93996),
 (&#39;for&#39;, 78755),
 (&#39;on&#39;, 64869),
 (&#39;was&#39;, 64388),
 (&#39;with&#39;, 59724),
 (&#39;he&#39;, 57684),
 (&#39;this&#39;, 51879),
 (&#39;as&#39;, 51202),
 (&quot;n&#39;t&quot;, 49291),
 (&#39;we&#39;, 47694)]
</pre></div>
</div>
</div>
</div>
<p>There are about 72,000 words that only appear once in the corpus, technically known as <a class="reference external" href="https://en.wikipedia.org/wiki/Hapax_legomenon">hapax legomena</a>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">singletons</span> <span class="o">=</span> <span class="p">[</span><span class="n">word</span> <span class="k">for</span> <span class="p">(</span><span class="n">word</span><span class="p">,</span> <span class="n">freq</span><span class="p">)</span> <span class="ow">in</span> <span class="n">counter</span><span class="o">.</span><span class="n">items</span><span class="p">()</span> <span class="k">if</span> <span class="n">freq</span> <span class="o">==</span> <span class="mi">1</span><span class="p">]</span>
<span class="nb">len</span><span class="p">(</span><span class="n">singletons</span><span class="p">),</span> <span class="nb">len</span><span class="p">(</span><span class="n">singletons</span><span class="p">)</span> <span class="o">/</span> <span class="n">counter</span><span class="o">.</span><span class="n">total</span><span class="p">()</span> <span class="o">*</span> <span class="mi">100</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(72159, 0.811715228893143)
</pre></div>
</div>
</div>
</div>
<p>Here’s a random selection of them. Many are proper names, typos, or other non-words, but some are legitimate but rare words.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">singletons</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([&#39;xcor&#39;, &#39;metress&#39;, &#39;commonspace&#39;, &#39;attilan&#39;, &#39;nutritus&#39;,
       &#39;under-estimated&#39;, &#39;danci&#39;, &#39;thoughness&#39;, &#39;gmulder&#39;, &#39;multigrade&#39;,
       &#39;tazzarine&#39;, &#39;well-remembered&#39;, &#39;snapchat&#39;, &#39;yt&#39;,
       &quot;everything&#39;sback&quot;, &#39;moonclown&#39;, &#39;maschek&#39;, &#39;infront&#39;, &#39;meowing&#39;,
       &#39;unhorses&#39;, &#39;waitressed&#39;, &#39;getbuckyballs.com&#39;, &#39;eye-rolling&#39;,
       &#39;right.follow&#39;, &#39;al-maliki&#39;, &#39;where-it&#39;, &#39;candelabras&#39;,
       &#39;trillion-dollar&#39;, &#39;poltically&#39;, &#39;way-stone&#39;, &#39;end-of-empire&#39;,
       &#39;antiforgiveness&#39;, &#39;noncommunicative&#39;, &#39;astronomical-sized&#39;,
       &#39;ms-like&#39;, &#39;colicky&#39;, &#39;mightly&#39;, &#39;lynsey&#39;, &#39;fifield&#39;,
       &#39;consummately&#39;, &#39;oursega&#39;, &#39;steplewski&#39;, &#39;businessleaders&#39;,
       &#39;pacifies&#39;, &#39;post-linsanity&#39;, &#39;high-born&#39;, &#39;okay.bye&#39;, &#39;mini-camp&#39;,
       &#39;than-expected&#39;, &#39;x-lab&#39;, &#39;www.visitturin2006.com&#39;,
       &#39;decreasing-there&#39;, &#39;kleiner&#39;, &#39;cosher&#39;, &#39;drm&#39;, &#39;castleman&#39;,
       &#39;treelet&#39;, &#39;ostapowicz&#39;, &#39;gerrymander&#39;, &#39;kibitzing&#39;, &#39;resequenced&#39;,
       &#39;goat-man&#39;, &#39;drenaje&#39;, &#39;tionist&#39;, &#39;betrothed&#39;, &#39;cannondale.com&#39;,
       &#39;praddock&#39;, &#39;napoleanic&#39;, &#39;tiltfilter&#39;, &#39;gowe&#39;, &#39;marchev&#39;, &#39;fugly&#39;,
       &#39;mouthit&#39;, &#39;blumberg&#39;, &#39;langone&#39;, &#39;self-superiority&#39;, &#39;etsu&#39;,
       &#39;friesian&#39;, &#39;blixt&#39;, &#39;couzens&#39;, &#39;firestorms&#39;, &#39;headachy&#39;,
       &#39;bisected&#39;, &#39;reel-to-reel&#39;, &#39;obscurant&#39;, &#39;with.the&#39;, &#39;yelp.com&#39;,
       &#39;frostily&#39;, &#39;hemsehus&#39;, &#39;pa-lin&#39;, &#39;sun-blocking&#39;, &#39;baerga&#39;,
       &#39;rightinfrontofyou&#39;, &#39;guangxi&#39;, &#39;jeiky&#39;, &#39;babbacombe&#39;,
       &#39;deborahlchamberlain&#39;, &#39;counterbalancing&#39;, &#39;baupoint&#39;, &#39;gowned&#39;],
      dtype=&#39;&lt;U24&#39;)
</pre></div>
</div>
</div>
</div>
<p>Now let’s see what the distribution of word frequencies looks like.</p>
</section>
<section id="zipfs-law">
<h2>Zipf’s Law<a class="headerlink" href="#zipfs-law" title="Permalink to this heading">#</a></h2>
<p>One way to visualize the distribution is a Zipf plot, which shows the ranks on the x-axis and the frequencies on the y-axis.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">freqs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="nb">sorted</span><span class="p">(</span><span class="n">counter</span><span class="o">.</span><span class="n">values</span><span class="p">(),</span> <span class="n">reverse</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">n</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">freqs</span><span class="p">)</span>
<span class="n">ranks</span> <span class="o">=</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">n</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Here’s what it looks like on a log-log scale.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">ranks</span><span class="p">,</span> <span class="n">freqs</span><span class="p">)</span>

<span class="n">decorate</span><span class="p">(</span>
    <span class="n">title</span><span class="o">=</span><span class="s2">&quot;Zipf plot&quot;</span><span class="p">,</span> <span class="n">xlabel</span><span class="o">=</span><span class="s2">&quot;Rank&quot;</span><span class="p">,</span> <span class="n">ylabel</span><span class="o">=</span><span class="s2">&quot;Frequency&quot;</span><span class="p">,</span> <span class="n">xscale</span><span class="o">=</span><span class="s2">&quot;log&quot;</span><span class="p">,</span> <span class="n">yscale</span><span class="o">=</span><span class="s2">&quot;log&quot;</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/204f4aae3fe537fefdbe43abadd4be2a854bc627c7d5e064d9efebd9cc6a58df.png" src="_images/204f4aae3fe537fefdbe43abadd4be2a854bc627c7d5e064d9efebd9cc6a58df.png" />
</div>
</div>
<p>Zipf’s law suggest that the result should be a straight line with slope close to -1.
It’s not exactly a straight line, but it’s close, and the slope is about -1.1.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">rise</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">log10</span><span class="p">(</span><span class="n">freqs</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">log10</span><span class="p">(</span><span class="n">freqs</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="n">rise</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>-5.664633515191604
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">run</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">log10</span><span class="p">(</span><span class="n">ranks</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">log10</span><span class="p">(</span><span class="n">ranks</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="n">run</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>5.180166032638616
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">rise</span> <span class="o">/</span> <span class="n">run</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>-1.0935235433575892
</pre></div>
</div>
</div>
</div>
<p>The Zipf plot is a well-known visual representation of the distribution of frequencies, but for the current problem, we’ll switch to a different representation.</p>
</section>
<section id="tail-distribution">
<h2>Tail Distribution<a class="headerlink" href="#tail-distribution" title="Permalink to this heading">#</a></h2>
<p>Given the number of times each word appear in the corpus, we can compute the rates, which is the number of times we expect each word to appear in a sample of a given size, and the inverse rates, which are the number of words we need to see before we expect a given word to appear.</p>
<p>We will find it most convenient to work with the distribution of inverse rates on a log scale.
The first step is to use the observed frequencies to estimate word rates – we’ll estimate the rate at which each word would appear in a random sample.</p>
<p>We’ll do that by creating a beta distribution that represents the posterior distribution of word rates, given the observed frequencies (see <a class="reference external" href="https://allendowney.github.io/ThinkBayes2/chap18.html#the-conjugate-prior">this section of <em>Think Bayes</em></a>) – and then drawing a random sample from the posterior.
So words that have the same frequency will not generally have the same inferred rate.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">scipy.stats</span> <span class="kn">import</span> <span class="n">beta</span>

<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">17</span><span class="p">)</span>
<span class="n">alphas</span> <span class="o">=</span> <span class="n">freqs</span> <span class="o">+</span> <span class="mi">1</span>
<span class="n">betas</span> <span class="o">=</span> <span class="n">num_words</span> <span class="o">-</span> <span class="n">freqs</span> <span class="o">+</span> <span class="mi">1</span>
<span class="n">inferred_rates</span> <span class="o">=</span> <span class="n">beta</span><span class="p">(</span><span class="n">alphas</span><span class="p">,</span> <span class="n">betas</span><span class="p">)</span><span class="o">.</span><span class="n">rvs</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<p>Now we can compute the inverse rates, which are the number of words we have to sample before we expect to see each word once.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">inverse_rates</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">/</span> <span class="n">inferred_rates</span>
</pre></div>
</div>
</div>
</div>
<p>And here are their magnitudes, expressed as logarithms base 10.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">mags</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">log10</span><span class="p">(</span><span class="n">inverse_rates</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>To represent the distribution of these magnitudes, we’ll use a <code class="docutils literal notranslate"><span class="pre">Surv</span></code> object, which represents survival functions, but we’ll use a variation of the survival function which is the probability that a randomly-chosen value is greater than or equal to a given quantity.
The following function computes this version of a survival function, which is called a tail probability.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">empiricaldist</span> <span class="kn">import</span> <span class="n">Surv</span>


<span class="k">def</span> <span class="nf">make_surv</span><span class="p">(</span><span class="n">seq</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Make a non-standard survival function, P(X&gt;=x)&quot;&quot;&quot;</span>
    <span class="n">pmf</span> <span class="o">=</span> <span class="n">Pmf</span><span class="o">.</span><span class="n">from_seq</span><span class="p">(</span><span class="n">seq</span><span class="p">)</span>
    <span class="n">surv</span> <span class="o">=</span> <span class="n">pmf</span><span class="o">.</span><span class="n">make_surv</span><span class="p">()</span> <span class="o">+</span> <span class="n">pmf</span>

    <span class="c1"># correct for numerical error</span>
    <span class="n">surv</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="k">return</span> <span class="n">Surv</span><span class="p">(</span><span class="n">surv</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Here’s how we make the survival function.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">surv</span> <span class="o">=</span> <span class="n">make_surv</span><span class="p">(</span><span class="n">mags</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>And here’s what it looks like.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">surv</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">marker</span><span class="o">=</span><span class="s2">&quot;.&quot;</span><span class="p">,</span> <span class="n">ms</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;data&quot;</span><span class="p">)</span>
<span class="n">decorate</span><span class="p">(</span><span class="n">xlabel</span><span class="o">=</span><span class="s2">&quot;Inverse rate (log10 words per appearance)&quot;</span><span class="p">,</span> <span class="n">ylabel</span><span class="o">=</span><span class="s2">&quot;Tail probability&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/078987501b2d709bae40b54221d0c4c7338c85e16c542b44f5e34f3e623b329a.png" src="_images/078987501b2d709bae40b54221d0c4c7338c85e16c542b44f5e34f3e623b329a.png" />
</div>
</div>
<p>The tail distribution has the sigmoid shape that is characteristic of normal distributions and <span class="math notranslate nohighlight">\(t\)</span> distributions, although it is notably asymmetric.</p>
<p>And here’s what the tail probabilities look like on a log-y scale.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">surv</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">marker</span><span class="o">=</span><span class="s2">&quot;.&quot;</span><span class="p">,</span> <span class="n">ms</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;data&quot;</span><span class="p">)</span>
<span class="n">decorate</span><span class="p">(</span><span class="n">xlabel</span><span class="o">=</span><span class="s2">&quot;Inverse rate (words per appearance)&quot;</span><span class="p">,</span> <span class="n">yscale</span><span class="o">=</span><span class="s2">&quot;log&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/86a4688865a9f94cbcdca451633b01acdd1b3020fd9d4de6dd77ba3c23e7d931.png" src="_images/86a4688865a9f94cbcdca451633b01acdd1b3020fd9d4de6dd77ba3c23e7d931.png" />
</div>
</div>
<p>If this distribution were normal, we would expect this curve to drop off with increasing slope.
But for the words with the lowest frequencies – that is, the highest inverse rates – it is almost a straight line.
And that suggests that a <span class="math notranslate nohighlight">\(t\)</span> distribution might be a good model for this data.</p>
</section>
<section id="fitting-a-model">
<h2>Fitting a Model<a class="headerlink" href="#fitting-a-model" title="Permalink to this heading">#</a></h2>
<p>To estimate the frequency of rare words, we will need to model the tail behavior of this distribution and extrapolate it beyond the data.
So let’s fit a <span class="math notranslate nohighlight">\(t\)</span> distribution and see how it looks.
I’ll use code from <a class="reference external" href="https://allendowney.github.io/ProbablyOverthinkingIt/longtail.html">Chapter 8 of <em>Probably Overthinking It</em></a>, which is all about these long-tailed distributions.</p>
<p>The following function makes a <code class="docutils literal notranslate"><span class="pre">Surv</span></code> object that represents a <span class="math notranslate nohighlight">\(t\)</span> distribution with the given parameters.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">scipy.stats</span> <span class="kn">import</span> <span class="n">t</span> <span class="k">as</span> <span class="n">t_dist</span>


<span class="k">def</span> <span class="nf">truncated_t_sf</span><span class="p">(</span><span class="n">qs</span><span class="p">,</span> <span class="n">df</span><span class="p">,</span> <span class="n">mu</span><span class="p">,</span> <span class="n">sigma</span><span class="p">):</span>
    <span class="n">ps</span> <span class="o">=</span> <span class="n">t_dist</span><span class="o">.</span><span class="n">sf</span><span class="p">(</span><span class="n">qs</span><span class="p">,</span> <span class="n">df</span><span class="p">,</span> <span class="n">mu</span><span class="p">,</span> <span class="n">sigma</span><span class="p">)</span>
    <span class="n">surv_model</span> <span class="o">=</span> <span class="n">Surv</span><span class="p">(</span><span class="n">ps</span> <span class="o">/</span> <span class="n">ps</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">qs</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">surv_model</span>
</pre></div>
</div>
</div>
</div>
<p>If we are given the <code class="docutils literal notranslate"><span class="pre">df</span></code> parameter, we can use the following function to find the values of <code class="docutils literal notranslate"><span class="pre">mu</span></code> and <code class="docutils literal notranslate"><span class="pre">sigma</span></code> that best fit the data, focusing on the central part of the distribution.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">scipy.optimize</span> <span class="kn">import</span> <span class="n">least_squares</span>


<span class="k">def</span> <span class="nf">fit_truncated_t</span><span class="p">(</span><span class="n">df</span><span class="p">,</span> <span class="n">surv</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Given df, find the best values of mu and sigma.&quot;&quot;&quot;</span>
    <span class="n">low</span><span class="p">,</span> <span class="n">high</span> <span class="o">=</span> <span class="n">surv</span><span class="o">.</span><span class="n">qs</span><span class="o">.</span><span class="n">min</span><span class="p">(),</span> <span class="n">surv</span><span class="o">.</span><span class="n">qs</span><span class="o">.</span><span class="n">max</span><span class="p">()</span>
    <span class="n">qs_model</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">low</span><span class="p">,</span> <span class="n">high</span><span class="p">,</span> <span class="mi">1000</span><span class="p">)</span>
    <span class="n">ps</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">,</span> <span class="mi">20</span><span class="p">)</span>
    <span class="n">qs</span> <span class="o">=</span> <span class="n">surv</span><span class="o">.</span><span class="n">inverse</span><span class="p">(</span><span class="n">ps</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">error_func_t</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">df</span><span class="p">,</span> <span class="n">surv</span><span class="p">):</span>
        <span class="n">mu</span><span class="p">,</span> <span class="n">sigma</span> <span class="o">=</span> <span class="n">params</span>
        <span class="n">surv_model</span> <span class="o">=</span> <span class="n">truncated_t_sf</span><span class="p">(</span><span class="n">qs_model</span><span class="p">,</span> <span class="n">df</span><span class="p">,</span> <span class="n">mu</span><span class="p">,</span> <span class="n">sigma</span><span class="p">)</span>

        <span class="n">error</span> <span class="o">=</span> <span class="n">surv</span><span class="p">(</span><span class="n">qs</span><span class="p">)</span> <span class="o">-</span> <span class="n">surv_model</span><span class="p">(</span><span class="n">qs</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">error</span>

    <span class="n">pmf</span> <span class="o">=</span> <span class="n">surv</span><span class="o">.</span><span class="n">make_pmf</span><span class="p">()</span>
    <span class="n">pmf</span><span class="o">.</span><span class="n">normalize</span><span class="p">()</span>
    <span class="n">params</span> <span class="o">=</span> <span class="n">pmf</span><span class="o">.</span><span class="n">mean</span><span class="p">(),</span> <span class="n">pmf</span><span class="o">.</span><span class="n">std</span><span class="p">()</span>
    <span class="n">res</span> <span class="o">=</span> <span class="n">least_squares</span><span class="p">(</span><span class="n">error_func_t</span><span class="p">,</span> <span class="n">x0</span><span class="o">=</span><span class="n">params</span><span class="p">,</span> <span class="n">args</span><span class="o">=</span><span class="p">(</span><span class="n">df</span><span class="p">,</span> <span class="n">surv</span><span class="p">),</span> <span class="n">xtol</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">)</span>
    <span class="k">assert</span> <span class="n">res</span><span class="o">.</span><span class="n">success</span>
    <span class="k">return</span> <span class="n">res</span><span class="o">.</span><span class="n">x</span>
</pre></div>
</div>
</div>
</div>
<p>But since we are not given <code class="docutils literal notranslate"><span class="pre">df</span></code>, we can use the following function to search for the value that best fits the tail of the distribution.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">scipy.optimize</span> <span class="kn">import</span> <span class="n">minimize</span>


<span class="k">def</span> <span class="nf">minimize_df</span><span class="p">(</span><span class="n">df0</span><span class="p">,</span> <span class="n">surv</span><span class="p">,</span> <span class="n">bounds</span><span class="o">=</span><span class="p">[(</span><span class="mi">1</span><span class="p">,</span> <span class="mf">1e3</span><span class="p">)],</span> <span class="n">ps</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="n">low</span><span class="p">,</span> <span class="n">high</span> <span class="o">=</span> <span class="n">surv</span><span class="o">.</span><span class="n">qs</span><span class="o">.</span><span class="n">min</span><span class="p">(),</span> <span class="n">surv</span><span class="o">.</span><span class="n">qs</span><span class="o">.</span><span class="n">max</span><span class="p">()</span>
    <span class="n">qs_model</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">low</span><span class="p">,</span> <span class="n">high</span> <span class="o">*</span> <span class="mf">1.2</span><span class="p">,</span> <span class="mi">2000</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">ps</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">t</span> <span class="o">=</span> <span class="n">surv</span><span class="o">.</span><span class="n">ps</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">surv</span><span class="o">.</span><span class="n">ps</span><span class="p">[</span><span class="o">-</span><span class="mi">5</span><span class="p">]</span>
        <span class="n">low</span><span class="p">,</span> <span class="n">high</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">log10</span><span class="p">(</span><span class="n">t</span><span class="p">)</span>
        <span class="n">ps</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">logspace</span><span class="p">(</span><span class="n">low</span><span class="p">,</span> <span class="n">high</span><span class="p">,</span> <span class="mi">30</span><span class="p">,</span> <span class="n">endpoint</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

    <span class="n">qs</span> <span class="o">=</span> <span class="n">surv</span><span class="o">.</span><span class="n">inverse</span><span class="p">(</span><span class="n">ps</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">error_func_tail</span><span class="p">(</span><span class="n">params</span><span class="p">):</span>
        <span class="p">(</span><span class="n">df</span><span class="p">,)</span> <span class="o">=</span> <span class="n">params</span>
        <span class="nb">print</span><span class="p">(</span><span class="n">df</span><span class="p">)</span>
        <span class="n">mu</span><span class="p">,</span> <span class="n">sigma</span> <span class="o">=</span> <span class="n">fit_truncated_t</span><span class="p">(</span><span class="n">df</span><span class="p">,</span> <span class="n">surv</span><span class="p">)</span>
        <span class="n">surv_model</span> <span class="o">=</span> <span class="n">truncated_t_sf</span><span class="p">(</span><span class="n">qs_model</span><span class="p">,</span> <span class="n">df</span><span class="p">,</span> <span class="n">mu</span><span class="p">,</span> <span class="n">sigma</span><span class="p">)</span>

        <span class="n">errors</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">log10</span><span class="p">(</span><span class="n">surv</span><span class="p">(</span><span class="n">qs</span><span class="p">))</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">log10</span><span class="p">(</span><span class="n">surv_model</span><span class="p">(</span><span class="n">qs</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">errors</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>

    <span class="n">params</span> <span class="o">=</span> <span class="p">(</span><span class="n">df0</span><span class="p">,)</span>
    <span class="n">res</span> <span class="o">=</span> <span class="n">minimize</span><span class="p">(</span><span class="n">error_func_tail</span><span class="p">,</span> <span class="n">x0</span><span class="o">=</span><span class="n">params</span><span class="p">,</span> <span class="n">bounds</span><span class="o">=</span><span class="n">bounds</span><span class="p">,</span> <span class="n">tol</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">,</span> <span class="n">method</span><span class="o">=</span><span class="s2">&quot;Powell&quot;</span><span class="p">)</span>
    <span class="k">assert</span> <span class="n">res</span><span class="o">.</span><span class="n">success</span>
    <span class="k">return</span> <span class="n">res</span><span class="o">.</span><span class="n">x</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">df</span> <span class="o">=</span> <span class="n">minimize_df</span><span class="p">(</span><span class="mi">25</span><span class="p">,</span> <span class="n">surv</span><span class="p">)</span>
<span class="n">df</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>25.0
382.58404523885497
618.4159547611448
236.8319095222899
146.75213571656514
91.07977380572478
56.67236191084039
35.407411894884405
22.26495001595599
14.142461878928419
25.697947062229677
20.707337220921392
22.572143210805486
22.390897998699558
22.387674578312218
22.38696883830132
22.386635466210475
22.38630209411468
19.77327093242095
382.58404523885497
618.4159547611448
236.8319095222899
146.75213571656516
91.0797738057248
56.672361910840394
35.40741189488441
22.264950015955993
14.142461878928424
25.697947054112568
20.7073371943388
22.572143293741753
22.390897870539447
22.38767398796015
22.386983906149368
22.386650572810865
22.38731723948787
</pre></div>
</div>
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([22.38698391])
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">mu</span><span class="p">,</span> <span class="n">sigma</span> <span class="o">=</span> <span class="n">fit_truncated_t</span><span class="p">(</span><span class="n">df</span><span class="p">,</span> <span class="n">surv</span><span class="p">)</span>
<span class="n">df</span><span class="p">,</span> <span class="n">mu</span><span class="p">,</span> <span class="n">sigma</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(array([22.38698391]), 6.430702047528606, 0.490849531484811)
</pre></div>
</div>
</div>
</div>
<p>Here’s the <code class="docutils literal notranslate"><span class="pre">t</span></code> distribution that best fits the data.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">low</span><span class="p">,</span> <span class="n">high</span> <span class="o">=</span> <span class="n">surv</span><span class="o">.</span><span class="n">qs</span><span class="o">.</span><span class="n">min</span><span class="p">(),</span> <span class="n">surv</span><span class="o">.</span><span class="n">qs</span><span class="o">.</span><span class="n">max</span><span class="p">()</span>
<span class="n">qs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">low</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">2000</span><span class="p">)</span>
<span class="n">surv_model</span> <span class="o">=</span> <span class="n">truncated_t_sf</span><span class="p">(</span><span class="n">qs</span><span class="p">,</span> <span class="n">df</span><span class="p">,</span> <span class="n">mu</span><span class="p">,</span> <span class="n">sigma</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">surv_model</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">color</span><span class="o">=</span><span class="s2">&quot;gray&quot;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.4</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;model&quot;</span><span class="p">)</span>
<span class="n">surv</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">marker</span><span class="o">=</span><span class="s2">&quot;.&quot;</span><span class="p">,</span> <span class="n">ms</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;data&quot;</span><span class="p">)</span>
<span class="n">decorate</span><span class="p">(</span><span class="n">xlabel</span><span class="o">=</span><span class="s2">&quot;Inverse rate (log10 words per appearance)&quot;</span><span class="p">,</span> <span class="n">ylabel</span><span class="o">=</span><span class="s2">&quot;Tail probability&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/27abde601ec8e4c26eaf981a39b75aa33767a315458187d9770ccf75319fcbb4.png" src="_images/27abde601ec8e4c26eaf981a39b75aa33767a315458187d9770ccf75319fcbb4.png" />
</div>
</div>
<p>With the y-axis on a linear scale, we can see that the model fits the data reasonably well, except for a range between 5 and 6 – that is for words that appear about 1 time in a million.</p>
<p>Here’s what the model looks like on a log-y scale.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">surv_model</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">color</span><span class="o">=</span><span class="s2">&quot;gray&quot;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.4</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;model&quot;</span><span class="p">)</span>
<span class="n">surv</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">marker</span><span class="o">=</span><span class="s2">&quot;.&quot;</span><span class="p">,</span> <span class="n">ms</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;data&quot;</span><span class="p">)</span>
<span class="n">decorate</span><span class="p">(</span>
    <span class="n">xlabel</span><span class="o">=</span><span class="s2">&quot;Inverse rate (log10 words per appearance)&quot;</span><span class="p">,</span>
    <span class="n">ylabel</span><span class="o">=</span><span class="s2">&quot;Tail probability&quot;</span><span class="p">,</span>
    <span class="n">yscale</span><span class="o">=</span><span class="s2">&quot;log&quot;</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/b898ebaee16466d30a4337765c2bfd7db73d72148e687745ccb097b7069be8f8.png" src="_images/b898ebaee16466d30a4337765c2bfd7db73d72148e687745ccb097b7069be8f8.png" />
</div>
</div>
<p>The model fits the data well in the extreme tail, which is exactly where we need it.
And we can use the model to extrapolate a little beyond the data, to make sure we cover the range that will turn out to be likely in the scenario where we hear a word for this first time after 50 years.</p>
</section>
<section id="the-update">
<h2>The Update<a class="headerlink" href="#the-update" title="Permalink to this heading">#</a></h2>
<p>The model we’ve developed is the distribution of inverse rates for the words that appear in the corpus and, by extrapolation, for additional rare words that didn’t appear in the corpus.
This distribution will be the prior for the Bayesian update.
We just have to convert it from a survival function to a PMF (remembering that these are equivalent representations of the same distribution).</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">prior</span> <span class="o">=</span> <span class="n">surv_model</span><span class="o">.</span><span class="n">make_pmf</span><span class="p">()</span>
<span class="n">prior</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">label</span><span class="o">=</span><span class="s2">&quot;prior&quot;</span><span class="p">)</span>
<span class="n">decorate</span><span class="p">(</span>
    <span class="n">xlabel</span><span class="o">=</span><span class="s2">&quot;Inverse rate (log10 words per appearance)&quot;</span><span class="p">,</span>
    <span class="n">ylabel</span><span class="o">=</span><span class="s2">&quot;Density&quot;</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/93c35ad32d4c66ae0cc74ac35f62a6804f77d0149e67da86c27b66bd44c1d3fc.png" src="_images/93c35ad32d4c66ae0cc74ac35f62a6804f77d0149e67da86c27b66bd44c1d3fc.png" />
</div>
</div>
<p>To compute the likelihood of the observation, we have to transform the inverse rates to probabilities.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">ps</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">power</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="n">prior</span><span class="o">.</span><span class="n">qs</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Now suppose that in a given day, you read or hear 10,000 words in a context where you would notice if you heard a word for the first time.
Here’s the number of words you would hear in 50 years.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">words_per_day</span> <span class="o">=</span> <span class="mi">10_000</span>
<span class="n">days</span> <span class="o">=</span> <span class="mi">50</span> <span class="o">*</span> <span class="mi">365</span>
<span class="n">k</span> <span class="o">=</span> <span class="n">days</span> <span class="o">*</span> <span class="n">words_per_day</span>
<span class="n">k</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>182500000
</pre></div>
</div>
</div>
</div>
<p>Now, what’s the probability that you fail to encounter a word in <code class="docutils literal notranslate"><span class="pre">k</span></code> attempts and then encounter it on the next attempt?
We can answer that with the negative binomial distribution, which computes the probability of getting the <code class="docutils literal notranslate"><span class="pre">n</span></code>th success after <code class="docutils literal notranslate"><span class="pre">k</span></code> failures, for a given probability – or in this case, for  a sequence of possible probabilities.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">scipy.stats</span> <span class="kn">import</span> <span class="n">nbinom</span>

<span class="n">n</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">likelihood</span> <span class="o">=</span> <span class="n">nbinom</span><span class="o">.</span><span class="n">pmf</span><span class="p">(</span><span class="n">k</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">ps</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>With this likelihood and the prior, we can compute the posterior distribution in the usual way.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">posterior</span> <span class="o">=</span> <span class="n">prior</span> <span class="o">*</span> <span class="n">likelihood</span>
<span class="n">posterior</span><span class="o">.</span><span class="n">normalize</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>1.3581072166387401e-11
</pre></div>
</div>
</div>
</div>
<p>And here’s what it looks like.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">prior</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;prior&quot;</span><span class="p">)</span>
<span class="n">posterior</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">label</span><span class="o">=</span><span class="s2">&quot;posterior&quot;</span><span class="p">)</span>
<span class="n">decorate</span><span class="p">(</span>
    <span class="n">xlabel</span><span class="o">=</span><span class="s2">&quot;Inverse rate (log10 words per appearance)&quot;</span><span class="p">,</span>
    <span class="n">ylabel</span><span class="o">=</span><span class="s2">&quot;Density&quot;</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/707e01c74906fd7de4a642353cbe839bf595b752d455aa826ce7ab0a8987bbe9.png" src="_images/707e01c74906fd7de4a642353cbe839bf595b752d455aa826ce7ab0a8987bbe9.png" />
</div>
</div>
<p>If you go 50 years without hearing a word, that suggests that it is a rare word, and the posterior distribution reflects that logic.</p>
<p>The posterior distribution represents a range of possible values for the inverse rate of the word you heard.
Now we can use it to answer the question we started with: what is the probability of hearing the same word again on the same day – that is, within the next 10,000 words you hear?</p>
<p>To answer that, we can use the survival function of the <a class="reference external" href="https://allendowney.github.io/ThinkBayes2/chap18.html?highlight=binomial#binomial-likelihood">binomial distribution</a> to compute the probability of more than 0 successes in the next <code class="docutils literal notranslate"><span class="pre">n_pred</span></code> attempts.
We’ll compute this probability for each of the <code class="docutils literal notranslate"><span class="pre">ps</span></code> that correspond to the inverse rates in the posterior.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">scipy.stats</span> <span class="kn">import</span> <span class="n">binom</span>

<span class="n">n_pred</span> <span class="o">=</span> <span class="n">words_per_day</span>
<span class="n">ps_pred</span> <span class="o">=</span> <span class="n">binom</span><span class="o">.</span><span class="n">sf</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">n_pred</span><span class="p">,</span> <span class="n">ps</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>And we can use the probabilities in the posterior to compute the expected value – by the law of total probability, the result is the probability of hearing the same word again within a day.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">p</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">posterior</span> <span class="o">*</span> <span class="n">ps_pred</span><span class="p">)</span>
<span class="n">p</span><span class="p">,</span> <span class="mi">1</span> <span class="o">/</span> <span class="n">p</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(0.00016009921991573168, 6246.1266240169725)
</pre></div>
</div>
</div>
</div>
<p>The result is about 1 in 6000.</p>
<p>With all of the assumptions we made in this calculation, there’s no reason to be more precise than that.
And as I mentioned at the beginning, we should probably not take this conclusion to seriously.
If you hear a word for the first time after 50 years, there’s a good chance the word is “having a moment”, which greatly increases the chance you’ll hear it again.
I can’t think of why chartism might be in the news at the moment, but maybe this post will go viral and make it happen.</p>
<p>Copyright 2024 Allen B. Downey</p>
<p>License: <a class="reference external" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">Attribution-NonCommercial-ShareAlike 4.0 International (CC BY-NC-SA 4.0)</a></p>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer">
                  <!-- Previous / next buttons -->
<div class="prev-next-area">
    <a class="left-prev"
       href="beta_binomial.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">The All-Knowing Cube of Probability</p>
      </div>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">

  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#word-frequencies">Word Frequencies</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#zipfs-law">Zipf’s Law</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#tail-distribution">Tail Distribution</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#fitting-a-model">Fitting a Model</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-update">The Update</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Allen B. Downey
</p>

  </div>
  
  <div class="footer-item">
    
  <p class="copyright">
    
      © Copyright 2022.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="_static/scripts/bootstrap.js?digest=ac02cc09edc035673794"></script>
<script src="_static/scripts/pydata-sphinx-theme.js?digest=ac02cc09edc035673794"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>