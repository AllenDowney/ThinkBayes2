
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Radiation &#8212; Think Bayes</title>
    
  <link rel="stylesheet" href="_static/css/index.f658d18f9b420779cfdf24aa0a7e2d77.css">

    
  <link rel="stylesheet"
    href="_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      
  <link rel="stylesheet"
    href="_static/vendor/open-sans_all/1.44.1/index.css">
  <link rel="stylesheet"
    href="_static/vendor/lato_latin-ext/1.44.1/index.css">

    
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="_static/sphinx-book-theme.c441f2ba0852f4cabcb80105e3a46ae6.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="_static/js/index.d3f166471bb80abb5163.js">

    <script id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/togglebutton.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="_static/sphinx-book-theme.7d483ff0a819d6edff12ce0b1ead3928.js"></script>
    <script async="async" src="https://unpkg.com/thebelab@latest/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="_static/sphinx-thebe.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="prev" title="Bayesian Dice" href="bayes_dice.html" />

    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />



  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
<a class="navbar-brand text-wrap" href="index.html">
  
  
  <h1 class="site-logo" id="site-title">Think Bayes</h1>
  
</a>
</div><form class="bd-search d-flex align-items-center" action="search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form>
<nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
    <ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="index.html">
   Think Bayes 2
  </a>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Chapters
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="preface.html">
   Preface
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="chap01.html">
   Probability
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="chap02.html">
   Bayes’s Theorem
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="chap03.html">
   Distributions
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="chap04.html">
   Estimating Proportions
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="chap05.html">
   Estimating Counts
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="chap06.html">
   Odds and Addends
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="chap07.html">
   Minimum, Maximum, and Mixture
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="chap08.html">
   Poisson Processes
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="chap09.html">
   Decision Analysis
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="chap10.html">
   Testing
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="chap11.html">
   Comparison
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="chap12.html">
   Classification
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="chap13.html">
   Inference
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="chap14.html">
   Survival Analysis
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="chap15.html">
   Mark and Recapture
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="chap16.html">
   Logistic Regression
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="chap17.html">
   Regression
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="chap18.html">
   Conjugate Priors
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="chap19.html">
   MCMC
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="chap20.html">
   Approximate Bayesian Computation
  </a>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Examples
 </span>
</p>
<ul class="current nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="redline.html">
   The Red Line Problem
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="vaccine2.html">
   Estimating vaccine efficacy
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="usb.html">
   Flipping USB Connectors
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="sister.html">
   The Left Handed Sister Problem
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="bayes_dice.html">
   Bayesian Dice
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   Radiation
  </a>
 </li>
</ul>

</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="_sources/radiation.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        <a class="repository-button"
            href="https://github.com/AllenDowney/ThinkBayes2"><button type="button" class="btn btn-secondary topbarbtn"
                data-toggle="tooltip" data-placement="left" title="Source repository"><i
                    class="fab fa-github"></i>repository</button></a>
        
        
    </div>
</div>


            <!-- Full screen (wrap in <a> to have style consistency -->
            <a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
                    data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
                    title="Fullscreen mode"><i
                        class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        <a class="binder-button" href="https://mybinder.org/v2/gh/AllenDowney/ThinkBayes2/master?urlpath=tree/radiation.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Launch Binder" data-toggle="tooltip"
                data-placement="left"><img class="binder-button-logo"
                    src="_static/images/logo_binder.svg"
                    alt="Interact on binder">Binder</button></a>
        
        
        
        
    </div>
</div>

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show">
            
        <div class="tocsection onthispage pt-5 pb-3">
            <i class="fas fa-list"></i>
            Contents
        </div>
        <nav id="bd-toc-nav">
            <ul class="nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#modeling-a-radiation-sensor">
   Modeling a radiation sensor
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#first-update">
   First update
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#jeffreys-s-prior">
   Jeffreys’s prior
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#robot-a">
   Robot A
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#robot-b">
   Robot B
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#going-the-other-way">
   Going the other way
  </a>
 </li>
</ul>

        </nav>
        
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="radiation">
<h1>Radiation<a class="headerlink" href="#radiation" title="Permalink to this headline">¶</a></h1>
<p><a class="reference external" href="https://colab.research.google.com/github/AllenDowney/ThinkBayes2/blob/master/examples/radiation.ipynb">Click here to run this notebook on Colab</a></p>
<div class="section" id="modeling-a-radiation-sensor">
<h2>Modeling a radiation sensor<a class="headerlink" href="#modeling-a-radiation-sensor" title="Permalink to this headline">¶</a></h2>
<p>Here’s an example from Jaynes, <em>Probability Theory</em>, page 168:</p>
<blockquote>
<div><p>We have a radioactive source … which is emitting particles of some sort … There is a rate <span class="math notranslate nohighlight">\(p\)</span>, in particles per second, at which a radioactive nucleus sends particles through our counter; and each particle passing through produces counts at the rate <span class="math notranslate nohighlight">\(\theta\)</span>. From measuring the number {c1 , c2 , …} of counts in different seconds, what can we say about the numbers {n1 , n2 , …} actually passing through the counter in each second, and
what can we say about the strength of the source?</p>
</div></blockquote>
<p>I presented a <a class="reference external" href="https://www.greenteapress.com/thinkbayes/html/thinkbayes015.html#sec130">version of this problem</a> in the first edition of <em>Think Bayes</em>, but I don’t think I explained it well, and my solution was a bit of a mess.
In the second edition, I use more NumPy and SciPy, which makes it possible to express the solution more clearly and concisely, so let me give it another try.</p>
<p>As a model of the radioactive source, Jaynes suggests we imagine “<span class="math notranslate nohighlight">\(N\)</span> nuclei, each of which has independently the probability <span class="math notranslate nohighlight">\(r\)</span> of sending a particle through our counter in any one second”.
If <span class="math notranslate nohighlight">\(N\)</span> is large and <span class="math notranslate nohighlight">\(r\)</span> is small, the number or particles in a given second is well modeled by a Poisson distribution with parameter <span class="math notranslate nohighlight">\(s = N r\)</span>, where <span class="math notranslate nohighlight">\(s\)</span> is the strength of the source.</p>
<p>As a model of the sensor, we’ll assume that “each particle passing through the counter
has independently the probability <span class="math notranslate nohighlight">\(\phi\)</span> of making a count”.
So if we know the actual number of particles, <span class="math notranslate nohighlight">\(n\)</span>, and the efficiency of the sensor, <span class="math notranslate nohighlight">\(\phi\)</span>, the distribution of the count is binomial.</p>
<p>With that, we are ready to solve the problem, but first, an aside: I am not sure why Jaynes states the problem in terms of <span class="math notranslate nohighlight">\(p\)</span> and <span class="math notranslate nohighlight">\(\theta\)</span>, and then solves it in terms of <span class="math notranslate nohighlight">\(s\)</span> and <span class="math notranslate nohighlight">\(\phi\)</span>.
It might have been an oversight, or there might be subtle distinction he intended to draw the reader’s attention to.
The book is full of dire warnings about distinctions like this, but in this case I don’t see an explanation.</p>
<p>Anyway, following Jaynes, I’ll start with a uniform prior for <span class="math notranslate nohighlight">\(s\)</span>, over a range of values wide enough to cover the region where the likelihood of the data is non-negligible.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">empiricaldist</span> <span class="kn">import</span> <span class="n">Pmf</span>

<span class="n">ss</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">350</span><span class="p">,</span> <span class="mi">101</span><span class="p">)</span>
<span class="n">prior_s</span> <span class="o">=</span> <span class="n">Pmf</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">ss</span><span class="p">)</span>
<span class="n">prior_s</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>probs</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0.0</th>
      <td>1</td>
    </tr>
    <tr>
      <th>3.5</th>
      <td>1</td>
    </tr>
    <tr>
      <th>7.0</th>
      <td>1</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>For each value of <span class="math notranslate nohighlight">\(s\)</span>, the distribution of <span class="math notranslate nohighlight">\(n\)</span> is Poisson, so we can form the joint prior of <span class="math notranslate nohighlight">\(s\)</span> and <span class="math notranslate nohighlight">\(n\)</span> using the <code class="docutils literal notranslate"><span class="pre">poisson</span></code> function from SciPy.
I’ll use a range of values for <span class="math notranslate nohighlight">\(n\)</span> that, again, covers the region where the likelihood of the data is non-negligible.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">scipy.stats</span> <span class="kn">import</span> <span class="n">poisson</span>

<span class="n">ns</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">350</span><span class="p">)</span>
<span class="n">S</span><span class="p">,</span> <span class="n">N</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">meshgrid</span><span class="p">(</span><span class="n">ss</span><span class="p">,</span> <span class="n">ns</span><span class="p">)</span>
<span class="n">ps</span> <span class="o">=</span> <span class="n">poisson</span><span class="p">(</span><span class="n">S</span><span class="p">)</span><span class="o">.</span><span class="n">pmf</span><span class="p">(</span><span class="n">N</span><span class="p">)</span>
<span class="n">ps</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(350, 101)
</pre></div>
</div>
</div>
</div>
<p>The result is an array with one row for each value of <span class="math notranslate nohighlight">\(n\)</span> and one column for each value of <span class="math notranslate nohighlight">\(s\)</span>.
To get the prior probability for each pair, we multiply each row by the prior probabilities of <span class="math notranslate nohighlight">\(s\)</span>.
The following function encapsulates this computation and puts the result in a Pandas <code class="docutils literal notranslate"><span class="pre">DataFrame</span></code> that represents the joint prior.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>

<span class="k">def</span> <span class="nf">make_joint</span><span class="p">(</span><span class="n">prior_s</span><span class="p">,</span> <span class="n">ns</span><span class="p">):</span>
    <span class="n">ss</span> <span class="o">=</span> <span class="n">prior_s</span><span class="o">.</span><span class="n">qs</span>
    <span class="n">S</span><span class="p">,</span> <span class="n">N</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">meshgrid</span><span class="p">(</span><span class="n">ss</span><span class="p">,</span> <span class="n">ns</span><span class="p">)</span>
    <span class="n">ps</span> <span class="o">=</span> <span class="n">poisson</span><span class="p">(</span><span class="n">S</span><span class="p">)</span><span class="o">.</span><span class="n">pmf</span><span class="p">(</span><span class="n">N</span><span class="p">)</span> <span class="o">*</span> <span class="n">prior_s</span><span class="o">.</span><span class="n">ps</span>
    <span class="n">joint</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">ps</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="n">ns</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="n">ss</span><span class="p">)</span>
    <span class="n">joint</span><span class="o">.</span><span class="n">index</span><span class="o">.</span><span class="n">name</span> <span class="o">=</span> <span class="s1">&#39;n&#39;</span>
    <span class="n">joint</span><span class="o">.</span><span class="n">columns</span><span class="o">.</span><span class="n">name</span> <span class="o">=</span> <span class="s1">&#39;s&#39;</span>
    <span class="k">return</span> <span class="n">joint</span>
</pre></div>
</div>
</div>
</div>
<p>Here’s the joint prior:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">joint</span> <span class="o">=</span> <span class="n">make_joint</span><span class="p">(</span><span class="n">prior_s</span><span class="p">,</span> <span class="n">ns</span><span class="p">)</span>
<span class="n">joint</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th>s</th>
      <th>0.0</th>
      <th>3.5</th>
      <th>7.0</th>
      <th>10.5</th>
      <th>14.0</th>
      <th>17.5</th>
      <th>21.0</th>
      <th>24.5</th>
      <th>28.0</th>
      <th>31.5</th>
      <th>...</th>
      <th>318.5</th>
      <th>322.0</th>
      <th>325.5</th>
      <th>329.0</th>
      <th>332.5</th>
      <th>336.0</th>
      <th>339.5</th>
      <th>343.0</th>
      <th>346.5</th>
      <th>350.0</th>
    </tr>
    <tr>
      <th>n</th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1.0</td>
      <td>0.030197</td>
      <td>0.000912</td>
      <td>0.000028</td>
      <td>8.315287e-07</td>
      <td>2.510999e-08</td>
      <td>7.582560e-10</td>
      <td>2.289735e-11</td>
      <td>6.914400e-13</td>
      <td>2.087968e-14</td>
      <td>...</td>
      <td>4.755624e-139</td>
      <td>1.436074e-140</td>
      <td>4.336568e-142</td>
      <td>1.309530e-143</td>
      <td>3.954438e-145</td>
      <td>1.194137e-146</td>
      <td>3.605981e-148</td>
      <td>1.088912e-149</td>
      <td>3.288229e-151</td>
      <td>9.929590e-153</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0.0</td>
      <td>0.105691</td>
      <td>0.006383</td>
      <td>0.000289</td>
      <td>1.164140e-05</td>
      <td>4.394249e-07</td>
      <td>1.592338e-08</td>
      <td>5.609850e-10</td>
      <td>1.936032e-11</td>
      <td>6.577099e-13</td>
      <td>...</td>
      <td>1.514666e-136</td>
      <td>4.624158e-138</td>
      <td>1.411553e-139</td>
      <td>4.308354e-141</td>
      <td>1.314851e-142</td>
      <td>4.012300e-144</td>
      <td>1.224230e-145</td>
      <td>3.734968e-147</td>
      <td>1.139371e-148</td>
      <td>3.475357e-150</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.0</td>
      <td>0.184959</td>
      <td>0.022341</td>
      <td>0.001518</td>
      <td>8.148981e-05</td>
      <td>3.844967e-06</td>
      <td>1.671955e-07</td>
      <td>6.872067e-09</td>
      <td>2.710445e-10</td>
      <td>1.035893e-11</td>
      <td>...</td>
      <td>2.412106e-134</td>
      <td>7.444895e-136</td>
      <td>2.297302e-137</td>
      <td>7.087242e-139</td>
      <td>2.185939e-140</td>
      <td>6.740663e-142</td>
      <td>2.078131e-143</td>
      <td>6.405469e-145</td>
      <td>1.973961e-146</td>
      <td>6.081874e-148</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.0</td>
      <td>0.215785</td>
      <td>0.052129</td>
      <td>0.005313</td>
      <td>3.802858e-04</td>
      <td>2.242898e-05</td>
      <td>1.170368e-06</td>
      <td>5.612188e-08</td>
      <td>2.529749e-09</td>
      <td>1.087688e-10</td>
      <td>...</td>
      <td>2.560853e-132</td>
      <td>7.990854e-134</td>
      <td>2.492573e-135</td>
      <td>7.772342e-137</td>
      <td>2.422749e-138</td>
      <td>7.549543e-140</td>
      <td>2.351752e-141</td>
      <td>7.323587e-143</td>
      <td>2.279925e-144</td>
      <td>7.095520e-146</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.0</td>
      <td>0.188812</td>
      <td>0.091226</td>
      <td>0.013946</td>
      <td>1.331000e-03</td>
      <td>9.812677e-05</td>
      <td>6.144433e-06</td>
      <td>3.437465e-07</td>
      <td>1.770824e-08</td>
      <td>8.565541e-10</td>
      <td>...</td>
      <td>2.039079e-130</td>
      <td>6.432637e-132</td>
      <td>2.028331e-133</td>
      <td>6.392751e-135</td>
      <td>2.013910e-136</td>
      <td>6.341616e-138</td>
      <td>1.996049e-139</td>
      <td>6.279975e-141</td>
      <td>1.974985e-142</td>
      <td>6.208580e-144</td>
    </tr>
  </tbody>
</table>
<p>5 rows × 101 columns</p>
</div></div></div>
</div>
<p>Now we’re ready to compute the likelihood of the data.
In this problem, it depends only on <span class="math notranslate nohighlight">\(n\)</span>, regardless of <span class="math notranslate nohighlight">\(s\)</span>, so we only have to compute it once.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">scipy.stats</span> <span class="kn">import</span> <span class="n">binom</span>

<span class="n">phi</span> <span class="o">=</span> <span class="mf">0.1</span>
<span class="n">c</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">likelihood</span> <span class="o">=</span> <span class="n">binom</span><span class="p">(</span><span class="n">ns</span><span class="p">,</span> <span class="n">phi</span><span class="p">)</span><span class="o">.</span><span class="n">pmf</span><span class="p">(</span><span class="n">c</span><span class="p">)</span>
<span class="n">likelihood</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(350,)
</pre></div>
</div>
</div>
</div>
<p>The result is an array of likelihoods, one for each value of <span class="math notranslate nohighlight">\(n\)</span>.
To do the Bayesian update, we need to multiply each column in the prior by this array of likelihoods.
We can do that using the <code class="docutils literal notranslate"><span class="pre">multiply</span></code> method with the <code class="docutils literal notranslate"><span class="pre">axis</span></code> argument.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">product</span> <span class="o">=</span> <span class="n">joint</span><span class="o">.</span><span class="n">multiply</span><span class="p">(</span><span class="n">likelihood</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">product</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th>s</th>
      <th>0.0</th>
      <th>3.5</th>
      <th>7.0</th>
      <th>10.5</th>
      <th>14.0</th>
      <th>17.5</th>
      <th>21.0</th>
      <th>24.5</th>
      <th>28.0</th>
      <th>31.5</th>
      <th>...</th>
      <th>318.5</th>
      <th>322.0</th>
      <th>325.5</th>
      <th>329.0</th>
      <th>332.5</th>
      <th>336.0</th>
      <th>339.5</th>
      <th>343.0</th>
      <th>346.5</th>
      <th>350.0</th>
    </tr>
    <tr>
      <th>n</th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>...</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>...</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>...</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>...</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>...</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
    </tr>
  </tbody>
</table>
<p>5 rows × 101 columns</p>
</div></div></div>
</div>
<p>The following function encapsulates this computation, normalizes the result, and returns the posterior distribution.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">utils</span> <span class="kn">import</span> <span class="n">normalize</span>

<span class="k">def</span> <span class="nf">update</span><span class="p">(</span><span class="n">joint</span><span class="p">,</span> <span class="n">phi</span><span class="p">,</span> <span class="n">c</span><span class="p">):</span>
    <span class="n">ns</span> <span class="o">=</span> <span class="n">joint</span><span class="o">.</span><span class="n">index</span>
    <span class="n">likelihood</span> <span class="o">=</span> <span class="n">binom</span><span class="p">(</span><span class="n">ns</span><span class="p">,</span> <span class="n">phi</span><span class="p">)</span><span class="o">.</span><span class="n">pmf</span><span class="p">(</span><span class="n">c</span><span class="p">)</span>
    <span class="n">posterior</span> <span class="o">=</span> <span class="n">joint</span><span class="o">.</span><span class="n">multiply</span><span class="p">(</span><span class="n">likelihood</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">normalize</span><span class="p">(</span><span class="n">posterior</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">posterior</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="first-update">
<h2>First update<a class="headerlink" href="#first-update" title="Permalink to this headline">¶</a></h2>
<p>Let’s test the update function with the first example, on page 178:</p>
<blockquote>
<div><p>During the first second, <code class="docutils literal notranslate"><span class="pre">c1</span> <span class="pre">=</span> <span class="pre">10</span></code> counts are registered. What can [we] say about the number <code class="docutils literal notranslate"><span class="pre">n1</span></code> of particles?</p>
</div></blockquote>
<p>Here’s the update:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">c1</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">phi</span> <span class="o">=</span> <span class="mf">0.1</span>
<span class="n">posterior</span> <span class="o">=</span> <span class="n">update</span><span class="p">(</span><span class="n">joint</span><span class="p">,</span> <span class="n">phi</span><span class="p">,</span> <span class="n">c1</span><span class="p">)</span>
<span class="n">posterior</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th>s</th>
      <th>0.0</th>
      <th>3.5</th>
      <th>7.0</th>
      <th>10.5</th>
      <th>14.0</th>
      <th>17.5</th>
      <th>21.0</th>
      <th>24.5</th>
      <th>28.0</th>
      <th>31.5</th>
      <th>...</th>
      <th>318.5</th>
      <th>322.0</th>
      <th>325.5</th>
      <th>329.0</th>
      <th>332.5</th>
      <th>336.0</th>
      <th>339.5</th>
      <th>343.0</th>
      <th>346.5</th>
      <th>350.0</th>
    </tr>
    <tr>
      <th>n</th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>...</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>...</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>...</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>...</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>...</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
    </tr>
  </tbody>
</table>
<p>5 rows × 101 columns</p>
</div></div></div>
</div>
<p>The following figures is a contour plot of the joint posterior distribution.
As you might expect, <span class="math notranslate nohighlight">\(s\)</span> and <span class="math notranslate nohighlight">\(n\)</span> are highly correlated; that is, if we believe <span class="math notranslate nohighlight">\(s\)</span> is low, we should believe that <span class="math notranslate nohighlight">\(n\)</span> is low, and likewise if <span class="math notranslate nohighlight">\(s\)</span> is high.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">utils</span> <span class="kn">import</span> <span class="n">plot_contour</span>

<span class="n">plot_contour</span><span class="p">(</span><span class="n">posterior</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/radiation_22_0.png" src="_images/radiation_22_0.png" />
</div>
</div>
<p>From the posterior distribution, we can extract the marginal distributions of <span class="math notranslate nohighlight">\(s\)</span> and <span class="math notranslate nohighlight">\(n\)</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">utils</span> <span class="kn">import</span> <span class="n">marginal</span>

<span class="n">posterior_s</span> <span class="o">=</span> <span class="n">marginal</span><span class="p">(</span><span class="n">posterior</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
<span class="n">posterior_s</span><span class="o">.</span><span class="n">plot</span><span class="p">()</span>
<span class="n">posterior_s</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>109.99983355767465
</pre></div>
</div>
<img alt="_images/radiation_24_1.png" src="_images/radiation_24_1.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">posterior_n</span> <span class="o">=</span> <span class="n">marginal</span><span class="p">(</span><span class="n">posterior</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">posterior_n</span><span class="o">.</span><span class="n">plot</span><span class="p">()</span>
<span class="n">posterior_n</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>108.99984691276569
</pre></div>
</div>
<img alt="_images/radiation_25_1.png" src="_images/radiation_25_1.png" />
</div>
</div>
<p>The posterior mean of <span class="math notranslate nohighlight">\(n\)</span> is close to 109, which is consistent with Equation 6.116.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">posterior_n</span><span class="o">.</span><span class="n">idxmax</span><span class="p">(),</span> <span class="n">c</span><span class="o">/</span><span class="n">phi</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(99, 100.0)
</pre></div>
</div>
</div>
</div>
<p>The MAP is 99, which is one less than the analytic result in Equation 6.113, which is 100.
It looks like the posterior probabilities for 99 and 100 are the same, but the floating-point results differ slightly.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">posterior_n</span><span class="p">[</span><span class="mi">99</span><span class="p">]</span> <span class="o">-</span> <span class="n">posterior_n</span><span class="p">[</span><span class="mi">100</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>5.065392549852277e-16
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="jeffreys-s-prior">
<h2>Jeffreys’s prior<a class="headerlink" href="#jeffreys-s-prior" title="Permalink to this headline">¶</a></h2>
<p>Instead of a uniform prior for <span class="math notranslate nohighlight">\(s\)</span>, we can use a Jeffreys prior, in which the prior probability for each value of <span class="math notranslate nohighlight">\(s\)</span> is proportional to <span class="math notranslate nohighlight">\(1/s\)</span>.
This has the advantage of “invariance under certain changes of parameters”, which is “the only correct way to express complete ignorance of a scale parameter.”
However, Jaynes suggests that it is not clear “whether <span class="math notranslate nohighlight">\(s\)</span> can properly be regarded as a scale parameter in this problem.”</p>
<p>Nevertheless, he suggests we try it and see what happens.
Here’s the Jeffreys prior for <span class="math notranslate nohighlight">\(s\)</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">prior_jeff</span> <span class="o">=</span> <span class="n">Pmf</span><span class="p">(</span><span class="mi">1</span><span class="o">/</span><span class="n">ss</span><span class="p">[</span><span class="mi">1</span><span class="p">:],</span> <span class="n">ss</span><span class="p">[</span><span class="mi">1</span><span class="p">:])</span>
<span class="n">prior_jeff</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>probs</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>3.5</th>
      <td>0.285714</td>
    </tr>
    <tr>
      <th>7.0</th>
      <td>0.142857</td>
    </tr>
    <tr>
      <th>10.5</th>
      <td>0.095238</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>We can use it to compute the joint prior of <span class="math notranslate nohighlight">\(s\)</span> and <span class="math notranslate nohighlight">\(n\)</span>, and update it with <code class="docutils literal notranslate"><span class="pre">c1</span></code>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">joint_jeff</span> <span class="o">=</span> <span class="n">make_joint</span><span class="p">(</span><span class="n">prior_jeff</span><span class="p">,</span> <span class="n">ns</span><span class="p">)</span>
<span class="n">posterior_jeff</span> <span class="o">=</span> <span class="n">update</span><span class="p">(</span><span class="n">joint_jeff</span><span class="p">,</span> <span class="n">phi</span><span class="p">,</span> <span class="n">c1</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Here’s the marginal posterior distribution of <span class="math notranslate nohighlight">\(n\)</span>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">posterior_n</span> <span class="o">=</span> <span class="n">marginal</span><span class="p">(</span><span class="n">posterior_jeff</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">posterior_n</span><span class="o">.</span><span class="n">plot</span><span class="p">()</span>
<span class="n">posterior_n</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>99.99995605790188
</pre></div>
</div>
<img alt="_images/radiation_35_1.png" src="_images/radiation_35_1.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">posterior_n</span><span class="o">.</span><span class="n">idxmax</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>91
</pre></div>
</div>
</div>
</div>
<p>The posterior mean is close to 100 and the MAP is 91; both are consistent with the results in Equation 6.122.</p>
</div>
<div class="section" id="robot-a">
<h2>Robot A<a class="headerlink" href="#robot-a" title="Permalink to this headline">¶</a></h2>
<p>Now we get to what I think is the most interesting part of this example, which is to take into account a second observation under two models of the scenario:</p>
<blockquote>
<div><p>Two robots, [A and B], have different prior information about the source of the particles.
The source is hidden in another room which A and B are not allowed to enter.
A has no knowledge at all about the source of particles; for all [it] knows, … the other room might be full of little [people] who run back and forth, holding first one radioactive source, then another, up to the exit window.</p>
<p>B has one additional qualitative fact: [it] knows that the source is a radioactive sample of long lifetime, in a fixed position.</p>
</div></blockquote>
<p>In other words, B has reason to believe that the source strength <span class="math notranslate nohighlight">\(s\)</span> is constant from one interval to the next, while A admits the possibility that <span class="math notranslate nohighlight">\(s\)</span> is different for each interval.</p>
<p>The following figure, from Jaynes, represents these models graphically (Jaynes calls them “logical situations” because he seems to be allergic to the word “model”).</p>
<img src="https://github.com/AllenDowney/ThinkBayes2/raw/master/examples/jaynes177.png" width="400">
<p>For A, the “different intervals are logically independent”, so the update with <code class="docutils literal notranslate"><span class="pre">c2</span> <span class="pre">=</span> <span class="pre">16</span></code> starts with the same prior.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">c2</span> <span class="o">=</span> <span class="mi">16</span>
<span class="n">posterior2</span> <span class="o">=</span> <span class="n">update</span><span class="p">(</span><span class="n">joint</span><span class="p">,</span> <span class="n">phi</span><span class="p">,</span> <span class="n">c2</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Here’s the posterior marginal distribution of <code class="docutils literal notranslate"><span class="pre">n2</span></code>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">posterior_n2</span> <span class="o">=</span> <span class="n">marginal</span><span class="p">(</span><span class="n">posterior2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">posterior_n2</span><span class="o">.</span><span class="n">plot</span><span class="p">()</span>
<span class="n">posterior_n2</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>168.947980523708
</pre></div>
</div>
<img alt="_images/radiation_41_1.png" src="_images/radiation_41_1.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">posterior_n2</span><span class="o">.</span><span class="n">idxmax</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>160
</pre></div>
</div>
</div>
</div>
<p>The posterior mean is close to 169, which is consistent with the result in Equation 6.124.
The MAP is 160, which is consistent with 6.123.</p>
</div>
<div class="section" id="robot-b">
<h2>Robot B<a class="headerlink" href="#robot-b" title="Permalink to this headline">¶</a></h2>
<p>For B, the “logical situation” is different. If we consider <span class="math notranslate nohighlight">\(s\)</span> to be constant, we can – and should! – take the information from the first update into account when we perform the second update.
We can do that by using the posterior distribution of <span class="math notranslate nohighlight">\(s\)</span> from the first update to form the joint prior for the second update, like this:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">joint</span> <span class="o">=</span> <span class="n">make_joint</span><span class="p">(</span><span class="n">posterior_s</span><span class="p">,</span> <span class="n">ns</span><span class="p">)</span>
<span class="n">posterior</span> <span class="o">=</span> <span class="n">update</span><span class="p">(</span><span class="n">joint</span><span class="p">,</span> <span class="n">phi</span><span class="p">,</span> <span class="n">c2</span><span class="p">)</span>
<span class="n">posterior_n</span> <span class="o">=</span> <span class="n">marginal</span><span class="p">(</span><span class="n">posterior</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">posterior_n</span><span class="o">.</span><span class="n">plot</span><span class="p">()</span>
<span class="n">posterior_n</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>137.499999315101
</pre></div>
</div>
<img alt="_images/radiation_45_1.png" src="_images/radiation_45_1.png" />
</div>
</div>
<p>The posterior mean of <span class="math notranslate nohighlight">\(n\)</span> is close to 137.5, which is consistent with Equation 6.134.
The MAP is 132, which is one less than the analytic result, 133.
But again, there are two values with the same probability except for floating-point errors.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">posterior_n</span><span class="o">.</span><span class="n">idxmax</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>132
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">posterior_n</span><span class="p">[</span><span class="mi">132</span><span class="p">]</span> <span class="o">-</span> <span class="n">posterior_n</span><span class="p">[</span><span class="mi">133</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>2.914335439641036e-16
</pre></div>
</div>
</div>
</div>
<p>Under B’s model, the data from the first interval updates our belief about <span class="math notranslate nohighlight">\(s\)</span>, which influences what we believe about <code class="docutils literal notranslate"><span class="pre">n2</span></code>.</p>
</div>
<div class="section" id="going-the-other-way">
<h2>Going the other way<a class="headerlink" href="#going-the-other-way" title="Permalink to this headline">¶</a></h2>
<p>That might not seem surprising, but there is an additional point Jaynes makes with this example, which is that it also works the other way around: Having seen <code class="docutils literal notranslate"><span class="pre">c2</span></code>, we have more information about <span class="math notranslate nohighlight">\(s\)</span>, which means we can – and should! – go back and reconsider what we concluded about <code class="docutils literal notranslate"><span class="pre">n1</span></code>.</p>
<p>We can do that by imagining we did the experiments in the opposite order, so</p>
<ol class="simple">
<li><p>We’ll start again with a joint prior based on a uniform distribution for <span class="math notranslate nohighlight">\(s\)</span>,</p></li>
<li><p>Update it based on <code class="docutils literal notranslate"><span class="pre">n2</span></code>,</p></li>
<li><p>Use the posterior distribution of <span class="math notranslate nohighlight">\(s\)</span> to form a new joint prior,</p></li>
<li><p>Update it based on <code class="docutils literal notranslate"><span class="pre">c1</span></code>, and</p></li>
<li><p>Extract the marginal posterior for <code class="docutils literal notranslate"><span class="pre">n1</span></code>.</p></li>
</ol>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">joint</span> <span class="o">=</span> <span class="n">make_joint</span><span class="p">(</span><span class="n">prior_s</span><span class="p">,</span> <span class="n">ns</span><span class="p">)</span>
<span class="n">posterior</span> <span class="o">=</span> <span class="n">update</span><span class="p">(</span><span class="n">joint</span><span class="p">,</span> <span class="n">phi</span><span class="p">,</span> <span class="n">c2</span><span class="p">)</span>
<span class="n">posterior_s</span> <span class="o">=</span> <span class="n">marginal</span><span class="p">(</span><span class="n">posterior</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
<span class="n">posterior_s</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>169.94393251129674
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">joint</span> <span class="o">=</span> <span class="n">make_joint</span><span class="p">(</span><span class="n">posterior_s</span><span class="p">,</span> <span class="n">ns</span><span class="p">)</span>
<span class="n">posterior</span> <span class="o">=</span> <span class="n">update</span><span class="p">(</span><span class="n">joint</span><span class="p">,</span> <span class="n">phi</span><span class="p">,</span> <span class="n">c1</span><span class="p">)</span>
<span class="n">posterior_n2</span> <span class="o">=</span> <span class="n">marginal</span><span class="p">(</span><span class="n">posterior</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">posterior_n2</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>131.49999935386944
</pre></div>
</div>
</div>
</div>
<p>The posterior mean is close to 131.5, which is consistent with Equation 6.133.
And the MAP is 126, which is one less than the result in Equation 6.132, again due to floating-point error.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">posterior_n2</span><span class="o">.</span><span class="n">idxmax</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>126
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">posterior_n2</span><span class="p">[</span><span class="mi">126</span><span class="p">]</span> <span class="o">-</span> <span class="n">posterior_n2</span><span class="p">[</span><span class="mi">127</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>3.8163916471489756e-16
</pre></div>
</div>
</div>
</div>
<p>Here’s what the new distribution of <code class="docutils literal notranslate"><span class="pre">n1</span></code> looks like compared to the original, which was based on <code class="docutils literal notranslate"><span class="pre">c1</span></code> only.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">joint</span> <span class="o">=</span> <span class="n">make_joint</span><span class="p">(</span><span class="n">prior_s</span><span class="p">,</span> <span class="n">ns</span><span class="p">)</span>
<span class="n">posterior</span> <span class="o">=</span> <span class="n">update</span><span class="p">(</span><span class="n">joint</span><span class="p">,</span> <span class="n">phi</span><span class="p">,</span> <span class="n">c1</span><span class="p">)</span>
<span class="n">posterior_n</span> <span class="o">=</span> <span class="n">marginal</span><span class="p">(</span><span class="n">posterior</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">utils</span> <span class="kn">import</span> <span class="n">decorate</span>

<span class="n">posterior_n</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">label</span><span class="o">=</span><span class="s1">&#39;Model A&#39;</span><span class="p">)</span>
<span class="n">posterior_n2</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">label</span><span class="o">=</span><span class="s1">&#39;Model B&#39;</span><span class="p">)</span>
<span class="n">decorate</span><span class="p">(</span><span class="n">title</span><span class="o">=</span><span class="s1">&#39;Posterior distributions of n under different models&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/radiation_58_0.png" src="_images/radiation_58_0.png" />
</div>
</div>
<p>With the additional information from <code class="docutils literal notranslate"><span class="pre">c2</span></code>:</p>
<ul class="simple">
<li><p>We give higher probability to large values of <span class="math notranslate nohighlight">\(s\)</span>, so we also give higher probability to large values of <code class="docutils literal notranslate"><span class="pre">n1</span></code>, and</p></li>
<li><p>The width of the distribution is narrower, which shows that with more information about <span class="math notranslate nohighlight">\(s\)</span>, we have more information about <code class="docutils literal notranslate"><span class="pre">n1</span></code>.</p></li>
</ul>
<p>This is one of several examples Jaynes uses to distinguish between “logical and causal dependence.” In this example, causal dependence only goes in the forward direction: “<span class="math notranslate nohighlight">\(s\)</span> is the physical cause which partially determines <span class="math notranslate nohighlight">\(n\)</span>; and then <span class="math notranslate nohighlight">\(n\)</span> in turn is the physical cause which partially determines <span class="math notranslate nohighlight">\(c\)</span>”.</p>
<p>Therefore, <code class="docutils literal notranslate"><span class="pre">c1</span></code> and <code class="docutils literal notranslate"><span class="pre">c2</span></code> are causally independent: if the number of particles counted in one interval is unusually high (or low), that does not cause the number of particles during any other interval to be higher or lower.</p>
<p>But if <span class="math notranslate nohighlight">\(s\)</span> is unknown, they are not <em>logically</em> independent. For example, if <code class="docutils literal notranslate"><span class="pre">c1</span></code> is lower than expected, based on the prior distribution of <span class="math notranslate nohighlight">\(s\)</span>, that implies that lower values of <span class="math notranslate nohighlight">\(s\)</span> are more likely, which implies that lower values of <code class="docutils literal notranslate"><span class="pre">n2</span></code> are more likely, which implies that lower values of <code class="docutils literal notranslate"><span class="pre">c2</span></code> are more likely.</p>
<p>And, as we’ve seen, it works the other way, too.
For example, if <code class="docutils literal notranslate"><span class="pre">c2</span></code> is higher than expected, that implies that higher values of <span class="math notranslate nohighlight">\(s\)</span>, <code class="docutils literal notranslate"><span class="pre">n1</span></code>, and <code class="docutils literal notranslate"><span class="pre">c1</span></code> are more likely.</p>
<p>If you find the second result more surprising – that is, if you think it’s weird that <code class="docutils literal notranslate"><span class="pre">c2</span></code> changes what we believe about <code class="docutils literal notranslate"><span class="pre">n1</span></code> and <code class="docutils literal notranslate"><span class="pre">c1</span></code> – that implies that you are not (yet) distinguishing logical and causal dependence.</p>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        
        <div class='prev-next-bottom'>
            
    <a class='left-prev' id="prev-link" href="bayes_dice.html" title="previous page">Bayesian Dice</a>

        </div>
        
        </div>
    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By Allen B. Downey<br/>
        
            &copy; Copyright 2020.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>

    
  <script src="_static/js/index.d3f166471bb80abb5163.js"></script>


    
  </body>
</html>